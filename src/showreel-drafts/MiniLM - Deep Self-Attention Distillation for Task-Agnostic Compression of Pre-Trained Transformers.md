---
link: https://arxiv.org/pdf/2002.10957.pdf
title: MiniLM - Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
image: /images/showreel/MiniLM - Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.jpg
date: 2020-02-25
tags: compression, text (NLP)
draft: draft
preview:
---


