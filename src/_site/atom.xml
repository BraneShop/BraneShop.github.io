<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Braneshop | braneshop.com.au</title>
    <link href="https://braneshop.com.au/atom.xml" rel="self" />
    <link href="https://braneshop.com.au" />
    <id>https://braneshop.com.au/atom.xml</id>
    <author>
        <name>Braneshop Team</name>
        <email>noonsilk+-braneshop@gmail.com</email>
    </author>
    <updated>2019-07-24T00:00:00Z</updated>
    <entry>
    <title>Scholarships for August Technical Deep Learning Workshop Close on Sunday!</title>
    <link href="https://braneshop.com.au/posts/Aug-2019-Scholarships-Close-On-Sunday.html" />
    <id>https://braneshop.com.au/posts/Aug-2019-Scholarships-Close-On-Sunday.html</id>
    <published>2019-07-24T00:00:00Z</published>
    <updated>2019-07-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on July 24, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Scholarships for August Technical Deep Learning Workshop Close on Sunday!</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a></span>
  

  
  <p>Video summary:</p>
  <center>
    <iframe src="https://player.vimeo.com/video/349597581" width="800" height="400" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
    <p>Watch on Vimeo - <a href="https://player.vimeo.com/video/349597581">Note on scholarships.</a></p>
  </center>
  

  

  <div class="info">
    
  </div>

  <p>Ruth and I are very pleased to be offering scholarships for our workshop coming up on the 8th of August. We’ve already accept first-round scholarships (6 people), and so there are some remaining places.</p>
<p>We’re encouraging applications from anyone that has typically faced barriers in their career/learning in the tech industry. One of our big aims at Braneshop is to increase the representation in the AI industry, and this is one way we’re working towards that goal.</p>
<p>You can find the application form here: <a href="https://noonvandersilk.typeform.com/to/Tnfm4a">Scholarship for the 6 Week Technical Deep Learning Workshop</a>, and of course more details about the workshop itself here: <a href="/6-week-workshop-on-deep-learning.html">6 Week Workshop</a>.</p>
<p>We’ve also got a scholarship for the <a href="/ai-for-leadership.html">AI For Leadership Workshop</a>, coming up in September. This is an important one for us as well, as, in order to see change broadly through-out the industry, we will need change in leadership positions.</p>
<p>Here at the Braneshop we’re creating a welcoming and supportive community for everyone to be involved in AI.</p>
<h4 id="technical-details-about-the-video">Technical details about the video …</h4>
<p>I had a lot of fun making the video above. You’ll notice, if you watch it, that my person is cut out, and a new weird background is inserted. I did this using <a href="https://github.com/tensorflow/models/tree/master/research/deeplab">deep-lab</a> from TensorFlow. If you’re feeling adventurous you can try out their demo for yourself, on <a href="https://colab.sandbox.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb">Google Colab</a>.</p>
<p>With that in hand, I worked out how to extract all the frames of the video as individual images, then ran that network over each of them (there were ~1600 images, but it only took a few minutes on my laptop (which doesn’t have a GPU)). After that, I made the little background animation<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, overlayed the images, and stitched them back together. I did all this using the “<a href="https://ffmpeg.org/">ffmpeg</a>” and <a href="https://imagemagick.org/index.php">ImageMagick</a><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>Also, while looking around and eventually settling on deep-lab, I found a <a href="https://www.tensorflow.org/js">TensorFlow.js</a> <a href="https://github.com/tensorflow/tfjs-models/tree/master/body-pix">demo project</a> that performs <a href="https://storage.googleapis.com/tfjs-models/demos/body-pix/index.html">person-segmentation in the browser</a>! It’s not amazingly high quality, but it runs in the browser! Pretty cool.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1">I used my “<a href="https://github.com/silky/cppn-cli">cppn-cli</a>” program to do this.
<pre class="terminal">
# Having a pre-existing checkpoint
cppn existing sample --checkpoint_dir logs/cf9ecd76 --width 288 --height 513 \
 --out out/vid-bg --z_steps 1614
</pre>
<a href="#fnref1">↩</a></li>
<li id="fn2">Roughly, here are the commands I used (ignoring the ones I used to generate the animations):
<pre class="terminal">
# Extract images
ffmpeg -i original.mp4 images/img%05d.jpg -hide_banner
#
# In the "vid-bg" folder; make composite images.
for i in *.png; do convert $i ~/dev/deep-lab/masked-images/img$i \
  -gravity center -compose over -composite ~/dev/deep-lab/with-bg/$i.jpg; \
done;
#
# Learn the framerate of the original video
ffprobe -v 0 -of csv=p=0 -select_streams v:0 -show_entries stream=r_frame_rate \
    original.mp4
#
# Make a video from images
ffmpeg -framerate 29.5 -pattern_type glob -i 'images/*.jpg' \
   -c:v libx264 -pix_fmt yuv420p out.mp4
#
# Copy audio into a video, from a video (named here "audio.mp4")
ffmpeg -i video.mp4 -i audio.mp4 -c copy -map 0:0 -map 1:1 -shortest out.mp4
</pre>
<a href="#fnref2">↩</a></li>
</ol>
</div>
</div>
]]></summary>
</entry>
<entry>
    <title>Braneshop scholarships and why we're focused on building a community</title>
    <link href="https://braneshop.com.au/posts/Braneshop-Scholarships-and-Why-We-are-Focused-On-Building-A-Community.html" />
    <id>https://braneshop.com.au/posts/Braneshop-Scholarships-and-Why-We-are-Focused-On-Building-A-Community.html</id>
    <published>2019-07-12T00:00:00Z</published>
    <updated>2019-07-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on July 12, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Braneshop scholarships and why we're focused on building a community</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a></span>
  

  

  
    <center> <p> <img src="/images/blog/noon-ruth-rrr.jpg" width="100%" /> 
    <br /><small>Ruth and I dropped by RRR to chat about the Braneshop and the AI community.</small></p> </center>
  

  <div class="info">
    
  </div>

  <p>Earlier this week Ruth and I were on RRR’s <a href="https://www.rrr.org.au/explore/programs/byte-into-it/episodes/8310-byte-into-it-10-july-2019">Byte Into It</a> talking about our workshops, and our approach to enriching the AI community in general (click the link if you want to have a listen to the show).</p>
<p>I wanted to discuss a bit our intention with our scholarships, and our vision for the AI community.</p>
<h4 id="the-braneshop-community">The Braneshop Community</h4>
<p>We love community here at the Braneshop. In fact, in an amazing coincidence, we’ve actually moved in to a space <em>called</em> <a href="https://ochouse.com.au">Our Community House</a>, out in North Melbourne.</p>
<p>One thing we want to do is help people feel at home in the AI community.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Deep learning showreel now online!</title>
    <link href="https://braneshop.com.au/posts/Showreel-Online.html" />
    <id>https://braneshop.com.au/posts/Showreel-Online.html</id>
    <published>2019-07-11T00:00:00Z</published>
    <updated>2019-07-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on July 11, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Deep learning showreel now online!</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a></span>
  

  

  
    <center> <p> <img src="/images/showreel/Understanding and Predicting Visual Humour.jpg" width="100%" /> 
    <br /><small><a href="https://arxiv.org/pdf/1512.04407.pdf">My favourite paper of 2015</a>, and maybe of all time. See the <a href="/showreel.html">showreel</a> page for more!</small></p> </center>
  

  <div class="info">
    
  </div>

  <p>We love staying up to date with the latest deep learning research. I typically catch up daily, via the <a href="https://scirate.com">SciRate</a> website (check if out if you haven’t already!)</p>
<p>Every now and then, a paper really stands out as either cool, interesting, innovative, or useful. When something really appealed to me, I’d typically post it in various places, such as LinkedIn, or slack, or twitter, or just file it away in my head, waiting for the perfect moment to pop up again.</p>
<p>We also typically include these as part of the workshops, to get people a bit excited about the field.</p>
<p>Well, now I’ve decided to put that list online: <a href="/showreel.html">Deep learning showreel!</a></p>
<p>Each paper comes with an indicative picture, and a little blurb explaining why it’s interesting/important or what we like about it. Feel free to take a browse only at those pictures that interest you. You can also browse by tag; for example here’s the items for <a href="showreel-tags/computer-vision.html">computer-vision</a> (you can see all the tags at the bottom of that page).</p>
<p>This list will also go out in our monthly newsletter, so if you’re interested in such things, then subscribe below!</p>
<p>If you’re technical, and want to get the items before they hit the page, I’ve <a href="https://github.com/BraneShop/showreel">set up a bit of a system on github</a> that lets me collect the interesting papers there, for later processing.</p>
<p>We hope you find this useful!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>State of the art attention models in PyText - Part 1</title>
    <link href="https://braneshop.com.au/posts/State-of-the-art-attention-models-in-PyText-Part-1.html" />
    <id>https://braneshop.com.au/posts/State-of-the-art-attention-models-in-PyText-Part-1.html</id>
    <published>2019-07-03T00:00:00Z</published>
    <updated>2019-07-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on July  3, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">State of the art attention models in PyText - Part 1</h4>

  
    <span class="tags">Tags: <a href="/tags/deep-dive.html">deep-dive</a>, <a href="/tags/pytext.html">pytext</a></span>
  

  
  <p>Video summary:</p>
  <center>
    <iframe src="https://player.vimeo.com/video/347859684" width="800" height="400" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
    <p>Watch on Vimeo - <a href="https://player.vimeo.com/video/347859684">Braneshop - Attention in PyText - Part 1</a></p>
  </center>
  

  
    <center> <p> <img src="/images/blog/at_1.jpg" width="100%" /> 
    <br /><small>Attention being computed across a custom-dataset and being dsiplayed in TensorBoard!</small></p> </center>
  

  <div class="info">
    
			<center><p> The code used for this blog-post is <a href="https://github.com/Braneshop/pytext">here</a>. </p></center>
    
  </div>

  <p>I wanted to take some time to discuss a library that seems to have flown under the radar a little bit. It’s called <a href="https://github.com/facebookresearch/pytext">PyText</a>, and we’re going to explore how you can use it to run state of the art attention models!</p>
<p>Here’s our plan:</p>
<ol style="list-style-type: decimal">
<li>Set a dataset,</li>
<li>See how we prepare the data for training in PyText,</li>
<li>Get PyText,</li>
<li>Train it,</li>
<li>Visualise it,</li>
<li>Consider extensions,</li>
</ol>
<p>Finally, we’ll jump in to a fork of PyText where I’ve added a few features that help out during training.</p>
<p>If you want to follow along, all the code for this post is available <a href="%22%22">here</a>.</p>
<h4 id="the-dataset">The Dataset</h4>
<p>I’m going to play around with my favourite dataset; <a href="https://scirate.com/noonsilk/scites">my “scites”</a> on <a href="https://scirate.com">SciRate</a> — i.e. those research papers that I’m interested in.</p>
<p>We’re going to frame this data as a recommendation problem:</p>
<blockquote class="callout">
Given some new research paper, will I be interested in it? And if so, why?
</blockquote>
<p>This will be labelled data. Specifically, we will need two fields: <code>title_and_abstract</code> (a blob of text which is the title of the paper and the abstract concatenated together) and <code>scited</code> (a number, <code>1</code> if I “scited” it, <code>0</code> if I didn’t).</p>
<p>Here’s an example, in Python:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">title_and_abstract <span class="op">=</span> <span class="st">&quot;&quot;&quot;Generalized criteria of symmetry breaking. A strategy</span>
<span class="st">for quantum time crystals. The aim of this paper is to propose a criterion</span>
<span class="st">of spontaneous symmetry breaking that makes reference to the properties of</span>
<span class="st">pure phases defined by a translationally invariant state. By avoiding</span>
<span class="st">any reference to the ground state, at the basis of the standard</span>
<span class="st">approach, this criterion applies to a wider class of models. An</span>
<span class="st">interesting application is the breaking of time translations. Indeed, we</span>
<span class="st">discuss explicit theoretical models which exhibit the prototypical</span>
<span class="st">features of quantum time crystals, without the need of a time-dependent</span>
<span class="st">Hamiltonian.&quot;&quot;&quot;</span>
scited <span class="op">=</span> <span class="dv">0</span>
arxiv_id <span class="op">=</span> <span class="st">&quot;1906.12293&quot;</span></code></pre></div>
<p>(Note: For reference, we’ll also keep the arXiv id around, so we can cross-check more information, as we need it, by <a href="https://scirate.com/arxiv/1906.12293">visiting the link</a>)</p>
<h5 id="the-dataset---building-it">The Dataset - Building it</h5>
<p>Now, given that I manage the SciRate website, I have direct access to the database. Here’s the kind of dataset I’d like to build:</p>
<ul>
<li>12,000 datapoints of papers that I’ve “scited”,</li>
<li>12,000 datapoints of papers that I’ve opted <em>not</em> to scite,</li>
</ul>
<p>Then, from that set, I’m going to set aside 1.5k for “validation”, and a further 500 that, I promise you now, I will only use at the end of this article, for the purposes of estimating how well the model is doing. In pictures:</p>
<p><img src="/images/blog/pytext-data-split.png" width="800" alt="Data split for
the recommendation problem." title="Data split for the recommendation
problem." /></p>
<p>Okay, so let’s now make three csv’s, <code>train.csv</code>, <code>validation.csv</code> and <code>test.csv</code>. For right now we’ll not look into how PyText wants the data formatted; let’s just assume that if we have these CSVs we can munge it into the right format later.</p>
<p>Here’s what I did:</p>
<pre class="terminal">conda create -n pytext-scirate-recommendation python=3
source activate pytext-scirate-recommendation
pip install jupyter psycopg2
</pre>
<p>Then, in the Jupyter notebook:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">

<span class="im">import</span> psycopg2

my_user_id <span class="op">=</span> <span class="dv">857</span>
max_items  <span class="op">=</span> 12_000

scited_query <span class="op">=</span> <span class="ss">f&quot;&quot;&quot;</span>
<span class="ss">select</span>
<span class="ss">    abstract,</span>
<span class="ss">    title,</span>
<span class="ss">    uid,</span>
<span class="ss">    1 as scited</span>
<span class="ss">from</span>
<span class="ss">    papers</span>
<span class="ss">inner join</span>
<span class="ss">    scites on</span>
<span class="ss">    scites.paper_uid = papers.uid</span>
<span class="ss">    and scites.user_id = </span><span class="sc">{</span>my_user_id<span class="sc">}</span>
<span class="ss">where</span>
<span class="ss">    papers.submit_date &gt; &#39;01-Jan-2016&#39;</span>
<span class="ss">order</span>
<span class="ss">    by random()</span>
<span class="ss">limit</span>
<span class="ss">    </span><span class="sc">{</span>max_items<span class="sc">}</span>
<span class="ss">&quot;&quot;&quot;</span>

not_scited_query <span class="op">=</span> <span class="ss">f&quot;&quot;&quot;</span>
<span class="ss">select</span>
<span class="ss">    abstract,</span>
<span class="ss">    title,</span>
<span class="ss">    uid,</span>
<span class="ss">    0 as scited</span>
<span class="ss">from</span>
<span class="ss">    papers</span>
<span class="ss">left outer join</span>
<span class="ss">    scites on</span>
<span class="ss">    scites.paper_uid = papers.uid</span>
<span class="ss">    and scites.user_id = </span><span class="sc">{</span>my_user_id<span class="sc">}</span>
<span class="ss">where</span>
<span class="ss">    scites.id is null</span>
<span class="ss">    and</span>
<span class="ss">    papers.submit_date &gt; &#39;01-Jan-2016&#39;</span>
<span class="ss">order</span>
<span class="ss">    by random()</span>
<span class="ss">limit</span>
<span class="ss">    </span><span class="sc">{</span>max_items<span class="sc">}</span>
<span class="ss">&quot;&quot;&quot;</span>

<span class="cf">with</span> psycopg2.<span class="ex">connect</span>(<span class="st">&quot;dbname=scirate&quot;</span>) <span class="im">as</span> connection:
    cursor <span class="op">=</span> connection.cursor()
    
    cursor.execute(scited_query)
    scited <span class="op">=</span> cursor.fetchall()

    cursor.execute(not_scited_query)
    not_scited <span class="op">=</span> cursor.fetchall()

<span class="cf">assert</span> <span class="bu">len</span>(scited) <span class="op">==</span> <span class="bu">len</span>(not_scited) <span class="op">==</span> max_items

train <span class="op">=</span> scited[:10_000]       <span class="op">+</span> not_scited[:10_000]
val   <span class="op">=</span> scited[10_000:11_500] <span class="op">+</span> not_scited[10_000:11_500]
test  <span class="op">=</span> scited[11_500:]       <span class="op">+</span> not_scited[11_500:]

<span class="cf">assert</span> <span class="bu">len</span>(train) <span class="op">==</span> 20_000
<span class="cf">assert</span> <span class="bu">len</span>(val)   <span class="op">==</span>  3_000
<span class="cf">assert</span> <span class="bu">len</span>(test)  <span class="op">==</span>  1_000

<span class="im">import</span> pandas <span class="im">as</span> pd

<span class="kw">def</span> write_csv (rows, name):
    df <span class="op">=</span> pd.DataFrame(rows, columns<span class="op">=</span>[<span class="st">&quot;abstract&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;arxiv_id&quot;</span>, <span class="st">&quot;scited&quot;</span>])
    df[<span class="st">&quot;abstract_and_title&quot;</span>] <span class="op">=</span> (df[<span class="st">&quot;title&quot;</span>] <span class="op">+</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> df[<span class="st">&quot;abstract&quot;</span>])<span class="op">\</span>
                                .replace(<span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>, <span class="st">&quot; &quot;</span>)<span class="op">\</span>
                                .replace(<span class="st">&quot;</span><span class="ch">\r</span><span class="st">&quot;</span>, <span class="st">&quot; &quot;</span>)<span class="op">\</span>
                                .replace(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="st">&quot; &quot;</span>)
    df <span class="op">=</span> df.drop(columns<span class="op">=</span>[<span class="st">&quot;abstract&quot;</span>, <span class="st">&quot;title&quot;</span>])
    
    df.to_csv(name <span class="op">+</span> <span class="st">&quot;.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>, header<span class="op">=</span><span class="va">False</span>, sep<span class="op">=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)


write_csv(train, <span class="st">&quot;../data/train&quot;</span>)
write_csv(val,   <span class="st">&quot;../data/val&quot;</span>)
write_csv(test,  <span class="st">&quot;../data/test&quot;</span>)</code></pre></div>
<p>This is just a little hacked-together bit of code that grabs out papers that I’ve definitely scited, and those for which I haven’t scited. Note that it limits it to those papers from the last 5 years, and also randomly orders them. If we wanted to be a bit more precise, we could do the random ordering according to a seed, but we’re just in a blog post here, so we can be a bit related :)</p>
<p>Note that this code requires the SciRate database, so you can’t run it, but here’s the output: <a href="https://drive.google.com/file/d/16HIXwe_8MP2fXm_aqrHCdkLZzpjBFwMr/view?usp=sharing">noons scites - download</a> if you want to follow along with this exact dataset.</p>
<p>Great! Now I have my data in the <code>data</code> folder:</p>
<pre class="terminal">
04:41 PM noon ∈ pytext-scirate-recommendation ls -alh data 
total 26M
drwxrwxr-x 4 noon noon 4.0K Jul  3 17:09 .
drwxrwxr-x 7 noon noon 4.0K Jul  2 08:26 ..
-rw-rw-r-- 1 noon noon 1.1M Jul  3 14:47 test.csv
-rw-rw-r-- 1 noon noon  22M Jul  3 14:47 train.csv
-rw-rw-r-- 1 noon noon 3.2M Jul  3 14:47 val.csv
</pre>
<p>The sizes look good (i.e. val is smaller than train, and test is smaller still, etc, so my script works!) Let’s continue on.</p>
<h4 id="preparing-the-data-for-pytext">Preparing the data for PyText</h4>
<p>Okay, now that we have <em>some</em> data, let’s see what format PyText wants. Let’s head over to the <a href="https://pytext.readthedocs.io/en/master/">PyText Documentation</a>.</p>
<h5 id="turns-out-we-have-to-first-pick-a-model">… turns out we have to first pick a model</h5>
<p>Glancing at the <a href="https://pytext.readthedocs.io/en/master/train_your_first_model.html">Train your first model</a> we can see that we need to build some kind of configuration object; and that in particular we need to decide on a “task” and a “model”. Specifically, we need to build a JSON file like so:</p>
<div class="sourceCode"><pre class="sourceCode json"><code class="sourceCode json"><span class="fu">{</span>
  <span class="dt">&quot;version&quot;</span><span class="fu">:</span> <span class="dv">8</span><span class="fu">,</span>
  <span class="dt">&quot;task&quot;</span><span class="fu">:</span> <span class="fu">{</span>
    <span class="dt">&quot;DocumentClassificationTask&quot;</span><span class="fu">:</span> <span class="fu">{</span>
      <span class="dt">&quot;data&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;source&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;TSVDataSource&quot;</span><span class="fu">:</span> <span class="fu">{</span>
            <span class="dt">&quot;field_names&quot;</span><span class="fu">:</span> <span class="ot">[</span><span class="st">&quot;label&quot;</span><span class="ot">,</span> <span class="st">&quot;slots&quot;</span><span class="ot">,</span> <span class="st">&quot;text&quot;</span><span class="ot">]</span><span class="fu">,</span>
            <span class="dt">&quot;train_filename&quot;</span><span class="fu">:</span> <span class="st">&quot;pytext/tests/data/train_data_tiny.tsv&quot;</span><span class="fu">,</span>
            <span class="dt">&quot;test_filename&quot;</span><span class="fu">:</span> <span class="st">&quot;pytext/tests/data/test_data_tiny.tsv&quot;</span><span class="fu">,</span>
            <span class="dt">&quot;eval_filename&quot;</span><span class="fu">:</span> <span class="st">&quot;pytext/tests/data/test_data_tiny.tsv&quot;</span>
          <span class="fu">}</span>
        <span class="fu">}</span>
      <span class="fu">},</span>
      <span class="dt">&quot;model&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;DocModel&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;representation&quot;</span><span class="fu">:</span> <span class="fu">{</span>
            <span class="dt">&quot;DocNNRepresentation&quot;</span><span class="fu">:</span> <span class="fu">{}</span>
          <span class="fu">}</span>
        <span class="fu">}</span>
      <span class="fu">}</span>
    <span class="fu">}</span>
  <span class="fu">}</span>
<span class="fu">}</span></code></pre></div>
<p>So, let’s think out our problem here as a “classification” problem. We have two independent classes, “scited” and “not scited”.</p>
<p>Now, glancing at the <a href="https://github.com/facebookresearch/pytext#overview">PyText classifiers</a> that they have set up by default, the one I want to use is this one: <a href="https://arxiv.org/abs/1703.03130">Lin et al. (2017): A Structured Self-attentive Sentence Embedding</a>. I’m picking this one because, firstly, it’s got this notion of <em>attention</em>. Scrolling through the paper, this is the kind of output we can expect to get from this paper:</p>
<center>
<img src="/images/blog/attention.jpg" width="900" title="Attention in the sentence embedding paper" alt="Attention in the sentence-embedding paper." />
</center>
<p>Looks cool! It’s certainly the idea we’re going for — a kind of “explanation” — the relevant words are highlighted (with a strength!) indicating why the decision (here, a 1-star review) was made.</p>
<p>Now, glancing at the documentation for the config file, our <code>&quot;model&quot;</code> section will need to be like this:</p>
<div class="sourceCode"><pre class="sourceCode json"><code class="sourceCode json"><span class="fu">{</span>
  <span class="dt">&quot;model&quot;</span><span class="fu">:</span> <span class="fu">{</span>
    <span class="dt">&quot;representation&quot;</span><span class="fu">:</span> <span class="fu">{</span>
      <span class="dt">&quot;BiLSTMDocAttention&quot;</span><span class="fu">:</span> <span class="fu">{}</span>
    <span class="fu">}</span>
  <span class="fu">}</span>
<span class="fu">}</span></code></pre></div>
<p>Going back to the above config, we note that we don’t have a “TSV” (Tab-seperated values) we have a CSV, so we might need something like this:</p>
<div class="sourceCode"><pre class="sourceCode json"><code class="sourceCode json"><span class="fu">{</span>
  <span class="dt">&quot;DocumentClassificationTask&quot;</span><span class="fu">:</span> <span class="fu">{</span>
    <span class="dt">&quot;data&quot;</span><span class="fu">:</span> <span class="fu">{</span>
      <span class="dt">&quot;source&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;TSVDataSource&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;field_names&quot;</span><span class="fu">:</span>    <span class="ot">[</span><span class="st">&quot;arxiv_id&quot;</span><span class="ot">,</span> <span class="st">&quot;label&quot;</span><span class="ot">,</span> <span class="st">&quot;text&quot;</span><span class="ot">]</span><span class="fu">,</span>
          <span class="dt">&quot;train_filename&quot;</span><span class="fu">:</span> <span class="st">&quot;./data/train.csv&quot;</span><span class="fu">,</span>
          <span class="dt">&quot;test_filename&quot;</span><span class="fu">:</span>  <span class="st">&quot;./data/test.csv&quot;</span><span class="fu">,</span>
          <span class="dt">&quot;eval_filename&quot;</span><span class="fu">:</span>  <span class="st">&quot;./data/val.csv&quot;</span>
        <span class="fu">}</span>
      <span class="fu">}</span>
    <span class="fu">}</span>
  <span class="fu">}</span>
<span class="fu">}</span></code></pre></div>
<p>(Note: the <code>field_names</code> needs to match the order in the CSV, and furthermore, hte fields “label” and “text” are used because they correspond to the names <em>in pytext code</em>, not because they are the names of the columns in our document. Further note: <a href="https://github.com/facebookresearch/pytext/issues/747">there’s a bug</a> where the TCV-parser doesn’t respect quoted strings with the seperator in them.)</p>
<p>Okay. Ambitiously, let’s just try putting everything together, and running it! I’m going to call it <code>02-Jul-2019-Run-1.json</code> and it will have:</p>
<div class="sourceCode"><pre class="sourceCode json"><code class="sourceCode json"><span class="fu">{</span>
  <span class="dt">&quot;version&quot;</span><span class="fu">:</span> <span class="dv">8</span><span class="fu">,</span>
  <span class="dt">&quot;task&quot;</span><span class="fu">:</span> <span class="fu">{</span>
    <span class="dt">&quot;DocumentClassificationTask&quot;</span><span class="fu">:</span> <span class="fu">{</span>
      <span class="dt">&quot;data&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;source&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;TSVDataSource&quot;</span><span class="fu">:</span> <span class="fu">{</span>
            <span class="dt">&quot;field_names&quot;</span><span class="fu">:</span>    <span class="ot">[</span><span class="st">&quot;arxiv_id&quot;</span><span class="ot">,</span> <span class="st">&quot;label&quot;</span><span class="ot">,</span> <span class="st">&quot;text&quot;</span><span class="ot">]</span><span class="fu">,</span>
            <span class="dt">&quot;train_filename&quot;</span><span class="fu">:</span> <span class="st">&quot;./data/train.csv&quot;</span><span class="fu">,</span>
            <span class="dt">&quot;test_filename&quot;</span><span class="fu">:</span>  <span class="st">&quot;./data/test.csv&quot;</span><span class="fu">,</span>
            <span class="dt">&quot;eval_filename&quot;</span><span class="fu">:</span>  <span class="st">&quot;./data/val.csv&quot;</span>
          <span class="fu">}</span>
        <span class="fu">}</span>
      <span class="fu">},</span>
      <span class="dt">&quot;model&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;DocModel&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;representation&quot;</span><span class="fu">:</span> <span class="fu">{</span>
            <span class="dt">&quot;BiLSTMDocAttention&quot;</span><span class="fu">:</span> <span class="fu">{}</span>
          <span class="fu">}</span>
        <span class="fu">}</span>
      <span class="fu">}</span>
    <span class="fu">}</span>
  <span class="fu">}</span>
<span class="fu">}</span></code></pre></div>
<p>(Note: It took me a few goes to get this config right; in the first instance I even forgot to include the labels (“0” or “1”) in the original CSV data! But through the magic of blogging, it looks like I got it right the first time!)</p>
<p>Firstly, I’ll just install <code>pytext</code>!</p>
<pre class="terminal">
pip install pytext-nlp
</pre>
<p>Then, I’ll just run it according to the docs!</p>
<pre class="terminal">
pytext train < config/02-Jul-2019-Run-1.json
</pre>
<p>Now, I’m only running this on my laptop; no GPU, and after 3 epochs (~20 minutes) this is what pops out of the terminal:</p>
<pre class="terminal">
Stage.EVAL
loss: 0.443127
Accuracy: 81.00

Macro P/R/F1 Scores:
+----------------------+-----------+--------+------+---------+
| Label                | Precision | Recall | F1   | Support |
+----------------------+-----------+--------+------+---------+
| 0                    |      0.77 |   0.87 | 0.82 |    1500 |
| 1                    |      0.86 |   0.75 | 0.80 |    1500 |
+----------------------+-----------+--------+------+---------+
| Overall macro scores | 0.82      | 0.81   | 0.81 |         |
+----------------------+-----------+--------+------+---------+

Soft Metrics:
+-------+-------------------+---------+
| Label | Average precision | ROC AUC |
+-------+-------------------+---------+
|     0 |             0.884 |   0.893 |
|     1 |             0.894 |   0.893 |
+-------+-------------------+---------+

Recall at Precision
+-------+---------+---------+---------+---------+---------+
| Label | R@P 0.2 | R@P 0.4 | R@P 0.6 | R@P 0.8 | R@P 0.9 |
+-------+---------+---------+---------+---------+---------+
| 0     |   1.000 |   1.000 |   0.987 |   0.843 |   0.551 |
| 1     |   1.000 |   1.000 |   0.978 |   0.833 |   0.624 |
+-------+---------+---------+---------+---------+---------+

Matthews correlation coefficient: 0.625

ROC AUC: 0.893
</pre>
<p>This says that this model got ~80% of the classifications right, and then has a few more details. In any case, this is pretty great!</p>
<p>So, there’s a few directions we can head in, now. Certainly it would be useful to see the training runs in TensorBoard. It would also be great to be able to run inference on a single title and abstract. We could also go into the weeds and try and tweak the model, run it on a GPU, and other such things, to see how far we can push the accuracy up.</p>
<p>Let’s go for the TensorBoard/visualisation option, as it’ll be a useful thing to have, no matter which further steps we want to take.</p>
<h4 id="visualising-the-training">Visualising the training</h4>
<p>It’s no coincidence that I’m choosing TensorBoard here, as this is something that <a href="https://pytext.readthedocs.io/en/master/visualize_your_model.html">PyText supports out of the box</a>. So let’s follow along. First, we install it:</p>
<pre class="terminal">
pip install tensorboard
</pre>
<p>Observe that there is a <code>runs</code> folder with some data in it:</p>
<pre class="terminal">
(pytext-scirate-recommendation) 11:04 AM noon ∈ pytext-scirate-recommendation ls runs
Jul02_08-26-12_gpac  Jul02_08-31-57_gpac  Jul02_10-18-55_gpac  Jul02_10-33-37_gpac
</pre>
<p>Then, we just run TensorBoard!</p>
<pre class="terminal">
tensorboard --logdir runs
</pre>
<p>(Note: I got a warning that I hadn’t installed <code>tensorflow</code>, but it didn’t cause me any problems.)</p>
<p>Here’s what I see:</p>
<center>
<img src="/images/blog/tensorboard-pytext.png" width="900" title="TensorBoard graphs for our previous training run." alt="TensorBoard graphs for our previous training run." />
</center>
<p>Of course, we didn’t run for many epochs, so we don’t see much improvement. (Perhaps it’s also worth noting that after just 1 epoch we were already at 70% accuracy).</p>
<p>Now, this is all well-and-good, but we want something a bit more interesting. At the top of this article, we were promised <em>attention</em>; i.e. that we could see what words were contributing to the predictions. Wouldn’t it be cool if we could <em>see</em> this in TensorBoard?! Yes, is the answer!</p>
<h4 id="building-attention-visualisation-into-tensorboard">Building attention visualisation into TensorBoard</h4>
<p>In order to do this, we’re going to have to get funky with the pytext source code (unfortunately, this isn’t a feature that’s supported out-of-the-box.)</p>
<p>This is our process:</p>
<ol style="list-style-type: decimal">
<li>Work out how to get attention out of the model,</li>
<li>Pick a few fixed samples that we run through the network during evaluation,</li>
<li>Somehow coax TensorBoard to display those samples with attention.</li>
</ol>
<p>So, if you’re happy to follow along, first, get yourself a copy of the pytext codebase:</p>
<pre class="terminal">
git clone https://github.com/facebookresearch/pytext.git
</pre>
<p>Now, because we’re going to hack on this local version, instead of the version we’ve installed, you’ll want to uninstall the pytext we installed before, (from within your conda environment), and then jump into your copy of the pytext repo, and install <em>that</em> in “develop” mode, so we can make code changes and have them reflected:</p>
<pre class="terminal">
pip uninstall pytext-nlp

# Jump to the folder where you cloned pytext
python setup.py develop
</pre>
<p>Then, let’s dive in.</p>
<h4 id="getting-attention-out-of-the-model">1. Getting attention out of the model</h4>
<p>We recall we’re using the <code>BiLSTMDocAttention</code> “representation” (remember, this is what we specified in our JSON configuration file). Here, “representation” is the same as an “embedding”, namely, a way of encoding a document as a vector. So, we know that the code related to this thing, should match the description of the model in the <a href="https://arxiv.org/pdf/1703.03130.pdf">paper</a>. This will help us working out which tensor is the attention, in the code.</p>
<p>Now, please note — I’ve looked into this before, way back when I was playing with this repo around 8 months ago (in a previous job); <a href="https://github.com/facebookresearch/pytext/issues/143">I opened an issue at the time</a>, and it’s still open. So, we’ll attempt to use this blog post as an opportunity to try and make a PR that fixes this problem.</p>
<p>As a result, I know that the first thing to look for is if the relevant parts of the code do two things: 1) expose the appropriate attention tensor, so that it can be used by us to evaluate the attention across the input and 2) <a href="https://github.com/facebookresearch/pytext/issues/173">correctly implement the “hops” functionality from the paper</a>, so that we get the full power of the attention that the paper (supposedly!) provides.</p>
<p>We’ll address the first item first, as it should be independent of the second. We’ll leave the second item for</p>
<h5 id="so-how-do-we-get-the-attention-out-and-display-it">So, how do we get the attention out, and display it?</h5>
<p>Unfortunately, it turned out to be an involved process. I ended up spending the better part of a day hacking around to make this work. It’s all captured in a branch which you can view here: <a href="https://github.com/BraneShop/pytext/compare/master...BraneShop:attention-hacking">attention-hacking branch</a>. It’s not super elegant, but it’s enough to keep us going.</p>
<p>Here’s the final config file I ended up using:</p>
<div class="sourceCode"><pre class="sourceCode json"><code class="sourceCode json"><span class="fu">{</span>
  <span class="dt">&quot;version&quot;</span><span class="fu">:</span> <span class="dv">8</span><span class="fu">,</span>
  <span class="dt">&quot;task&quot;</span><span class="fu">:</span> <span class="fu">{</span>
    <span class="dt">&quot;DocumentClassificationTask&quot;</span><span class="fu">:</span> <span class="fu">{</span>
      <span class="dt">&quot;trainer&quot;</span><span class="fu">:</span>  <span class="fu">{</span> <span class="dt">&quot;epochs&quot;</span><span class="fu">:</span> <span class="dv">20</span> <span class="fu">},</span>
      <span class="dt">&quot;data&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;source&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;TSVDataSource&quot;</span><span class="fu">:</span> <span class="fu">{</span>
            <span class="dt">&quot;field_names&quot;</span><span class="fu">:</span>    <span class="ot">[</span><span class="st">&quot;arxiv_id&quot;</span><span class="ot">,</span> <span class="st">&quot;label&quot;</span><span class="ot">,</span> <span class="st">&quot;text&quot;</span><span class="ot">]</span><span class="fu">,</span>
            <span class="dt">&quot;train_filename&quot;</span><span class="fu">:</span> <span class="st">&quot;./data/train.csv&quot;</span><span class="fu">,</span>
            <span class="dt">&quot;test_filename&quot;</span><span class="fu">:</span>  <span class="st">&quot;./data/test.csv&quot;</span><span class="fu">,</span>
            <span class="dt">&quot;eval_filename&quot;</span><span class="fu">:</span>  <span class="st">&quot;./data/val.csv&quot;</span>
          <span class="fu">}</span>
        <span class="fu">}</span>
      <span class="fu">},</span>
      <span class="dt">&quot;model&quot;</span><span class="fu">:</span> <span class="fu">{</span>
        <span class="dt">&quot;DocModel&quot;</span><span class="fu">:</span> <span class="fu">{</span>
          <span class="dt">&quot;decoder&quot;</span><span class="fu">:</span> <span class="fu">{</span> <span class="dt">&quot;has_attention&quot;</span><span class="fu">:</span> <span class="kw">true</span> <span class="fu">},</span>
          <span class="dt">&quot;representation&quot;</span><span class="fu">:</span> <span class="fu">{</span>
            <span class="dt">&quot;BiLSTMDocAttention&quot;</span><span class="fu">:</span> <span class="fu">{}</span>
          <span class="fu">}</span>
        <span class="fu">}</span>
      <span class="fu">}</span>
    <span class="fu">}</span>
  <span class="fu">}</span>
<span class="fu">}</span></code></pre></div>
<p>(The main thing is that I added a new config option, <code>has_attention</code>, that lets me introduce this extra functionality without (hopefully!) breaking anything.)</p>
<p>I won’t bore you with all the details (you can glance at the diff above to see what I actually changed; it’s not a lot), but the main highlights were:</p>
<ul class="normal">
<li>
Try not to break everything,
</li>
<li>
Pass the attention around so that it can be logged to TensorBoard,
</li>
<li>
Hack TensorBoard a bit and log an <em>image</em> instead of text, so that highlighting can be displayed (See <a href="https://github.com/tensorflow/tensorboard/issues/1740#issuecomment-507766667">this issue</a> for why I had to do this.)
</li>
</ul>
<p>The final thing looks like so, in TensorBoard:</p>
<center>
<img src="/images/blog/at_1.jpg" width="900" /> <br /> <img src="/images/blog/at_2.jpg" width="900" />
</center>
<p>This has the important idea. Namely, the “important” words coloured in. There’s also the tokens (<code>__UNKNOWN__</code> and <code>__PAD__</code>) that PyText adds, automatically, to deal with low-frequency words and the fact that we need to feed in fixed-length sequences to get the benefits of fast computation on the GPU.</p>
<p>In the next post (this took me so long I had to make it two parts!) we’ll look at the remaining items: Fixing the implementation to match the paper; working out if the attention information it’s providing is even sensible, consider some extensions, and building a little “production-ready” interface!</p>
<hr />
<p>As always, if you have any feedback/corrections/comments then get in <a href="/contact.html">touch</a>!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Venue for the first workshop - BlueRock!</title>
    <link href="https://braneshop.com.au/posts/2019-06-28-First-workshop-at-Blue-Rock.html" />
    <id>https://braneshop.com.au/posts/2019-06-28-First-workshop-at-Blue-Rock.html</id>
    <published>2019-06-28T00:00:00Z</published>
    <updated>2019-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on June 28, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Venue for the first workshop - BlueRock!</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a></span>
  

  

  
    <center> <p> <img src="/images/blog/BlueRock.jpg" width="100%" /> 
    <br /><small>The <a href="https://thebluerock.com.au/" rel="noopener">BlueRock</a> offices.</small></p> </center>
  

  <div class="info">
    
  </div>

  <p>We’re very grateful to the very nice people at <a
href="https://thebluerock.com.au/" rel="noopener">BlueRock</a> (be sure to check out their website!) who have agreed to be our venue hosts for the upcoming <a
href="/6-week-workshop-on-deep-learning.html">6 week technical deep learning workshop</a>, kicking off on the 8th of August!</p>
<p>The office is right next to <a
href="https://www.google.com.au/maps/place/BlueRock/@-37.811281,144.9549693,17z/data=!3m1!4b1!4m5!3m4!1s0x6ad65d4b1f638f5d:0x4a6c97cd2eb48059!8m2!3d-37.811281!4d144.957158"
rel="noopener">flag staff station</a>, so it’s a very convenient location for us. It’s a nice big board room that we’ll be able to arrange in a nice way for our group-style format.</p>
<p>We look forward to seeing you there!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Deep learning quick start</title>
    <link href="https://braneshop.com.au/posts/2019-06-21-Deep-learning-quick-start.html" />
    <id>https://braneshop.com.au/posts/2019-06-21-Deep-learning-quick-start.html</id>
    <published>2019-06-21T00:00:00Z</published>
    <updated>2019-06-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on June 21, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Deep learning quick start</h4>

  
    <span class="tags">Tags: <a href="/tags/tools.html">tools</a></span>
  

  

  

  <div class="info">
    
			<center><p> The code used for this blog-post is <a href="https://github.com/Braneshop/quickstart-elm-app">here</a>. </p></center>
    
  </div>

  <p>It’s been in vogue for some time now to have a little “quick-start” install on your website. Notable examples are:</p>
<ul class="normal">
<li>
The <a href="https://spacy.io/usage" rel="noopener">spaCy</a> website,
</li>
<li>
<a href="https://pytorch.org/" rel="noopener">PyTorch</a>.
</li>
</ul>
<p>My friend Hayden suggested that it might be nice to have a slightly higher-level one, so I made it: <a href="/quickstart.html">Braneshop deep learning quick start</a>, and you can find a link to it at the bottom of the page.</p>
<p>The main thing I wanted to get across, for whatever it’s worth, is that one of your best options is <a href="https://colab.research.google.com/">Google Colaboratory</a>. The other thing I did was to just collect together the information from the TensorFlow and the PyTorch installation pages.</p>
<p>I actually had a bit of fun because I made it in Elm. You can find it at the link at the top, or <a href="https://github.com/Braneshop/quickstart-elm-app"
rel="noopener">here</a>. It’s not the most elegant thing you’ll ever see, but it was fun to play around with.</p>
<p>Let me know if it’s missing any paths you think are interesting! Over time I’ll hopefully add more to it, but for now I hope it helps some one, even if it just inspires you to try something in Elm!</p>
<p>Note that this isn’t intended as a comprehensive quickstart guide, more just a simple little widget to select the right installation commands for whatever deep learning environment you want to use.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Ruth joins Braneshop, and new workshop launch!</title>
    <link href="https://braneshop.com.au/posts/2019-06-09-Ruth-joins-Braneshop-and-Workshop-Launch.html" />
    <id>https://braneshop.com.au/posts/2019-06-09-Ruth-joins-Braneshop-and-Workshop-Launch.html</id>
    <published>2019-06-09T00:00:00Z</published>
    <updated>2019-06-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on June  9, 2019
    
      by Ruth Pearson
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Ruth joins Braneshop, and new workshop launch!</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a></span>
  

  

  
    <center> <p> <img src="/images/ruth-whiteboard.jpg" width="100%" /> 
    <br /><small>Me (Ruth!) teaching neural networks at the <a href="http://womeninai.com.au">Women in AI</a> leadership course in Brisbane, 2019.</small></p> </center>
  

  <div class="info">
    
  </div>

  <p>Hello friends!</p>
<p>This is your first Braneshop blog post from me, and I’m very excited to be writing it to tell you about our first <a href="/6-week-workshop-on-deep-learning.html">6 Week Technical Deep Learning Workshop</a> scheduled to start on the 8th of August, which is <a href="https://events.humanitix.com.au/braneshop-6-week-technical-deep-learning-workshop">on sale now!</a></p>
<p>(Not to mention our first <a href="/ai-for-leadership.html">AI for Leadership</a> workshop which is also on sale now, and our <a href="/deep-learning-workshop.html">Intensive Technical Deep Learning Workshop</a> which we can bring to your organisation — <a href="https://noonvandersilk.typeform.com/to/DYKvWN">get in touch!</a>)</p>
<p>I recently joined my friend Noon at Braneshop as an instructor, here’s why:</p>
<ol style="list-style-type: decimal">
<li><p>I love demystifying science into simple concepts to make it fun and accessible to people like you.</p></li>
<li><p>As a queer woman in the AI industry, I don’t see many people who look like me and I know how intimidating that can be. I’m here to help people from diverse backgrounds and be a part of the platform that elevates their skills and representation.</p></li>
<li><p>Noon has a lot of experience teaching technical content and we love working, learning and collaborating together.</p></li>
</ol>
<p>I come from an academic physics background and I learnt code in order to do my research. This commonly leads to sloppy coding and bad habits. I can’t say I’m completely cured (the physicist will be forever inside me!) but five years of working in tech has got me mostly straightened out in terms of writing nice code. More importantly, from working in data science and AI I have learnt the tools, tips and tricks that make the job easy and fun.</p>
<p>Academia is just one way into AI, there are actually as many ways as you could care to count. This is fantastic news, since it is so important to bring a diverse and interdisciplinary approach to AI tools which are becoming so prevalent in our world. There are lots of great resources online, and many of you will already be using AI in your lives and work, whether you realise it or not. Whichever path has brought you to this interest in AI, our workshop will give you hands on learning and leave you more confident to continue in the industry. My colleague and friend, Noon, has a very strong tech background, so between the two of us we’ve got all your needs catered for in our workshop.</p>
<p>In fact, many concepts we will cover in the workshop once felt foreign and scary to me, I picked them up bit by bit. And so can you. They actually aren’t scary at all, you just need someone to explain them to you in a logical way, starting from the basics. That’s what we’re here for.</p>
<p>It’s surprising how much you realise you know when someone is willing to join the dots for you. We’re ready to be your dot joiners! Come! Bring your dots! Let us join them!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Deep learning in the browser!</title>
    <link href="https://braneshop.com.au/posts/2019-04-10-Deep-learning-in-the-browser-at-the-web-meetup.html" />
    <id>https://braneshop.com.au/posts/2019-04-10-Deep-learning-in-the-browser-at-the-web-meetup.html</id>
    <published>2019-04-10T00:00:00Z</published>
    <updated>2019-04-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on April 10, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Deep learning in the browser!</h4>

  
    <span class="tags">Tags: <a href="/tags/talk.html">talk</a>, <a href="/tags/tensorflowjs.html">tensorflowjs</a></span>
  

  

  

  <div class="info">
    
			<center><p> The code used for this blog-post is <a href="https://github.com/silky/tfjs-fashion-mnist-classifier">here</a>. </p></center>
    
  </div>

  <p>Last night I gave a talk at <a href="https://www.meetup.com/the-web/events/260211393/">The Web Meetup</a> on deep learning in the browser. It was mostly about how to deploy models built TensorFlow to the browser, by way of TensorFlow.js, but we did manage to try a model on the <a href="https://colab.research.google.com/">Google Colaboratory</a>, and deploy it to the browser all in the space of about 5 minutes!</p>
<p>We also discussed a few of the items that I brought up in my recent post about <a href="/posts/2019-02-08-TensorFlowJS-How-to-easily-deploy-deep-learning-models.html">deploying models via TensorFlow.js</a>.</p>
<center>
<img src="/images/blog/wmu-1.jpg" /> <br /> Deep learning in the browser! - Presentation at the April Web Meetup.
</center>
<p>Here are the links to the presentation and the code:</p>
<ul class="normal">
<li>
Presentation - <a href="https://docs.google.com/presentation/d/1_px6paltT1ZHZPKKTK_o-8KWKppremKcwe-5GY94kos/edit">Deep learning in the browser!</a>
</li>
<li>
Online Demo - <a href="https://silky.github.io/tfjs-fashion-mnist-classifier/index.html">tfjs fashion mnist classifier</a>
<li>
Code - <a href="https://github.com/silky/tfjs-fashion-mnist-classifier" class="uri">https://github.com/silky/tfjs-fashion-mnist-classifier</a>
</li>
</ul>
<p>Some of the discussion points from the audience were:</p>
<ul class="normal">
<li>
Q: What are some real-world use-cases of TensorFlow.js? (A: I haven’t seen many applications yet, but they’re sure to exist!)
</li>
<li>
Q: What do you think the scope is for new applications of AI in the browser? (A: Heaps! We need to play with the tooling to find out what they are)
</li>
<li>
Q: Do the models need to be significantly smaller to run in the browser? (A: Depends if you have a GPU when you run it; but not generally, no. A bit smaller, yes.
</li>
</ul>
<p>We wrapped up with a few thoughts around the future of UX and AI. Ultimately, I think this is going to be a field of significant growth and importance, so now’s the time to get into it, if it’s of interest to you!</p>
<hr />
<p>If you’re looking to adopt AI in your organisation, then check out our upcoming <a href="/ai-for-leadership.html">AI For Leadership</a> course!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Australian Human Rights Commission AI - Submission</title>
    <link href="https://braneshop.com.au/posts/2019-03-14-HRC-AI-Submission.html" />
    <id>https://braneshop.com.au/posts/2019-03-14-HRC-AI-Submission.html</id>
    <published>2019-03-14T00:00:00Z</published>
    <updated>2019-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on March 14, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Australian Human Rights Commission AI - Submission</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a>, <a href="/tags/ethics.html">ethics</a></span>
  

  

  

  <div class="info">
    
  </div>

  <p>At Braneshop we’re keen to see AI develop positively in our community. The rules and regulation around AI will play a key role in ensuring that the AI that is ultimately built and used ends up creating good community outcomes.</p>
<p>The Australian Human Rights Commission recently put out a <a href="https://tech.humanrights.gov.au/consultation">call for consultation on AI - Governance and Leadership</a>. The deadline for a submission is the 18th of March at 5pm (so there’s still time if you’re quick!)</p>
<p>Working with Martin and Sam at <a href="https://www.northraine.com/">Northraine</a> and other interested parties, we prepared a joint submission, which you can read here: <a href="/files/HRC_Submission.pdf">Our HRC Submission</a>.</p>
<p>One really important message regarding ethics and AI, that I’ve seen recently, comes from the following papers: <a href="https://scirate.com/arxiv/1903.03425">The Ethics of AI Ethics – An Evaluation of Guidelines</a>.</p>
<p>Quoting the author:</p>
<blockquote>
<p>“Ethics is then no longer understood as a deontologically inspired tick-box exercise, but as a project of advancing personalities, changing attitudes, strengthen responsibilities and gaining courage to refrain from certain actions, which are deemed unethical. […] <strong>It should not be the objective of ethics to stifle activity, but to do the exact opposite</strong>, i.e. broadening the scope of action, uncovering blind spots, promoting autonomy and freedom, and fostering self-responsibility.” - <small>Thilo Hagendorff</small></p>
</blockquote>
<p>Emphasis mine.</p>
<p>This is a crucial and important thought when considereding any kind of “normative”, rule-based, punishment-driven acitivies, over those that enrich, embolden and allow for, and encourage, the growth of people and communities.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Mindful Neural Networks</title>
    <link href="https://braneshop.com.au/posts/2019-03-06-Mindful-Neural-Networks.html" />
    <id>https://braneshop.com.au/posts/2019-03-06-Mindful-Neural-Networks.html</id>
    <published>2019-03-06T00:00:00Z</published>
    <updated>2019-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on March  6, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Mindful Neural Networks</h4>

  
    <span class="tags">Tags: <a href="/tags/announcement.html">announcement</a>, <a href="/tags/creative-ai.html">creative-ai</a></span>
  

  

  

  <div class="info">
    
  </div>

  <p>Last night at the <a href="https://www.meetup.com/Melbourne-Creative-AI-Meetup/">Creative AI Meetup</a> we had an awesome conversation around the idea of “Mindful” neural networks.</p>
<center>
<img width="600" src="/images/blog/thich-nhat-tanh.png" /> <br /> <i>Thich Nhat Tanh</i>
</center>
<p>We were hosted by our friends at <a href="https://sheda.ltd">Sheda</a> over the <a href="https://neighbourhood.work">Neighbourhood.work</a> in Fitzroy, and we had a really nice group of people with varied and interesting backgrounds show up:</p>
<center>
<a href="/images/blog/creative-ai-march-2019.jpg"><img width="600" src="/images/blog/creative-ai-march-2019.jpg" /></a> <br /> <i>The Mindful Neural Network Group. Photo courtesy of Eike!</i>
</center>
<p>We did the usual Creative-AI opening where we review a bunch of recent innovations in the field in the last few weeks. You can find the projects we discussed by looking over the slides here: <a href="https://docs.google.com/presentation/d/167A6XXp9NIOb0tPTls51PveyMgKjGEEpKaeospVr54g/edit">Creative AI Meetup - March 2019</a>.</p>
<p>Most notable were the variety of “this X does not exist” websites powered by <a href="https://github.com/NVlabs/stylegan">StyleGAN</a>:</p>
<ul class="normal">
<li>
Person: <a href="https://thispersondoesnotexist.com" class="uri">https://thispersondoesnotexist.com</a>
</li>
<li>
Cat: <a href="https://thiscatdoesnotexist.com" class="uri">https://thiscatdoesnotexist.com</a>
</li>
<li>
Airbnb: <a href="https://thisrentaldoesnotexist.com" class="uri">https://thisrentaldoesnotexist.com</a>
</li>
</ul>
<p>and the <a href="https://github.com/openai/gpt-2">GPT-2 model</a>, which produced the following gem, when prompted with a segment of <a href="https://www.goodreads.com/book/show/687278.When_Things_Fall_Apart">when things fall apart</a>:</p>
<pre><code>Input  &gt;&gt;&gt;

To be fully alive, fully human, and completely awake is to be


Output &gt;&gt;&gt;

completely free from judgement and morality, to have freedom in any worldview
through suffering (with anything that comes from between two extremes can
often be fine), to be absolutely free from stimuli getting in your way,
to be completely free from fear and humiliation, to be completely free
from a series of simple pleasures, to be absolutely free from your own
emotional and social dependence inherent in anything being different
from yours. [...]</code></pre>
<p>We did eventually got around to discussing the mindfulness aspect of neural networks, and what ideas we thought we could transfer over.</p>
<p>As a group, we covered:</p>
<ul class="normal">
<li>
<strong>Attention</strong>: <a href="https://scirate.com/arxiv/1706.03762">Is attention enough?</a> Should we make our networks “aware” of their own thoughts? Is that te same as “attention” as-it’s implemented presently? We discussed briefly this paper: <a href="https://scirate.com/arxiv/1902.10186">Attention is not Explanation</a>.
</li>
<li>
<strong>Anxiety</strong>: We discussed anxiety as “fear of the future”, and considered that perhaps neural nets have no awareness of the future, as they only make judgements on single instance, or alternatively the interpretation that actually neural networks are incredibly anxious as during training that are entirely focused on the future, and how they can change themselves to do better.
</li>
<li>
<strong>Presence</strong>: We wondered whether neural networks are “present” in “the moment”, or if this even makes sense.
</li>
<li>
<strong>Consciousness</strong>: We didn’t go too in-depth on this, but we happened to have a <a href="https://www.linkedin.com/in/rafik-hadfi/">researcher working in this area in the room</a>, so we discussed it on an off. We did discuss our favourite reading on the topic. On free will, mine is this <a href="https://arxiv.org/abs/1306.0159">The Ghost in the Quantum Turing Machine</a>, and on consciouness and thinking it is <a href="https://www.goodreads.com/book/show/123471.I_Am_a_Strange_Loop">I Am a Strange Loop</a>, <a href="https://www.goodreads.com/book/show/24113.G_del_Escher_Bach">Gödel, Escher, Bach</a>, and <a href="https://www.goodreads.com/book/show/7711871-surfaces-and-essences">Surfaces and Essences</a> (the last of which I covered on <a href="https://silky.github.io/posts/2019-01-03-2018-books.html">my 2018 reading list</a>).
</li>
<li>
<strong>Intent</strong>: We talked a little bit about how it’s important to know <em>why</em> a given dataset was constructed, as this might lead to us understanding it’s bias.
</li>
<li>
<strong><a href="https://en.wikipedia.org/wiki/P%C4%81ramit%C4%81">Paramitas</a>/Loss Functions</strong>: We had a moderate amount of discussion around how “small” most neural networks are in their output objective. They take in significantly rich data, but force it to be absolutely 1 or absolutely 0. Maybe there could be richer loss functions we could consider, that would allow for the network to focus in different ways. A recent bit of research along these lines is this: <a href="https://scirate.com/arxiv/1903.01182">Completment Objective Training</a>, i.e. have it make a good prediction of what it is, but also what it is <em>not</em>.
</li>
</ul>
<p>Overall, I had heaps of fun, and there was some discussion of a second installment, as there is heaps to explore here, so stay tuned to the <a href="https://www.meetup.com/Melbourne-Creative-AI-Meetup/">meetup</a> and join us next time!</p>
</div>
]]></summary>
</entry>

</feed>
