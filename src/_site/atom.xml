<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Braneshop | braneshop.com.au</title>
    <link href="https://braneshop.com.au/atom.xml" rel="self" />
    <link href="https://braneshop.com.au" />
    <id>https://braneshop.com.au/atom.xml</id>
    <author>
        <name>Braneshop Team</name>
        <email>noonsilk+-braneshop@gmail.com</email>
    </author>
    <updated>2019-06-09T00:00:00Z</updated>
    <entry>
    <title>Ruth joins Braneshop, and new workshop launch!</title>
    <link href="https://braneshop.com.au/posts/2019-06-09-Ruth-joins-Braneshop-and-Workshop-Launch.html" />
    <id>https://braneshop.com.au/posts/2019-06-09-Ruth-joins-Braneshop-and-Workshop-Launch.html</id>
    <published>2019-06-09T00:00:00Z</published>
    <updated>2019-06-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on June  9, 2019
    
      by Ruth Pearson
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Ruth joins Braneshop, and new workshop launch!</h4>

  
    <center> <p> <img src="/images/ruth-whiteboard.jpg" width="100%" /> 
    <br /><small>Me (Ruth!) teaching neural networks at the <a href="http://womeninai.com.au">Women in AI</a> leadership course in Brisbane, 2019.</small></p> </center>
  

  <div class="info">
    
  </div>

  <p>Hello friends!</p>
<p>This is your first Braneshop blog post from me, and I’m very excited to be writing it to tell you about our first <a href="/6-week-workshop-on-deep-learning.html">6 Week Technical Deep Learning Workshop</a> scheduled to start on the 8th of August, which is <a href="https://events.humanitix.com.au/braneshop-6-week-technical-deep-learning-workshop">on sale now!</a></p>
<p>(Not to mention our first <a href="/ai-for-leadership.html">AI for Leadership</a> workshop which is also on sale now, and our <a href="/deep-learning-workshop.html">Intensive Technical Deep Learning Workshop</a> which we can bring to your organisation — <a href="https://noonvandersilk.typeform.com/to/DYKvWN">get in touch!</a>)</p>
<p>I recently joined my friend Noon at Braneshop as an instructor, here’s why:</p>
<ol style="list-style-type: decimal">
<li><p>I love demystifying science into simple concepts to make it fun and accessible to people like you.</p></li>
<li><p>As a queer woman in the AI industry, I don’t see many people who look like me and I know how intimidating that can be. I’m here to help people from diverse backgrounds and be a part of the platform that elevates their skills and representation.</p></li>
<li><p>Noon has a lot of experience teaching technical content and we love working, learning and collaborating together.</p></li>
</ol>
<p>I come from an academic physics background and I learnt code in order to do my research. This commonly leads to sloppy coding and bad habits. I can’t say I’m completely cured (the physicist will be forever inside me!) but five years of working in tech has got me mostly straightened out in terms of writing nice code. More importantly, from working in data science and AI I have learnt the tools, tips and tricks that make the job easy and fun.</p>
<p>Academia is just one way into AI, there are actually as many ways as you could care to count. This is fantastic news, since it is so important to bring a diverse and interdisciplinary approach to AI tools which are becoming so prevalent in our world. There are lots of great resources online, and many of you will already be using AI in your lives and work, whether you realise it or not. Whichever path has brought you to this interest in AI, our workshop will give you hands on learning and leave you more confident to continue in the industry. My colleague and friend, Noon, has a very strong tech background, so between the two of us we’ve got all your needs catered for in our workshop.</p>
<p>In fact, many concepts we will cover in the workshop once felt foreign and scary to me, I picked them up bit by bit. And so can you. They actually aren’t scary at all, you just need someone to explain them to you in a logical way, starting from the basics. That’s what we’re here for.</p>
<p>It’s surprising how much you realise you know when someone is willing to join the dots for you. We’re ready to be your dot joiners! Come! Bring your dots! Let us join them!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>State of the art attention models in PyText</title>
    <link href="https://braneshop.com.au/posts/2019-06-03-State-of-the-art-attention-models-in-PyText.html" />
    <id>https://braneshop.com.au/posts/2019-06-03-State-of-the-art-attention-models-in-PyText.html</id>
    <published>2019-06-03T00:00:00Z</published>
    <updated>2019-06-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on June  3, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">State of the art attention models in PyText</h4>

  

  <div class="info">
    
  </div>

  <p>I wanted to take some time to discuss a library that seems to have flown under the radar a little bit. It’s called <a href="https://github.com/facebookresearch/pytext">PyText</a>, and I want to explore how you can use it to run state of the art attention models.</p>
<p>We’re going to proceed in the following way:</p>
<ol style="list-style-type: decimal">
<li>Set a dataset,</li>
<li>See how we prepare the data for training in PyText,</li>
<li>Get PyText,</li>
<li>Train it,</li>
<li>Visualise it,</li>
<li>Consider extension,</li>
</ol>
<p>Finally, we’ll jump in to a fork of PyText where I’ve added a few features that help out during training.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Deep learning in the browser!</title>
    <link href="https://braneshop.com.au/posts/2019-04-10-Deep-learning-in-the-browser-at-the-web-meetup.html" />
    <id>https://braneshop.com.au/posts/2019-04-10-Deep-learning-in-the-browser-at-the-web-meetup.html</id>
    <published>2019-04-10T00:00:00Z</published>
    <updated>2019-04-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on April 10, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Deep learning in the browser!</h4>

  

  <div class="info">
    
			<center><p> The code used for this blog-post is <a href="https://github.com/silky/tfjs-fashion-mnist-classifier">here</a>. </p></center>
    
  </div>

  <p>Last night I gave a talk at <a href="https://www.meetup.com/the-web/events/260211393/">The Web Meetup</a> on deep learning in the browser. It was mostly about how to deploy models built TensorFlow to the browser, by way of TensorFlow.js, but we did manage to try a model on the <a href="https://colab.research.google.com/">Google Colaboratory</a>, and deploy it to the browser all in the space of about 5 minutes!</p>
<p>We also discussed a few of the items that I brought up in my recent post about <a href="/posts/2019-02-08-TensorFlowJS-How-to-easily-deploy-deep-learning-models.html">deploying models via TensorFlow.js</a>.</p>
<center>
<img src="/images/blog/wmu-1.jpg" /> <br /> Deep learning in the browser! - Presentation at the April Web Meetup.
</center>
<p>Here are the links to the presentation and the code:</p>
<ul class="normal">
<li>
Presentation - <a href="https://docs.google.com/presentation/d/1_px6paltT1ZHZPKKTK_o-8KWKppremKcwe-5GY94kos/edit">Deep learning in the browser!</a>
</li>
<li>
Online Demo - <a href="https://silky.github.io/tfjs-fashion-mnist-classifier/index.html">tfjs fashion mnist classifier</a>
<li>
Code - <a href="https://github.com/silky/tfjs-fashion-mnist-classifier" class="uri">https://github.com/silky/tfjs-fashion-mnist-classifier</a>
</li>
</ul>
<p>Some of the discussion points from the audience were:</p>
<ul class="normal">
<li>
Q: What are some real-world use-cases of TensorFlow.js? (A: I haven’t seen many applications yet, but they’re sure to exist!)
</li>
<li>
Q: What do you think the scope is for new applications of AI in the browser? (A: Heaps! We need to play with the tooling to find out what they are)
</li>
<li>
Q: Do the models need to be significantly smaller to run in the browser? (A: Depends if you have a GPU when you run it; but not generally, no. A bit smaller, yes.
</li>
</ul>
<p>We wrapped up with a few thoughts around the future of UX and AI. Ultimately, I think this is going to be a field of significant growth and importance, so now’s the time to get into it, if it’s of interest to you!</p>
<hr />
<p>If you’re looking to adopt AI in your organisation, then check out our upcoming <a href="/ai-for-leadership.html">AI For Leadership</a> course!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Australian Human Rights Commission AI - Submission</title>
    <link href="https://braneshop.com.au/posts/2019-03-14-HRC-AI-Submission.html" />
    <id>https://braneshop.com.au/posts/2019-03-14-HRC-AI-Submission.html</id>
    <published>2019-03-14T00:00:00Z</published>
    <updated>2019-03-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on March 14, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Australian Human Rights Commission AI - Submission</h4>

  

  <div class="info">
    
  </div>

  <p>At Braneshop we’re keen to see AI develop positively in our community. The rules and regulation around AI will play a key role in ensuring that the AI that is ultimately built and used ends up creating good community outcomes.</p>
<p>The Australian Human Rights Commission recently put out a <a href="https://tech.humanrights.gov.au/consultation">call for consultation on AI - Governance and Leadership</a>. The deadline for a submission is the 18th of March at 5pm (so there’s still time if you’re quick!)</p>
<p>Working with Martin and Sam at <a href="https://www.northraine.com/">Northraine</a> and other interested parties, we prepared a joint submission, which you can read here: <a href="/files/HRC_Submission.pdf">Our HRC Submission</a>.</p>
<p>One really important message regarding ethics and AI, that I’ve seen recently, comes from the following papers: <a href="https://scirate.com/arxiv/1903.03425">The Ethics of AI Ethics – An Evaluation of Guidelines</a>.</p>
<p>Quoting the author:</p>
<blockquote>
<p>“Ethics is then no longer understood as a deontologically inspired tick-box exercise, but as a project of advancing personalities, changing attitudes, strengthen responsibilities and gaining courage to refrain from certain actions, which are deemed unethical. […] <strong>It should not be the objective of ethics to stifle activity, but to do the exact opposite</strong>, i.e. broadening the scope of action, uncovering blind spots, promoting autonomy and freedom, and fostering self-responsibility.” - <small>Thilo Hagendorff</small></p>
</blockquote>
<p>Emphasis mine.</p>
<p>This is a crucial and important thought when considereding any kind of “normative”, rule-based, punishment-driven acitivies, over those that enrich, embolden and allow for, and encourage, the growth of people and communities.</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Mindful Neural Networks</title>
    <link href="https://braneshop.com.au/posts/2019-03-06-Mindful-Neural-Networks.html" />
    <id>https://braneshop.com.au/posts/2019-03-06-Mindful-Neural-Networks.html</id>
    <published>2019-03-06T00:00:00Z</published>
    <updated>2019-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on March  6, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Mindful Neural Networks</h4>

  

  <div class="info">
    
  </div>

  <p>Last night at the <a href="https://www.meetup.com/Melbourne-Creative-AI-Meetup/">Creative AI Meetup</a> we had an awesome conversation around the idea of “Mindful” neural networks.</p>
<center>
<img width="600" src="/images/blog/thich-nhat-tanh.png" /> <br /> <i>Thich Nhat Tanh</i>
</center>
<p>We were hosted by our friends at <a href="https://sheda.ltd">Sheda</a> over the <a href="https://neighbourhood.work">Neighbourhood.work</a> in Fitzroy, and we had a really nice group of people with varied and interesting backgrounds show up:</p>
<center>
<a href="/images/blog/creative-ai-march-2019.jpg"><img width="600" src="/images/blog/creative-ai-march-2019.jpg" /></a> <br /> <i>The Mindful Neural Network Group. Photo courtesy of Eike!</i>
</center>
<p>We did the usual Creative-AI opening where we review a bunch of recent innovations in the field in the last few weeks. You can find the projects we discussed by looking over the slides here: <a href="https://docs.google.com/presentation/d/167A6XXp9NIOb0tPTls51PveyMgKjGEEpKaeospVr54g/edit">Creative AI Meetup - March 2019</a>.</p>
<p>Most notable were the variety of “this X does not exist” websites powered by <a href="https://github.com/NVlabs/stylegan">StyleGAN</a>:</p>
<ul class="normal">
<li>
Person: <a href="https://thispersondoesnotexist.com" class="uri">https://thispersondoesnotexist.com</a>
</li>
<li>
Cat: <a href="https://thiscatdoesnotexist.com" class="uri">https://thiscatdoesnotexist.com</a>
</li>
<li>
Airbnb: <a href="https://thisrentaldoesnotexist.com" class="uri">https://thisrentaldoesnotexist.com</a>
</li>
</ul>
<p>and the <a href="https://github.com/openai/gpt-2">GPT-2 model</a>, which produced the following gem, when prompted with a segment of <a href="https://www.goodreads.com/book/show/687278.When_Things_Fall_Apart">when things fall apart</a>:</p>
<pre><code>Input  &gt;&gt;&gt;

To be fully alive, fully human, and completely awake is to be


Output &gt;&gt;&gt;

completely free from judgement and morality, to have freedom in any worldview
through suffering (with anything that comes from between two extremes can
often be fine), to be absolutely free from stimuli getting in your way,
to be completely free from fear and humiliation, to be completely free
from a series of simple pleasures, to be absolutely free from your own
emotional and social dependence inherent in anything being different
from yours. [...]</code></pre>
<p>We did eventually got around to discussing the mindfulness aspect of neural networks, and what ideas we thought we could transfer over.</p>
<p>As a group, we covered:</p>
<ul class="normal">
<li>
<strong>Attention</strong>: <a href="https://scirate.com/arxiv/1706.03762">Is attention enough?</a> Should we make our networks “aware” of their own thoughts? Is that te same as “attention” as-it’s implemented presently? We discussed briefly this paper: <a href="https://scirate.com/arxiv/1902.10186">Attention is not Explanation</a>.
</li>
<li>
<strong>Anxiety</strong>: We discussed anxiety as “fear of the future”, and considered that perhaps neural nets have no awareness of the future, as they only make judgements on single instance, or alternatively the interpretation that actually neural networks are incredibly anxious as during training that are entirely focused on the future, and how they can change themselves to do better.
</li>
<li>
<strong>Presence</strong>: We wondered whether neural networks are “present” in “the moment”, or if this even makes sense.
</li>
<li>
<strong>Consciousness</strong>: We didn’t go too in-depth on this, but we happened to have a <a href="https://www.linkedin.com/in/rafik-hadfi/">researcher working in this area in the room</a>, so we discussed it on an off. We did discuss our favourite reading on the topic. On free will, mine is this <a href="https://arxiv.org/abs/1306.0159">The Ghost in the Quantum Turing Machine</a>, and on consciouness and thinking it is <a href="https://www.goodreads.com/book/show/123471.I_Am_a_Strange_Loop">I Am a Strange Loop</a>, <a href="https://www.goodreads.com/book/show/24113.G_del_Escher_Bach">Gödel, Escher, Bach</a>, and <a href="https://www.goodreads.com/book/show/7711871-surfaces-and-essences">Surfaces and Essences</a> (the last of which I covered on <a href="https://silky.github.io/posts/2019-01-03-2018-books.html">my 2018 reading list</a>).
</li>
<li>
<strong>Intent</strong>: We talked a little bit about how it’s important to know <em>why</em> a given dataset was constructed, as this might lead to us understanding it’s bias.
</li>
<li>
<strong><a href="https://en.wikipedia.org/wiki/P%C4%81ramit%C4%81">Paramitas</a>/Loss Functions</strong>: We had a moderate amount of discussion around how “small” most neural networks are in their output objective. They take in significantly rich data, but force it to be absolutely 1 or absolutely 0. Maybe there could be richer loss functions we could consider, that would allow for the network to focus in different ways. A recent bit of research along these lines is this: <a href="https://scirate.com/arxiv/1903.01182">Completment Objective Training</a>, i.e. have it make a good prediction of what it is, but also what it is <em>not</em>.
</li>
</ul>
<p>Overall, I had heaps of fun, and there was some discussion of a second installment, as there is heaps to explore here, so stay tuned to the <a href="https://www.meetup.com/Melbourne-Creative-AI-Meetup/">meetup</a> and join us next time!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>The Canary Set for Machine Learning Applications</title>
    <link href="https://braneshop.com.au/posts/2019-03-01-The-Canary-Set.html" />
    <id>https://braneshop.com.au/posts/2019-03-01-The-Canary-Set.html</id>
    <published>2019-03-01T00:00:00Z</published>
    <updated>2019-03-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on March  1, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">The Canary Set for Machine Learning Applications</h4>

  

  <div class="info">
    
  </div>

  <p>Earlier this week I was chatting to some people and we came up with a good idea. It’s what I refer to now as the “Canary Set”.</p>
<p>We’re probably all familiar with the standard “canary in the cage” idea:</p>
<center>
<p>
The standard canary in a cage.
</p>
<img src="/images/blog/canary-cage.png" alt="The standard canary in a cage." />
</center>
<p>It’s an item that can be carried with you into your hazardous job, and when it dies, it’s suggestive that the place your working in is unsafe. Poor canary.</p>
<p>I’d like to propose that this can be a useful idea in machine learning, where instead of a canary, we carry around an “interesting set” of data:</p>
<center>
<p>
A Canary Set of interesting data points.
</p>
<img src="/images/blog/canary-cage-data.png" alt="A Canary Set of data" />
</center>
<p>This would be called the “Canary Set” of data, and every time a new deployment of your ML model comes out, you run the mdoel across this dataset to see how it performs.</p>
<center>
<p>
Going in …
</p>
<br/> <img src="/images/blog/canary-cage-ai.png" alt="The canary set going through the next version of the AI" /> <br /> <br />
<p>
Coming out with a report …
</p>
<br/> <img src="/images/blog/canary-cage-ai-out.png" alt="The canary set coming out with a report!" />
</center>
<p>Let’s go into a few details of why we might want this, how it differs from the “test” or “validation” set during the training/evaluation phase of machine learning algorithms, and what benefits we might get from it.</p>
<h4 id="the-canary-set-main-justification">The Canary Set: Main Justification</h4>
<p>The main point of the canary set is to give <strong>users of your machine learning service</strong> control over a set of data that they want to evaluate your new models against. In this way, it differs significantly from the standard “test” set in that it doesn’t need to contain a well-distributed set of samples, it just needs to contain those samples that the <strong>user</strong> is interested in.</p>
<p>These might be:</p>
<ul class="normal">
<li>
Rare inputs,
</li>
<li>
Inputs for which they are uniquely concerned, as compared to other users,
</li>
<li>
“Extreme” inputs, that check areas they are interested in,
</li>
<li>
A random selection of their past inputs,
</li>
<li>
… many more ideas!
</li>
</ul>
<p>I think there’s a world of options here. The point is: <strong>The user gets some control</strong>. And this is a nice idea, given that we want them to be gaining confidence in the ML system.</p>
<h4 id="secondary-benefits">Secondary benefits</h4>
<p>Increasingly we’re going to see that there will be different environments for ML training and inference. We’ve already explored this in our last blog post on <a href="/posts/2019-02-08-TensorFlowJS-How-to-easily-deploy-deep-learning-models.html">TensorFlow.js</a>, but there are also the growing number of so-called “edge devices”, the <a href="https://cloud.google.com/edge-tpu/">Google Edge TPU</a>, the <a href="https://www.movidius.com/">Movidius</a> and the <a href="http://jevois.org/">JeVois</a>, among others.</p>
<p>The point is, we can’t actually guarantee that when we do testing of our model during training that it will agree exactly with the inference world, because there may be many stages in between.</p>
<p>This actually came up on a project I was working on last year. We trained a model, and when we deployed it to the cloud, it didn’t work well. We tracked it down to a bug in the GPU code relating to some post-processing of the models output. This error was only found becaues I decided it would be a good idea to test the model on some “real” data, before making it live. In other words, I made up my own canary set and tested the model on that first, before making it live.</p>
<h4 id="conclusion">Conclusion</h4>
<p>In summary, the idea is that in any machine learning service, for users, that you build, you could consider adding the facility for the users to be involved in selecting a set of data that is used to evaluate any new versions of the models.</p>
<p>The benefits are:</p>
<ul class="normal">
<li>
Increasing user engagement and trust,
</li>
<li>
finding new interesting edge-cases where your algorithm fails,
</li>
<li>
and ensuring the quality of inference when you may not be in control of the inference endpoint.
</li>
</ul>
<p>Hope this idea is helpful!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>TensorFlow.js - How to easily deploy deep learning models</title>
    <link href="https://braneshop.com.au/posts/2019-02-08-TensorFlowJS-How-to-easily-deploy-deep-learning-models.html" />
    <id>https://braneshop.com.au/posts/2019-02-08-TensorFlowJS-How-to-easily-deploy-deep-learning-models.html</id>
    <published>2019-02-08T00:00:00Z</published>
    <updated>2019-02-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on February  8, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">TensorFlow.js - How to easily deploy deep learning models</h4>

  

  <div class="info">
    
  </div>

  <p>In the following we’ll suppose you’ve trained a model in Python, and are now considering how to “productionise” the inference. The point of the article will be to convince you that it’s interesting to try and use <a href="https://js.tensorflow.org/">TensorFlow.js</a>.</p>
<h4 id="youve-got-a-model-how-to-deploy-it">You’ve got a model, how to deploy it?</h4>
<p>Once we’ve built a deep learning model, the next thing we natually want to do is deploy it to the people that want to use it.</p>
<p>We can typically make this as hard as we like, by first of all just building say a simple, say in Python, flask API endpoint, <code>/inference</code>, that will take in a json blob, and then push it into our model, and push back an inference:</p>
<center>
<img src="/images/blog/inference.jpg" />
</center>
<p>To something quite elaborate with various Amazon servers, Azure servers, Google servers, or the myriad other services providing deep learning inference and hosted model capability.</p>
<p>But there are a few problems with this approach:</p>
<ul class="norml">
<li>
You need a significant dev-ops capability to manage just the operating-system and data-management capabilities,
</li>
<li>
You need GPU resources (virtual or otherwise) to allocate for the inference,
</li>
<li>
Even having the GPU capability, you need to manage versioning of your models, how to swap them out,
</li>
<li>
And, you need to know how to ration and allocate the GPU resources between all your inference servers,
</li>
</ul>
<p>The point is, there’s much to do. In my exerience, this is infact a significant barrier/expense in deploying ML in production today.</p>
<p>Now, it may turn out that in your world you <em>need</em> to do this kind of deployment. But what I would like to think about briefly, is another approach …</p>
<h4 id="have-you-considered-javascript">Have you considered … JavaScript?</h4>
<p>So, what if I told you that you don’t need to set up any computers, and that in fact the computers you use will be entirely managed and set up by other people!? Great! What’s the catch? Well, you need to use JavaScript …</p>
<center>
<img src="/images/blog/tfjs.png" />
</center>
<p>The setup is that you want to do some inference on a “small” amount of user-supplied data. I.e. an image, a passage of text, even a video, or a piece of audio, but perhaps not 5GB of satellite images.</p>
<p>Then, the plan is like so:</p>
<ul class="normal">
<li>
You train your model as you typically would (in TensorFlow, let’s say),
</li>
<li>
You export the weights,
</li>
<li>
You build the “inference” part of your model in TensorFlow.js
</li>
<li>
The model runs in the browser,
</li>
<li>
That’s it!
</li>
</ul>
<p>The main idea is, just as we can build our entire model in TensorFlow, we can build the so-called “forward” part of our model in TensorFlow.js; that is, just those parts that are needed for inference, and not the parts are necessary for training and to support training. Then, we simply rebuild that part of our model using the TensorFlow.js standard library (such as <code>tanh</code> layers, or whatever our particular network requires).</p>
<p>Then, if we’ve stayed in the TensorFlow ecosystem it’s quite easy, we can just load in our weights basically immediately, but even if we haven’t, ultimately we can always map our exported weights into our network.</p>
<p>Once that’s done, you’re basically done. You only need to work out how to get your data into the browser to do inference on. Typically, if you’re working with images or video, this is easy. You can use the webcam, if you want a live stream, and really the world is your oyster in all the typical ways that it is on the web.</p>
<h4 id="the-upsides-and-downsides">The upsides and downsides</h4>
<p>The upsides are:</p>
<ul class="normal">
<li>
The model doesn’t run on your hardware ⇒ less resources required to deploy. In other words, if you want to handle 10,000 requests per minute, then you need significant GPU compute if you host the model on the server; but in this approach, you don’t need <em>any</em> additional compute; it all happens in the users computer.
</li>
<li>
Can utilise the exact same deployment and versioning techniques <small>(with large static content; namely the weights)</small> that you use for standard websites, which are tried and tested .
</li>
<li>
If the model is fast enough, can allow for very powerful “live” interaction by users.
</li>
<li>
Incredibly portable, as it runs in the browser, it can run on phones, small laptops, laptops with GPUs (and utilise those GPUs) and can be run entirely offline.
</li>
</ul>
<p>The downsides are:</p>
<ul class="normal">
<li>
You need to maintain a consistency between your Python models and your JavaScript models.
</li>
<li>
Because the model doesn’t run on hardware you control, the speed of inference can vary wildly.
</li>
<li>
In order to deal with the speed issue, we typically run a “smaller” or “compressed” model in the browser, which can be less accurate
</li>
</ul>
<h4 id="final-thoughts">Final thoughts</h4>
<p>Browser-based deep learning won’t be for everyone, but my feeling is that if you’re out in the world applying deep learning, you can probably find a way to run some kind of model in the browser.</p>
<p>Furthermore, the kinds of interactions you can get with users by having a “real-time” inference model are wildly different than from a slow web-request model, or an unknown amount of time if queues are involved. Notably, this kind of real-time interaction allowed me, previously, to build an interactive fashion design booth, where the audience could engage with a GAN, and also an interactive <a href="https://github.com/silky/dance-booth">dance booth</a>, where people go to dance with an AI.</p>
<center>
<img src="/images/blog/dance-booth-fashion-designer.png" />
</center>
<p>We were able to put together both of these projects much faster, and much more portably, because we were using JavaScript-based deep learning inference.</p>
<p>So I encourage you to take a look at <a href="https://js.tensorflow.org/">TensorFlow.js</a>, and try and build something with it!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>Thanks to Ren Imagery for our new logo!</title>
    <link href="https://braneshop.com.au/posts/2019-01-30-Best-Designer-Ren-Imagery.html" />
    <id>https://braneshop.com.au/posts/2019-01-30-Best-Designer-Ren-Imagery.html</id>
    <published>2019-01-30T00:00:00Z</published>
    <updated>2019-01-30T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on January 30, 2019
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">Thanks to Ren Imagery for our new logo!</h4>

  

  <div class="info">
    
  </div>

  <p>We have a new logo! You can see it loud and proud at the top of every page; but here is it again:</p>
<center>
<img style='background: black; padding: 20px;' src="/images/braneshop-blue.png" width="400" />
</center>
<p>I like the logo because conveys “brain-ness” with the wavey lines; perhaps it reminds you of your own brain. It also feels a bit “80s” and “fun” to me; a style that <a href="https://silky.github.io/posts/2019-01-27-Designing-Functional-Cloths-with-Haskell.html">I’m well-known to love</a>, and, of course, it’s amenable to funky bright colours.</p>
<p><a href="https://www.theloop.com.au/renlemon/portfolio/Graphic-Designer/Melbourne">Ren</a> is a well-rounded and wonderful designer, and I don’t just say that because she’s a friend of mine. Here are some of the reasons why:</p>
<ul class="normal">
<li>
<strong>Collaborative</strong>: Ren communicates frequently; asks your opinion, and submits some designs for review and to guide further design.
</li>
<li>
<strong>Diverse options</strong>: Unlike other designers I’ve worked with, each of Ren’s submitted designs was actually quite unique and distinctly different from the others. The differences are not just a slight colour changes, or lower case over upper case; each design has it’s own strong elements, story, and justifcation. I think this is a very important skill for design, that I’ve not seen many people be able to do, and Ren does it amazingly well.
</li>
<li>
<strong>Comprehensive</strong>: Once a final design was sleceted, Ren provides the logo in many formats and colours, and also includes nice feature “elements” (such as the squiggle in the bottom-left of the logo) as seperate files. This allows me to include and use parts of the logo thematically through-out the website.
</li>
<li>
<strong>Consistency</strong>: Ren has done design work for me in the past, and it has been consistently excellent. The previous work was for <a href="http://www.composeconference.org/">Compose Conference</a>, and the resulting logo was designed collaboratively between Ren, Lyndon and Myself, and I have to say that it’s now iconic in the functional-programming community as the “Lambda-Tram”:
<center>
<img src="/images/blog/LambdaTramWeb2017.png" />
</center>
</li>
</ul>
<p>So, if you’re looking for any kind of design work, definitely get in contact with her over at <a href="https://www.theloop.com.au/renlemon/portfolio/Graphic-Designer/Melbourne">Ren Imagery</a> or on <a href="https://www.linkedin.com/in/lauren-harris-22409899/">LinkedIn</a>!</p>
</div>
]]></summary>
</entry>
<entry>
    <title>How much data do you need to retrain a classifier?</title>
    <link href="https://braneshop.com.au/posts/2018-12-17-How-Much-Data-For-Retraining.html" />
    <id>https://braneshop.com.au/posts/2018-12-17-How-Much-Data-For-Retraining.html</id>
    <published>2018-12-17T00:00:00Z</published>
    <updated>2018-12-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="section">
  <div class="info">
    Posted on December 17, 2018
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="/blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">How much data do you need to retrain a classifier?</h4>

  

  <div class="info">
    
			<center><p> The code used for this blog-post is <a href="https://github.com/BraneShop/how-much-data-experiments">here</a>. </p></center>
    
  </div>

  <script src="https://cdn.jsdelivr.net/npm/vega@4.3.0"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@3.0.0-rc10"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@3.24.1"></script>
<p>A while ago, Gala had a great idea for a little animation: Give people an idea of how much data they need to train a machine learning model, by showing an animation of a model trained on, say, 5, 50, 500, 5,000, 50,000, 500,000 data points. The way I pictured it in my mind was that as you proceed to more and more data, the number of successful classifications gets larger; ultimately tapering off as we get to larger numbers. In my mind I saw it as the standard S-curve:</p>
<center>
<p>
<img src="/images/blog/s-curve.png" />
</p>
</center>
<div class="important">
<p><b>Quiz</b>: Before reading the rest of this article, have a go at this quick quiz.</p>
<p>With the network and data-set we’re going to be focused on (<a href="https://www.tensorflow.org/hub/tutorials/image_retraining">the TensorFlow transfer learning example using InceptionV3</a>), the expected validation accuracy is 90-95%, when run on the full data set (~700 images per class).</p>
<p>Note that this is using five kinds of flowers as the classes: tulip, rose, daisy, dandelion, sunflower.</p>
<p>Suppose we set aside 70 images per class for validation, in each of the following scenarios:</p>
<ul class="normal">
<li>
Q1: If we use 50% of the remaining images, what would you expect the validation accuracy to be?
</li>
<li>
Q2: If we only use 5% of the remaining images?
</li>
<li>
Q3: If we only use <b>3 images in total</b> per class?
</li>
<li>
Q4: What about only <b>1 image</b> per class!?
</li>
</ul>
</div>
<h4 id="background-on-transfer-learning">Background on Transfer Learning</h4>
<p>For this article, we’re focusing on using a pre-trained network and specialising it to our specific (classification) task. This idea is called “Transfer Learnining”.</p>
<p>Here’s a picture that describes the idea:</p>
<center>
<img src="/images/blog/transfer-learning.png" />
</center>
<p>There’s two steps:</p>
<p>First, you take some “pre-trained” network that is good at a particular task. That’s “1)” in the picture above; some network that is good a recognising trees, houses, and people. It’s been trained for this task and is really good at it.</p>
<p>Secondly, you cut off the final layer, and plug in your new classes that you want to predict. This gives you new parameters to learn (the edges coloured in magenta above). Then, you train your new network on your new data.</p>
<p>The point is, this network should have a lot of “juice” (i.e. learned recognition capability) in the earlier layers, that capture general-purpose structure; and the later layers, that you leave as learnable paremeters, allow the network to specialise to your task.</p>
<p>This is a really powerful idea in deep learning, and, in essence, is incredibly heavily used (from the idea of pre-trained word vectors, to pre-trained image classification networks, as we’re using here).</p>
<h4 id="lets-dive-in">Let’s dive in</h4>
<p>Here’s the plan:</p>
<ol style="list-style-type: decimal">
<li>Follow the TensorFlow Transfer Learning Tutorial, but</li>
<li>Create a “hold-out” dataset from the original dataset, say 10%,</li>
<li>Create downsampled datasets, from the new dataset, of the following: 100%, 90%, 80%, 70%, 60%, 50%, 40%, 30%, 20%, 10%, 5%, as well as the 3-image and 1-image sets. Visually:</li>
</ol>
<center>
<img src="/images/blog/how-much-data-selection.png" />
</center>
<ol start="4" style="list-style-type: decimal">
<li>Throw in some data augmentation, using <a href="https://github.com/aleju/imgaug">imgaug</a>,</li>
<li>Train all these experiments,</li>
<li>See how it all goes!</li>
</ol>
<p>I’ve wrapped this up into a repo so you can reproduce my results: <a href="https://github.com/BraneShop/how-much-data-experiments">braneshop/how-much-data-experiments</a>. Follow the link and check out the README if you want to run it yourself. It takes a few hours to run everything.</p>
<h4 id="results">Results</h4>
<p>Let’s take a look at the train/validation curves for all the training runs. You can click on the circle next to a given experiment name to show only that. Shift-click will show many at once. First, note that in the “just-1” and “just-3” image case, I hacked the normal training code to not worry about having any validation/test data allocation. As a result, we don’t have them in this graph.</p>
<div id="train-val-curves-1">

</div>
<style type="text/css"> div.vega-actions { display: none; } </style>
<script type="text/javascript">
	var spec = { 
		"$schema": "https://vega.github.io/schema/vega-lite/v3.json",
		"data": {"url": "/data/train-val-curves-1.json" },
		"hconcat": [ 
			{ "title": "Train (solid) / Validation (dashed) Curves For Experiments",
				"width": 700, "height": 400,
				"layer":
					[ // Training
						{ "mark": { "type": "line", "interpolate": "basis" },
							"encoding": {
								"x": {"field": "step", "type": "quantitative", "axis": {"title": "Step"}},
								"y": {"field": "loss_train", "type": "quantitative", "axis": {"title": "Value"}},
								"color": {"field": "Experiment", "type": "nominal", "legend": null},
								"tooltip": {"field": "loss_train", "type": "nominal"}
							},
							"transform": [{"filter": {"selection": "legend"}}],
						}
						// Validation
						, { "mark": { "type": "line", "interpolate": "basis", "strokeDash": [4,4] },
								"encoding": {
									"x": {"field": "step", "type": "quantitative", "axis": {"title": "Step"}},
									"y": {"field": "loss_val", "type": "quantitative", "axis": {"title": "Value"}},
									"color": {"field": "Experiment", "type": "nominal", "legend": null},
									"tooltip": {"field": "loss_val", "type": "nominal"}
								},
								"transform": [{"filter": {"selection": "legend"}}],
							}]
				},
				// Legend
				{ "mark": { "type": "circle", "size": 200, "cursor": "pointer"},
					"encoding": { 
						"y": {"type": "nominal", "field": "Experiment", "title": "Experiments"}, 
						"color": { 
							"condition": { "type": "nominal", "field": "Experiment", "legend": null, "selection": "legend" }, 
							"value": "lightgray" 
						}
					}, 
					"selection": { 
						"legend": { 
							"type": "multi", 
							"encodings": ["color"], 
							"on": "click", 
							"toggle": "event.shiftKey", 
							"resolve": "global", 
							"empty": "all"
							}
						}
					}
			]
	};

	vegaEmbed("#train-val-curves-1", spec);
</script>
<p>Observations:</p>
<ul class="normal">
<li>
When we have tiny data (the 5% case) the train loss is immediately tiny; and the val loss immediately high. This was my expectation.
</li>
<li>
If we compare, say, the 100-image case and the 100 with 10x augmentation, the augmented version is worth in train and validation. This was surprising to me.
</li>
<li>
There’s not a significant different in the curves between say the 100% case and the 50% case. This, again, was surprising to me.
</li>
<li>
Augmentation always made both curves worse. I wasn’t expecting this, and am not yet cetrain why it’s the case. I have two main ideas: I used a very elaborate augmentation (the complicated one from the readme on the <a href="https://github.com/aleju/imgaug">imgaug</a> repo), maybe that was a mistake. I think ultimately, the augmented images don’t look much like the testing images, so they encouraged they network to do worse.
</li>
</ul>
<p>The real proof is in the accuracy against the holdout set, though, so let’s see how the models performed in this case. The following graph is the result of running all the models on the holdout images, and taking <strong>the maximum predicted value</strong> of all the resulting class predictions.</p>
<div id="holdout-accuracy">

</div>
<script type="text/javascript">
	var spec 
		= { "$schema": "https://vega.github.io/schema/vega-lite/v3.json", "width": 700, "height": 400,
			  "mark": { "type": "bar", "interpolate": "basis" }, 
				"data": {"url": "/data/holdout-accuracies.json" },
				"encoding": { 
					"y": {"field": "Experiment", 
								"type": "nominal", 
								"sort": {"field": "Accuracy", "op": "average", "order": "ascending"}
							 },
				 "x": {"field": "Accuracy", 
							 "type": "quantitative", 
							 "axis": { "grid": true }, 
							 "scale": { "domain": [0,1] } 
							},
				 "tooltip": {"field": "Accuracy", "type": "quantitative"}
				 }
	};

	vegaEmbed("#holdout-accuracy", spec);
</script>
<p>There were <em>many</em> surprising things about this graph for me. Notably, it doesn’t look like the graph that I predicted at the start (note that the axes are flipped), but probably <em>the</em> most shocking thing for me is this:</p>
<p><strong>With just a single image per class</strong>, we get 68% accuracy on the holdout set! And with just 3 images, we get 75%! Recall that the holdout set consists of ~70 images per class.</p>
<p>Now, when I ran this initially, I hand-picked the 1-image and 3-images per class. I picked ones that looked pretty good, but I was still amazed at the results! I set up a notebook to run a few different examples of that, and I also evaluated the accuracy with the threshold approach, instead of the argmax; here are a few runs:</p>
<pre><code>seed = 1                    seed = 2                    seed = 3

1 image                     1 image                     1 image
              max: 0.5                    max: 0.53                   max: 0.55
  threshold (0.5): 0.3        threshold (0.5): 0.38       threshold (0.5): 0.39
  threshold (0.8): 0.13       threshold (0.8): 0.18       threshold (0.8): 0.15

3 images                    3 images                    3 images
              max: 0.68                   max: 0.66                   max: 0.67
  threshold (0.5): 0.55       threshold (0.5): 0.51       threshold (0.5): 0.54
  threshold (0.8): 0.23       threshold (0.8): 0.25       threshold (0.8): 0.23</code></pre>
<p>You can see that for randomly-picked images, the argmax sits aroud 53% for 1 image, and 67% for three images.</p>
<p>The point is, to me it’s quite amazing to see that picking the argmax, instead of using the threshold, gives such a radical improvement <em>on withheld data</em>.</p>
<p>I’ve made a <a href="https://colab.research.google.com/drive/1VWxeyhOGMTHHVZFe33s5zaHIKXdDUkA2">notebook on Google Colaboratory</a>, so if you have a Google account, you can run the experiments for yourself (you’ll have to save it as a new copy).</p>
<h4 id="discussion">Discussion</h4>
<p>For me, this was quite surprising. For one, I didn’t see the improvement at the middle-range of data that I was expecting (50% training data wasn’t much better than 100%, excluding the holdout set). But moreso, just how much juice can be squeezed out of this particular network + a handful of images is amazing.</p>
<p>The main thing this highlights to me is the power of <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning</a>, <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">active learning</a>, and <a href="https://en.wikipedia.org/wiki/One-shot_learning">One-shot learning</a>.</p>
<p>Specifically, it suggests to me that the best thing to do is to set up a learning+data acquisition process that makes predictions as fast as possible, and constantly learns as it goes:</p>
<center>
<img src="/images/blog/proposed-workflow.png" />
</center>
<p>Overall, the idea of incorporating data into a model as-it-infers isn’t new, but to me, again, what is surprising is just how little data you need to get started; at least in this specific case!</p>
<p>To reiterate differently: If you’re a business wondering how to adopt AI, one plan would be to build this kind of human-in-the-loop system, where any prediction that is, say, better than 50% chance of being right will be useful to humans. Then, have the humans feed this system as they use it, and it will radically improve with just a small number of confirmed answers!</p>
<p>So, to answer the question from the title, we might say “less than you think!”, if you have the right setup to incorporate new data into your system, as you go.</p>
<h4 id="open-questions">Open questions</h4>
<ul class="normal">
<li>
How do other models go in this measure? Say, segmentation models or bounding box models? Or NLP?
</li>
<li>
Is it really right to think of the softmax layer as <em>probabilities</em>? Certainly it satisfies a probability-distribution constraint, but training seems to simply push these values more apart? i.e. we don’t really care if they are probabilities, if we’re just going to pick the largest. Further, maybe we should only ever think of the probability as the combination of the decision rule (say argmax or &gt; 80%) <em>and</em> the holdout dataset that we evaluate that rule on.
</li>
<li>
Why did augmentation make things worse? Did I over-do it? Related paper: <a href="https://scirate.com/arxiv/1811.04768">Learning data augmentation policies using augmented random search</a>.
</li>
<li>
Could be build some kind of “dataset juicer” that tells us just how much juice we’ll get, for given increases in our labelled data, with respect to certain models and targets?
</li>
<li>
Relatedly, it would be nice to calculate some kind of “potential-juice” measure where we look at the number of variables we’re going to retrain; the number of free variables in our data, and then consider what kind of space of we’ll explore in this domain.
</li>
</ul>
<h4 id="related-papers">Related papers</h4>
<ul class="normal">
<li>
<a href="https://scirate.com/arxiv/1712.00409">Deep Learning Scaling is Predictable, Empirically</a> - This is a deeper investigation into these ideas, across a few different model types. It empirically answers my first open question.
</li>
<li>
<a href="https://scirate.com/arxiv/1811.04768">Learning data augmentation policies using augmented random search</a>.
</li>
</ul>
<h4 id="quiz-answers">Quiz answers</h4>
<div class="important">
<p>Using the maximum value of the class estimates (argmax):</p>
<ul class="normal">
<li>
Q1: If we use 50% of the remaining images, what would you expect the validation accuracy to be? Answer: ~91%
</li>
<li>
Q2: If we only use 5% of the remaining images? Answer: ~83%
</li>
<li>
Q3: If we only use <b>3 images in total</b> per class? Answer: ~67%
</li>
<li>
Q4: What about only <b>1 image</b> per class!? Answer: ~53%
</li>
</ul>
</div>
<h4 id="the-animation">The animation</h4>
<p>Here’s my attempt at the animation (click to view):</p>
<center>
<a href="https://imgur.com/a/swZ9qiQ"><img src="/images/blog/aug-00.jpg" /></a>
</center>
<p>The images with the red text above them are the ones where the inferences were wrong. Unfortunately (or fortunately!) it’s not very compelling, because all the accuracies are basically very high!</p>
</div>
]]></summary>
</entry>

</feed>
