<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<!--
You have entered the realm of

 ▄▄▄▄    ██▀███   ▄▄▄       ███▄    █ ▓█████   ██████  ██░ ██  ▒█████   ██▓███  
▓█████▄ ▓██ ▒ ██▒▒████▄     ██ ▀█   █ ▓█   ▀ ▒██    ▒ ▓██░ ██▒▒██▒  ██▒▓██░  ██▒
▒██▒ ▄██▓██ ░▄█ ▒▒██  ▀█▄  ▓██  ▀█ ██▒▒███   ░ ▓██▄   ▒██▀▀██░▒██░  ██▒▓██░ ██▓▒
▒██░█▀  ▒██▀▀█▄  ░██▄▄▄▄██ ▓██▒  ▐▌██▒▒▓█  ▄   ▒   ██▒░▓█ ░██ ▒██   ██░▒██▄█▓▒ ▒
░▓█  ▀█▓░██▓ ▒██▒ ▓█   ▓██▒▒██░   ▓██░░▒████▒▒██████▒▒░▓█▒░██▓░ ████▓▒░▒██▒ ░  ░
░▒▓███▀▒░ ▒▓ ░▒▓░ ▒▒   ▓▒█░░ ▒░   ▒ ▒ ░░ ▒░ ░▒ ▒▓▒ ▒ ░ ▒ ░░▒░▒░ ▒░▒░▒░ ▒▓▒░ ░  ░
▒░▒   ░   ░▒ ░ ▒░  ▒   ▒▒ ░░ ░░   ░ ▒░ ░ ░  ░░ ░▒  ░ ░ ▒ ░▒░ ░  ░ ▒ ▒░ ░▒ ░     
 ░    ░   ░░   ░   ░   ▒      ░   ░ ░    ░   ░  ░  ░   ░  ░░ ░░ ░ ░ ▒  ░░       
 ░         ░           ░  ░         ░    ░  ░      ░   ░  ░  ░    ░ ░           
      ░                                                                         
-->

  <meta name="google-site-verification" content="cKie6SUiby1JmI2F8RMscJRBTa28kTM6XNHS5l1GXxc" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131177872-1"></script>
  <script type="text/javascript">
    var host = window.location.hostname;
    if ( host != "localhost") {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-131177872-1');
    }
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>Deep learning showreel! - Braneshop</title>
  <meta name="geo.region" content="AU-VIC" />
  <meta name="geo.placename" content="Melbourne" />
  <link rel="stylesheet" type="text/css" href="./css/default.css?1" />
  <link rel="stylesheet" type="text/css" href="./css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=PT+Mono|Lato|DM+Serif+Text|Karma|Gilda+Display|Quando" rel="stylesheet">
  <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- Meta tags for little cards and such. -->
  <meta property="og:title" content="Deep learning showreel! - Braneshop" />
  
  <meta property="og:url" content="https://braneshop.com.au/showreel.html" />


<!-- Start of Async Drift Code -->
<script type="text/javascript">
"use strict";

!function() {
  var t = window.driftt = window.drift = window.driftt || [];
  if (!t.init) {
    if (t.invoked) return void (window.console && console.error && console.error("Drift snippet included twice."));
    t.invoked = !0, t.methods = [ "identify", "config", "track", "reset", "debug", "show", "ping", "page", "hide", "off", "on" ], 
    t.factory = function(e) {
      return function() {
        var n = Array.prototype.slice.call(arguments);
        return n.unshift(e), t.push(n), t;
      };
    }, t.methods.forEach(function(e) {
      t[e] = t.factory(e);
    }), t.load = function(t) {
      var e = 3e5, n = Math.ceil(new Date() / e) * e, o = document.createElement("script");
      o.type = "text/javascript", o.async = !0, o.crossorigin = "anonymous", o.src = "https://js.driftt.com/include/" + n + "/" + t + ".js";
      var i = document.getElementsByTagName("script")[0];
      i.parentNode.insertBefore(o, i);
    };
  }
}();
drift.SNIPPET_VERSION = '0.3.1';
drift.load('gge7kn9kp4gc');
</script>
<!-- End of Async Drift Code -->
</head>
<body>
  <div id="braneshop-fixed" onclick="document.location = '/';"></div>
  <!-- Header -->
  <div id="header">
    <div id="header-top">
      <h1><a href="./"><img id="header-logo" class="compressed" src="./images/braneshop-blue.png" title="Braneshop" alt="Braneshop" /></a></h1>
    </div>
    <h2>Expand your knowledge manifold!</h2>
  </div>
  <div class="menu-box">
    <ul class="menu">
      <li><a href="./" title="Home">Home</a></li>
      <li><a href="./workshops.html" title="Workshops">Workshops</a></li>
      <li><a href="./advisory-and-consulting.html" title="Advisory and Consulting">Advisory and Consulting</a></li>
      <li><a href="./about.html" title="About">About</a></li>
      <li><a href="./team.html" title="Team">Team</a></li>
      <li><a href="./events.html" title="Community and Events">Community &amp; Events</a></li>
      <li><a href="./blog.html" title="Blog">Blog</a></li>
      <li><a href="./showreel.html" title="Showreel">Showreel</a></li>
      <li><a href="./contact.html" title="Contact">Contact</a></li>
    </ul>
  </div>

  <!-- Content -->
  <div id="content">
    <div class="section" id="showreel">
  <h3> <span>Deep learning showreel! </span> </h3>
  <div id="show-reel">
    <p>
    We closely follow research in deep learning and AI. Here are a collection
    of cool, interesting, and fun applications that we've seen, with a brief
    explanation and a link to the original paper. All figures are taken
    directly from the paper or the associated website.
    </p>

    <p>Check back for regular updates, or sign up to the newsletter (below) to
    receive the latest cool stuff, monthly!</p>

    <ul class="blog-list normal">
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Seeing%20the%20World%20in%20a%20Bag%20of%20Chips.html" rel="noopener">Seeing the World in a Bag of Chips</a> &mdash; January 14, 2020 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/2001.04642.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Seeing the World in a Bag of Chips.jpg"><img alt src="./images/showreel/Seeing the World in a Bag of Chips.jpg" /></a></center>
          

          <p>Is the world reflected in a bag of chips? Yes! Turns out, using a neural network, if you want to know whats going on around you; you can just take a photo (with depth information) of the chip packet and figure it out!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Learning%20to%20Zoom-in%20via%20Learning%20to%20Zoom-out.html" rel="noopener">Learning to Zoom-in via Learning to Zoom-out</a> &mdash; January  8, 2020 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/2001.02381.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning to Zoom-in via Learning to Zoom-out.jpg"><img alt src="./images/showreel/Learning to Zoom-in via Learning to Zoom-out.jpg" /></a></center>
          

          <p>This paper tackles the now-standard problem of “enhancing” an image; i.e. being able to look into any region of an image in greater detail. The trick here is that they utilise the idea of learning to zoom <em>out</em> to also learn to zoom <em>in</em>. The results are very good!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Painting%20Many%20Pasts%20-%20Synthesizing%20Time%20Lapse%20Videos%20of%20Paintings.html" rel="noopener">Painting Many Pasts - Synthesizing Time Lapse Videos of Paintings</a> &mdash; January  4, 2020 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/video.html">video</a>
            | <small><a href="https://arxiv.org/pdf/2001.01026.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Painting Many Pasts - Synthesizing Time Lapse Videos of Paintings.jpg"><img alt src="./images/showreel/Painting Many Pasts - Synthesizing Time Lapse Videos of Paintings.jpg" /></a></center>
          

          <p>This is a cute one. The idea is simply to see if it’s’ possible to show the “painting progression” of a finished painting. I.e. what might be (photographs) of the steps of making this painting?</p>
<p>I think this is a really neat idea; would love to see some artists try and replicate the steps, to confirm that they (somewhat) match reality!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Generating%20Object%20Stamps.html" rel="noopener">Generating Object Stamps</a> &mdash; January  1, 2020 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/2001.02595.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Generating Object Stamps.jpg"><img alt src="./images/showreel/Generating Object Stamps.jpg" /></a></center>
          

          <p>Here we have the idea of, given some background image, can we fill it with arbitrary objects so that it looks realistic? Here they show that they are able to solve this problem very nicely, by filling a diverse range of wildlife scenes with animals!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/FaceShifter%20-%20Towards%20High%20Fidelity%20And%20Occlusion%20Aware%20Face%20Swapping.html" rel="noopener">FaceShifter - Towards High Fidelity And Occlusion Aware Face Swapping</a> &mdash; December 31, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1912.13457.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/FaceShifter - Towards High Fidelity And Occlusion Aware Face Swapping.jpg"><img alt src="./images/showreel/FaceShifter - Towards High Fidelity And Occlusion Aware Face Swapping.jpg" /></a></center>
          

          <p>I think the results of this one are amazing. They get this to work using a multi-staged network approach; i.e. a few different networks performing different functions, such as one to learn “attributes”, and another to refine the output of the main one. Lots to learn from this!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/SketchTransfer%20-%20A%20Challenging%20New%20Task%20for%20Exploring%20Detail-Invariance%20and%20the%20Abstractions%20Learned%20by%20Deep%20Networks.html" rel="noopener">SketchTransfer - A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks</a> &mdash; December 25, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/dataset.html">dataset</a>
            | <small><a href="https://arxiv.org/pdf/1912.11570.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/SketchTransfer - A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks.jpg"><img alt src="./images/showreel/SketchTransfer - A Challenging New Task for Exploring Detail-Invariance and the Abstractions Learned by Deep Networks.jpg" /></a></center>
          

          <p>This isn’t a network itself; it’s a dataset and a proposed task. The idea of this dataset is that there is definitely a similarity between <em>sketches</em> of real objects, and photos of those objects; but, neural networks aren’t great at knowing this abstraction, yet. I.e. If I have an object-classifier, it can’t very well classify sketches. We’ll see how people progress towards this problem via this dataset!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Visual%20Interactive%20Comparison%20of%20Vector%20Embeddings.html" rel="noopener">Visual Interactive Comparison of Vector Embeddings</a> &mdash; November  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/ux-of-ai.html">ux-of-ai</a>
            | <small><a href="https://arxiv.org/pdf/1911.01542.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Visual Interactive Comparison of Vector Embeddings.jpg"><img alt src="./images/showreel/Visual Interactive Comparison of Vector Embeddings.jpg" /></a></center>
          

          <p>As we cover in the <a href="./6-week-workshop-on-deep-learning.html">6 week workshop</a>, embeddings are the essence of deep learning. So any method to compare and understand them is going to be a big hit here.</p>
<p>As embeddings become more … embedded in all aspects of our AI piplines, tools like this will become increasingly important.</p>
<p>Furthermore, the idea of learning how to <i>explore</i> embedding spaces is also very valuable, and I think this is a useful step in that direction.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Satellite%20Pose%20Estimation%20Challenge.html" rel="noopener">Satellite Pose Estimation Challenge</a> &mdash; November  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/pose.html">pose</a>
            | <small><a href="https://arxiv.org/pdf/1911.02050.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Satellite Pose Estimation Challenge.jpg"><img alt src="./images/showreel/Satellite Pose Estimation Challenge.jpg" /></a></center>
          

          <p>This one is neat because, at least to me, I didn’t even realise this could be a problem. But, it turns out that if you’re in the space business, you might have occasion to be interested in which way your satellite is facing, <i>especially</i> if it’s not responding to your commands any further.</p>
<p>In this work, they introduce a dataset, and competition metrics, and some baselines. Interesting problem!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Dancing%20to%20Music.html" rel="noopener">Dancing to Music</a> &mdash; November  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/music.html">music</a>, <a href="./showreel-tags/pose.html">pose</a>
            | <small><a href="https://arxiv.org/pdf/1911.02001.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Dancing to Music.jpg"><img alt src="./images/showreel/Dancing to Music.jpg" /></a></center>
          

          <p>This is an idea that I worked on a long time ago. The idea is simple: Can we generate a dancing animation in response to music? Indeed, it’s possible.</p>
<p>Here, the contribution is around aligning movements to beats, and other bits of necessary busywork to decouple different musical elements so they can be used by the network.</p>
<p>You can see some generated videos over on their <a href="https://github.com/NVlabs/Dancing2Music">homepage</a>.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/What%20Does%20A%20Network%20Layer%20Hear.html" rel="noopener">What Does A Network Layer Hear</a> &mdash; November  4, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/audio.html">audio</a>
            | <small><a href="https://arxiv.org/pdf/1911.01102.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/What Does A Network Layer Hear.jpg"><img alt src="./images/showreel/What Does A Network Layer Hear.jpg" /></a></center>
          

          <p>This is just a nice idea that I haven’t seen before. We’ve seen plenty of interpretation by <i>visualisation</i>, but nothing in terms of sound! Neat idea.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Making%20an%20Invisibility%20Cloak.html" rel="noopener">Making an Invisibility Cloak</a> &mdash; October 31, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/real-world.html">real-world</a>
            | <small><a href="https://arxiv.org/pdf/1910.14667.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Making an Invisibility Cloak.jpg"><img alt src="./images/showreel/Making an Invisibility Cloak.jpg" /></a></center>
          

          <p>This time it’s a jumper that fools person-detectors. In this work, they do a comparision of how their approach works on many different detectors, and introduce some metrics to contrast them.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Where%20Humans%20Live%20-%20The%20World%20Settlement%20Footprint.html" rel="noopener">Where Humans Live - The World Settlement Footprint</a> &mdash; October 28, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/sustainable-development-goals.html">sustainable-development-goals</a>, <a href="./showreel-tags/real-world.html">real-world</a>
            | <small><a href="https://arxiv.org/pdf/1910.12707.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Where Humans Live - The World Settlement Footprint.jpg"><img alt src="./images/showreel/Where Humans Live - The World Settlement Footprint.jpg" /></a></center>
          

          <p>This is neat. Here, using deep learning, we’re able to compute this “global settlement map”, which would otherwise be totally unfeasible to obtain. A very nice real-world application.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Vision-Infused%20Deep%20Audio%20Inpainting.html" rel="noopener">Vision-Infused Deep Audio Inpainting</a> &mdash; October 24, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/audio.html">audio</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1910.10997.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Vision-Infused Deep Audio Inpainting.jpg"><img alt src="./images/showreel/Vision-Infused Deep Audio Inpainting.jpg" /></a></center>
          

          <p>This is an interesting one. The idea is, we’ve lost some audio information in our video. We still have the video imagery, but a chunk of audio is gone. Can we use the remaining audio <em>and</em> the video, to reconstruct the missing part? The answer is yes. Neat!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Quantifying%20the%20Carbon%20Emissions%20of%20Machine%20Learning.html" rel="noopener">Quantifying the Carbon Emissions of Machine Learning</a> &mdash; October 21, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/sustainable-development-goals.html">sustainable-development-goals</a>
            | <small><a href="https://arxiv.org/pdf/1910.09700.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Quantifying the Carbon Emissions of Machine Learning.jpg"><img alt src="./images/showreel/Quantifying the Carbon Emissions of Machine Learning.jpg" /></a></center>
          

          <p>This is interesting. There’s no deep-learning here; it’s simply a tool, to compare and calculate how much carbon is emitted by various cloud providers and various GPUs. They also give advice on where else you might like to run your computation in order to save emissions.</p>
<p>Check out the <a href="https://mlco2.github.io/impact/">website here</a>.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Evading%20Real-Time%20Person%20Detectors%20by%20Adversarial%20T-shirt.html" rel="noopener">Evading Real-Time Person Detectors by Adversarial T-shirt</a> &mdash; October 18, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1910.11099.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Evading Real-Time Person Detectors by Adversarial T-shirt.jpg"><img alt src="./images/showreel/Evading Real-Time Person Detectors by Adversarial T-shirt.jpg" /></a></center>
          

          <p>There’s a lot of this kind of work going around at the moment. Building on a large body of “adversarial” attacks; the researchers demonstrate that it’s possible to design an image, that can be printed on a t-shirt, that fools person detectors.</p>
<p>A significant aspect of this work was to do with how the shirt moves as the body moves; i.e. the shirt should work in “real-time” (and it does!)</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/A%20Visual%20Analysis%20Tool%20to%20Explore%20Learned%20Representations%20in%20Transformers%20Models.html" rel="noopener">A Visual Analysis Tool to Explore Learned Representations in Transformers Models</a> &mdash; October 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/text%20%28NLP%29.html">text (NLP)</a>, <a href="./showreel-tags/ux-of-ai.html">ux-of-ai</a>
            | <small><a href="http://arxiv.org/pdf/1910.05276.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/A Visual Analysis Tool to Explore Learned Representations in Transformers Models.jpg"><img alt src="./images/showreel/A Visual Analysis Tool to Explore Learned Representations in Transformers Models.jpg" /></a></center>
          

          <p>This is a nice exploration in the space of UX of AI, and the ability to understand what drives particular decisions and outcomes in a model. These days, big text models (a “Transformer” is a certain kind of text model) can be quite hard to understand.</p>
<p>The idea in this paper is to present a UI that can help with this! Nice development. We can expect to see much more of this kind of thing over the years.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Improving%20sample%20diversity%20of%20a%20pre-trained%2C%20class-conditional%20GAN%20by%20changing%20its%20class%20embeddings.html" rel="noopener">Improving sample diversity of a pre-trained, class-conditional GAN by changing its class embeddings</a> &mdash; October 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/art.html">art</a>
            | <small><a href="https://arxiv.org/pdf/1910.04760.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Improving sample diversity of a pre-trained, class-conditional GAN by changing its class embeddings.jpg"><img alt src="./images/showreel/Improving sample diversity of a pre-trained, class-conditional GAN by changing its class embeddings.jpg" /></a></center>
          

          <p>This is mostly a technical result, but it’s really neat for the art applications. One of the problems with a lot of generative networks is that they lack “diversity”. This means that most of the time, the kinds of pictures they generate are mostly the same. This work address this problem, and even more impressively, it addresses it <i>without</i> the need to train the model again!</p>
<p>Very cool.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Dialog%20on%20a%20Canvas%20with%20a%20Machine.html" rel="noopener">Dialog on a Canvas with a Machine</a> &mdash; October 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/real-world.html">real-world</a>
            | <small><a href="https://arxiv.org/pdf/1910.04386.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Dialog on a Canvas with a Machine.jpg"><img alt src="./images/showreel/Dialog on a Canvas with a Machine.jpg" /></a></center>
          

          <p>I really like this one. The main point here is a kind of collaboration between artists and some kind of generative network. I just love anything that brings humans and computers together in creative ways, and I think this is a great example.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/A%20Generative%20Approach%20Towards%20Improved%20Robotic%20Detection%20of%20Marine%20Litter.html" rel="noopener">A Generative Approach Towards Improved Robotic Detection of Marine Litter</a> &mdash; October 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/real-world.html">real-world</a>, <a href="./showreel-tags/sustainable-development-goals.html">sustainable-development-goals</a>
            | <small><a href="http://arxiv.org/pdf/1910.04754.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/A Generative Approach Towards Improved Robotic Detection of Marine Litter.jpg"><img alt src="./images/showreel/A Generative Approach Towards Improved Robotic Detection of Marine Litter.jpg" /></a></center>
          

          <p>This is another neat example of real-world AI. The point here is that working out what is trash or is otherwise a cool-looking sea animal or coral, is surprisingly hard.</p>
<p>One of the issues, as in many real-world situations, a lack of data.</p>
<p>But, by using some standard deep-learning approaches, namely by <i>generating</i> the images they need, they are able to get enough data to train a good model!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Adversarial%20Learning%20of%20Deepfakes%20in%20Accounting.html" rel="noopener">Adversarial Learning of Deepfakes in Accounting</a> &mdash; October  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/time-series.html">time-series</a>
            | <small><a href="https://arxiv.org/pdf/1910.03810.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Adversarial Learning of Deepfakes in Accounting.jpg"><img alt src="./images/showreel/Adversarial Learning of Deepfakes in Accounting.jpg" /></a></center>
          

          <p>This is a neat one because it’s taking a cute idea and adapting it to a (very exciting!) business process. Namely, the financial auditing of a business.</p>
<p>The point here is that, by being able to <i>generate</i> accounting records, we might be able to mislead auditors; or at least some computer-based auditing assessments. The idea would be, by using this kind of approach, one could construct misleading journal entries.</p>
<p>I think it’s really interesting to see this kind of adversarial work move out of the domain of images and into the finance world! I’m sure we’ll see more of these kind of general adversarial networks over coming years!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Prose%20for%20a%20Painting.html" rel="noopener">Prose for a Painting</a> &mdash; October  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/funny.html">funny</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/text%20%28NLP%29.html">text (NLP)</a>
            | <small><a href="http://arxiv.org/pdf/1910.03634.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Prose for a Painting.jpg"><img alt src="./images/showreel/Prose for a Painting.jpg" /></a></center>
          

          <p>This is a cute, simple application. The idea is: How can we get cool Shakespeare-style descriptions of paintings? The answer: We can use language “style-transfer” to change a real poem about the painting into the Shakespearean style. That’s it!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Learn%20to%20explain%20efficiently%20via%20neural%20logic%20inductive%20learning.html" rel="noopener">Learn to explain efficiently via neural logic inductive learning</a> &mdash; October  6, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/explainability%20%28XAI%29.html">explainability (XAI)</a>, <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/ux-of-ai.html">ux-of-ai</a>
            | <small><a href="https://arxiv.org/pdf/1910.02481.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learn to explain efficiently via neural logic inductive learning.jpg"><img alt src="./images/showreel/Learn to explain efficiently via neural logic inductive learning.jpg" /></a></center>
          

          <p>Along the lines of explainability, this is a fascinating and important step. Consider: How do you know that something is a car? What makes a person a person? This paper attempts to address this problem by a kind of “constructivist” approach. I.e. a car is a car because it has wheels.</p>
<p>I think this kind of logical reasoning will be increasingly important as models make richer decisions, and as we seek to get solid explanations from our models.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Addressing%20Failure%20Prediction%20by%20Learning%20Model%20Confidence.html" rel="noopener">Addressing Failure Prediction by Learning Model Confidence</a> &mdash; October  1, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1910.04851.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Addressing Failure Prediction by Learning Model Confidence.jpg"><img alt src="./images/showreel/Addressing Failure Prediction by Learning Model Confidence.jpg" /></a></center>
          

          <p>This is topic close to my heart. I’m very interested in thinking about how we interpret the predictions of ML models, and if a model can even give any kind of meaningful confidence prediction at all. This paper attempts to build this idea in. I’m sure it’s not the last word, but it’s a good step in a line of research that I think is very important.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Raiders%20of%20the%20Lost%20Art.html" rel="noopener">Raiders of the Lost Art</a> &mdash; September 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/real-world.html">real-world</a>
            | <small><a href="https://arxiv.org/pdf/1909.05677.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Raiders of the Lost Art.jpg"><img alt src="./images/showreel/Raiders of the Lost Art.jpg" /></a></center>
          

          <p>This is a super neat, practical application that I love. The authors address a neat problem: suppose some canvas has some art has been painted over. Can we recover that painting? Of course, given that some information is lost, we can only go so far, but they neatly use Style Transfer to re-paint the lost painting in the style of the supposed artist. Really cool!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Gravity%20as%20a%20Reference%20for%20Estimating%20a%20Person%E2%80%99s%20Height%20from%20Video.html" rel="noopener">Gravity as a Reference for Estimating a Person’s Height from Video</a> &mdash; September  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/physics.html">physics</a>, <a href="./showreel-tags/pose.html">pose</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1909.02211.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Gravity as a Reference for Estimating a Person’s Height from Video.jpg"><img alt src="./images/showreel/Gravity as a Reference for Estimating a Person’s Height from Video.jpg" /></a></center>
          

          <p>I love this one for the awesome idea they’ve had. They want to solve a reasonable problem - how tall is the person in this photo (here, actually, a video). They’re observation is that if they ask the person to <em>jump</em>, then they can use information about gravity as supporting evidence to estimate the height! It’s such a neat idea.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Translating%20Visual%20Art%20into%20Music.html" rel="noopener">Translating Visual Art into Music</a> &mdash; September  3, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/music.html">music</a>
            | <small><a href="https://arxiv.org/pdf/1909.01218.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Translating Visual Art into Music.jpg"><img alt src="./images/showreel/Translating Visual Art into Music.jpg" /></a></center>
          

          <p>This one is weird and I love it. The idea is to translate a painting, let’s say, into music, so that vision-impaired people can learn to enjoy paintings. This one is fascinating in that it tries to ensure that something like the original image can be reconstructed from the stream of music. They phrase art, then, as an information exchange. Crazy!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Enforcing%20Analytic%20Constraints%20in%20Neural-Networks%20Emulating%20Physical%20Systems.html" rel="noopener">Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems</a> &mdash; September  3, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/physics.html">physics</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1909.00912.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems.jpg"><img alt src="./images/showreel/Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems.jpg" /></a></center>
          

          <p>Simliar to the “MIPs as a Layer” paper, this one is again about enforcing constraints into neural networks, but here the view is that we want to do this so that it respects the physical constraints of physical systems. As we attempt to solve more Physics problems with deep learning, we’ll see much more of this. I think it’s exciting!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Story-oriented%20Image%20Selection%20and%20Placement.html" rel="noopener">Story-oriented Image Selection and Placement</a> &mdash; September  2, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/text%20%28NLP%29.html">text (NLP)</a>
            | <small><a href="https://arxiv.org/pdf/1909.00692.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Story-oriented Image Selection and Placement.jpg"><img alt src="./images/showreel/Story-oriented Image Selection and Placement.jpg" /></a></center>
          

          <p>This one is a neat and interesting idea. The point is, consider a paragraph of text. What image should accompany that text? It turns out, you can solve this problem by learning what’s in images, and what the text is talking about. I think this is something we’ll probably see more of over the years; aligned content generation, i.e. “this goes with that”.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Visual%20Deprojection.html" rel="noopener">Visual Deprojection</a> &mdash; September  1, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/compression.html">compression</a>
            | <small><a href="https://arxiv.org/pdf/1909.00475.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Visual Deprojection.jpg"><img alt src="./images/showreel/Visual Deprojection.jpg" /></a></center>
          

          <p>This is a moderately quirky one. The idea is that we take some high-dimensional information (say, 3D) and then project it down to 2D. The question is, how well can we reconstruct the original input? Turns out, pretty well. This idea can be useful when thinking about how to compress high-dimensional data.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Real-world%20Conversational%20AI%20for%20Hotel%20Bookings.html" rel="noopener">Real-world Conversational AI for Hotel Bookings</a> &mdash; August 27, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/real-world.html">real-world</a>, <a href="./showreel-tags/text%20%28NLP%29.html">text (NLP)</a>
            | <small><a href="https://arxiv.org/pdf/1908.10001.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Real-world Conversational AI for Hotel Bookings.jpg"><img alt src="./images/showreel/Real-world Conversational AI for Hotel Bookings.jpg" /></a></center>
          

          <p>We don’t feature a lot of chatbots here, but I thought this one was interesting because of how real-world focused it is. If you check the paper you’ll see that they have an explicit consideration for when a human should take over a convseration. Furthermore, this is a real system deployed in the real world. Furthermore, they should how they’ve utilised some cutting-edge models (BERT). It’s good to see this kind of real-world design.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Physics%20Informed%20Data%20Driven%20model%20for%20Flood%20Prediction.html" rel="noopener">Physics Informed Data Driven model for Flood Prediction</a> &mdash; August 23, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/real-world.html">real-world</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1908.10312.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Physics Informed Data Driven model for Flood Prediction.jpg"><img alt src="./images/showreel/Physics Informed Data Driven model for Flood Prediction.jpg" /></a></center>
          

          <p>This is one of a series of works that is interested in explicitly incorporating Physics into deep-learning based models. I think these ideas are really interesting and well worth exploring. Here they aim at speeding up computations by combining a GAN with standard simulation tools.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Federated%20Learning.html" rel="noopener">Federated Learning</a> &mdash; August 21, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/real-world.html">real-world</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1908.07873.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Federated Learning.jpg"><img alt src="./images/showreel/Federated Learning.jpg" /></a></center>
          

          <p>This one is interesting from a real-world/privacy angle. We know that we’re going to see more and more deep learning on phones. And we’re going to have our phones start to adapt to us. But we’d also like to know how to leverage data from <em>other</em> peoples activities, but in a private way. How can we train things in this regime? How should distributed training even work? This paper presents some current research, challenges, and ideas for the future.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/A%20Low-Cost%2C%20Open-Source%20Robotic%20Racecar%20for%20Education%20and%20Research.html" rel="noopener">A Low-Cost, Open-Source Robotic Racecar for Education and Research</a> &mdash; August 21, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/reinforcement-learning%20%28RL%29.html">reinforcement-learning (RL)</a>, <a href="./showreel-tags/robotics.html">robotics</a>
            | <small><a href="https://arxiv.org/pdf/1908.08031.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/A Low-Cost, Open-Source Robotic Racecar for Education and Research.jpg"><img alt src="./images/showreel/A Low-Cost, Open-Source Robotic Racecar for Education and Research.jpg" /></a></center>
          

          <p>This is a neat one for those wishing for more car-based deep learning projects. I myself have something similar to this, but with nowhere near the bells and whistles that this one has. This had LiDAR, RGBD camera, and even a collison-indicator! All up, a super cool project, that’s totally open-source!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Towards%20Arbitrary%20High%20Fidelity%20Face%20Manipulation.html" rel="noopener">Towards Arbitrary High Fidelity Face Manipulation</a> &mdash; August 20, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1908.07191.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/ Towards Arbitrary High Fidelity Face Manipulation.jpg"><img alt src="./images/showreel/ Towards Arbitrary High Fidelity Face Manipulation.jpg" /></a></center>
          

          <p>Every so often I stumble across a paper where, when they show the results, I don’t quite believe them. This is one of those papers. The manipulated photos in this work look so realistic to me that I’m <em>still</em> amazed! Very cool work. They are able to take in single-images, and then richly manipulate them to desired expressions!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Deep%20Sketch-based%203D%20Hair%20Modeling.html" rel="noopener">Deep Sketch-based 3D Hair Modeling</a> &mdash; August 20, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/fashion.html">fashion</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1908.07198.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Deep Sketch-based 3D Hair Modeling.jpg"><img alt src="./images/showreel/Deep Sketch-based 3D Hair Modeling.jpg" /></a></center>
          

          <p>This is one my long-time dreams. I’ve often wanted something to help me generate different haircuts. What this work introduces, is the ability to draw a rough sketch of the hair, and have the model produce a full, rich, 3D model of the hair! Amazing work.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Video%20synthesis%20of%20human%20upper%20body%20with%20realistic%20face.html" rel="noopener">Video synthesis of human upper body with realistic face</a> &mdash; August 19, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="http://arxiv.org/pdf/1908.06607.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Video synthesis of human upper body with realistic face.jpg"><img alt src="./images/showreel/Video synthesis of human upper body with realistic face.jpg" /></a></center>
          

          <p>We’re seeing a lot of work like this with the “#DeepFake” meme. The point here is that they’re completely generating (somewhat) arbitrary poses of people sitting at desks, talking. I.e. with generate hand-movements, body movements, head movements, and mouth movements. This work is just yet another that makes a significant contribution to this meme. Expect these fake videos to become better and better, and harder to detect.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Learning%20to%20Dress%203D%20People%20from%20Images.html" rel="noopener">Learning to Dress 3D People from Images</a> &mdash; August 19, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/fashion.html">fashion</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1908.06903.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning to Dress 3D People from Images.jpg"><img alt src="./images/showreel/Learning to Dress 3D People from Images.jpg" /></a></center>
          

          <p>A classic aim of the fashion retailer; this paper introduces some work which lets you clothe a 3D model from 2D images. We will undoubtedly see this kind of work on fashion websites very soon.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/3D%20Object%20Instance%20Re-Localization.html" rel="noopener">3D Object Instance Re-Localization</a> &mdash; August 16, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1908.06109.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/3D Object Instance Re-Localization.jpg"><img alt src="./images/showreel/3D Object Instance Re-Localization.jpg" /></a></center>
          

          <p>This is an odd one but probably important to someone. Suppose that you can compute a 3D object instance segmentation. I.e. you can locate where an object is in 3D. Then, given a <em>different view</em> of the scene, can you find the objects again? This is important, probably, because succeeding at the instance segmentation at arbitrary views of the scene is harder, you might think, then if you use your prior knowledge about the objects positions, having seen them before.</p>
<p>For me this is an interesting work related to the general idea of “temporal coherence”; i.e. using knowledge that we know from a previous timestep at a new timestep.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Differentiable%20Reasoning.html" rel="noopener">Differentiable Reasoning</a> &mdash; August 13, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1908.04700.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Differentiable Reasoning.jpg"><img alt src="./images/showreel/Differentiable Reasoning.jpg" /></a></center>
          

          <p>I really like the ideas of this one. There’s a bit of work going around this idea: contextual knowledge should help us. Here, the formalise this idea that we can do better at classification if we know other things about what we’re trying to classify. In the photo the example is that we can do better a deciding if something is a cushion if we know that it’s at the very least part of a chair.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Image%20Inpainting%20via%20Structure-aware%20Appearance%20Flow.html" rel="noopener">Image Inpainting via Structure-aware Appearance Flow</a> &mdash; August 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1908.03852.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Image Inpainting via Structure-aware Appearance Flow.jpg"><img alt src="./images/showreel/Image Inpainting via Structure-aware Appearance Flow.jpg" /></a></center>
          

          <p>Inpainting is a classic task in computer vision: Given some empty area of an otherwise complete image, can you figure out what should be there? This work is interesting because they realise that “structure” is important when thinking about solving this problem. In other words, you can say things like “This part, and this part should be the same”. It turns out that if they build a network with this consideration baked in, then they can do really well at this problem!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Visual%20Search%20at%20Pinterest.html" rel="noopener">Visual Search at Pinterest</a> &mdash; August  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/technical.html">technical</a>, <a href="./showreel-tags/visual-search.html">visual-search</a>, <a href="./showreel-tags/real-world.html">real-world</a>
            | <small><a href="https://arxiv.org/pdf/1908.01707.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Visual Search at Pinterest.jpg"><img alt src="./images/showreel/Visual Search at Pinterest.jpg" /></a></center>
          

          <p>This is a nice one, as it’s a very production-focused example of how Pinterest deploys different “visual search” capabilities. It’s interesting to see how they set up their training and deployment environments.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/MaskGAN%20-%20Diverse%20and%20Interactive%20Facial%20Image%20Manipulation.html" rel="noopener">MaskGAN - Diverse and Interactive Facial Image Manipulation</a> &mdash; July 27, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1907.11922.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/MaskGAN - Diverse and Interactive Facial Image Manipulation.jpg"><img alt src="./images/showreel/MaskGAN - Diverse and Interactive Facial Image Manipulation.jpg" /></a></center>
          

          <p>A classic idea of the times; here we have a network that is able to take as input a face, and then modify it according to some simple mask. I.e. you can draw exactly where in the picture you’d like a “smile” to be. They show how they can learn to manipulate many parameters in this way!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/On%20the%20%E2%80%9Csteerability%E2%80%9D%20of%20generative%20adversarial%20networks.html" rel="noopener">On the “steerability” of generative adversarial networks</a> &mdash; July 16, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1907.07171.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/On the “steerability” of generative adversarial networks.jpg"><img alt src="./images/showreel/On the “steerability” of generative adversarial networks.jpg" /></a></center>
          

          <p>This is a cool paper. We’ve seen lots of generative work from “Generative Adversarial Networks (GANs)”. In this work, they explore how “controllable” such networks are. I.e., can we generate a picture of a dog, and <em>then</em> zoom in on it’s face? Can we generate a building and change it from night to day? They perform some investigations in this area, and show that there is lots to be done, but solving these kinds of problems will become very important as we see these generative networks used more widely.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Autonomous%20Driving%20in%20the%20Lung.html" rel="noopener">Autonomous Driving in the Lung</a> &mdash; July 16, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1907.08136.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Autonomous Driving in the Lung.jpg"><img alt src="./images/showreel/Autonomous Driving in the Lung.jpg" /></a></center>
          

          <p>This is some neat work. First, they use data from a patients on medical scan. Then, the learn how to navigate in this rich 3d world from video images. Prety cool!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Mixed%20Integer%20Program%20as%20a%20Layer.html" rel="noopener">Mixed Integer Program as a Layer</a> &mdash; July 12, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1907.05912.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Mixed Integer Program as a Layer.jpg"><img alt src="./images/showreel/Mixed Integer Program as a Layer.jpg" /></a></center>
          

          <p>MIPs are close to my heart; and I really enjoy papers that combine many techniques together. This one is interesting because, again, it’s this idea of combining constraints into neural networks, and in particular bringing this information into the optimization of the overall network.</p>
<p>I’m not sure this idea has reached peak popularity yet, and it’s great to see more things being squeezed into the general capability of these deep networks.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Learning%20to%20Self-Correct%20from%20Demonstrations.html" rel="noopener">Learning to Self-Correct from Demonstrations</a> &mdash; July 12, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/reinforcement-learning%20%28RL%29.html">reinforcement-learning (RL)</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1907.05634.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning to Self-Correct from Demonstrations.jpg"><img alt src="./images/showreel/Learning to Self-Correct from Demonstrations.jpg" /></a></center>
          

          <p>This one is a bit technical, but the main idea here is that they are able to “moderate” how reinforcement-learning networks will extrapolate, when they are learning by example. An analogy would be that, when you watch someone take a sip from a cup, you assume “brilliant, I can drink from any thing that I am holding”, and then you try and drink from a pen, or a book, or such. Here, they introduce the idea that perhaps you should act a bit conservatively in areas where you are unsure, such as holding new things.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Hello%2C%20It%27s%20GPT-2%20-%20How%20Can%20I%20Help%20You.html" rel="noopener">Hello, It's GPT-2 - How Can I Help You</a> &mdash; July 12, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/text%20%28NLP%29.html">text (NLP)</a>, <a href="./showreel-tags/ux-of-ai.html">ux-of-ai</a>
            | <small><a href="https://arxiv.org/pdf/1907.05774.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
          <center><a href="./images/showreel/Hello, It's GPT-2 - How Can I Help You.jpg"><img alt src="./images/showreel/Hello, It's GPT-2 - How Can I Help You.jpg" width="500" /></a></center>
          

          <p>This is an interesting one. They use the now-famous <a href="https://github.com/openai/gpt-2">GPT-2 network</a> to help them understand queries from users; they then build a sense of “belief” about what the user wants (in the image you can see the system learning they want a “hotel” that it “expensive” in the “center of town”. Then, from that belief, they generate a text response. This is our usual kind of favourite thing: the combination of many techniques to produce a nice result.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Eliminating%20Forced%20Work%20via%20Satellites.html" rel="noopener">Eliminating Forced Work via Satellites</a> &mdash; July 12, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/real-world.html">real-world</a>, <a href="./showreel-tags/sustainable-development-goals.html">sustainable-development-goals</a>
            | <small><a href="https://arxiv.org/pdf/1907.05552.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Eliminating Forced Work via Satellites.jpg"><img alt src="./images/showreel/Eliminating Forced Work via Satellites.jpg" /></a></center>
          

          <p>This is an amazing application. The researchers train a fairly standard detection network, “resnet”, and train it to detect certain objects in satellite imagery. Here, though, what they are detecting are “brick kilns”; places where there may be forced labour. By helping identify these locations, they can then be referred to the authorities!</p>
<p>This is a beautiful application of deep learning, and the authors note that they are also addressing one of the <a href="https://sustainabledevelopment.un.org/">UNs sustainable development goals</a>!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Generative%20Choreography.html" rel="noopener">Generative Choreography</a> &mdash; July 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1907.05297.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Generative Choreography.jpg"><img alt src="./images/showreel/Generative Choreography.jpg" /></a></center>
          

          <p>Here the use a standard tool from text processing, the “Long Short-Term Memory (LSTM)” network to watch dance sequences and generate new ones. This is something I’m personally very interested in, and in fact have done work in before! So it’s nice to see some more contributions to this area.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Synthetic%20fruit.html" rel="noopener">Synthetic fruit</a> &mdash; July 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1907.04759.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Synthetic fruit.jpg"><img alt src="./images/showreel/Synthetic fruit.jpg" /></a></center>
          

          <p>This is an old idea, and just one example among many. There’s nothing inherently outstanding in this paper, but we just wanted to note the very useful technique of using “fake” (synthetic) data to help solve real-world problems. This is a very useful technique, especially in light of the remarkable abilities of <a href="./posts/2018-12-17-How-Much-Data-For-Retraining.html">transfer learning</a> to help us adapt to new data.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Out-of-Distribution%20Detection%20using%20Generative%20Models.html" rel="noopener">Out-of-Distribution Detection using Generative Models</a> &mdash; July 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1907.04572.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Out-of-Distribution Detection using Generative Models.jpg"><img alt src="./images/showreel/Out-of-Distribution Detection using Generative Models.jpg" /></a></center>
          

          <p><a href="./posts/2018-12-17-How-Much-Data-For-Retraining.html">In an old blogpost</a> we discussed the problem of networks making over-confident predictions. This paper focused on over-confidence on images that the network has <em>never</em> seen (i.e. trained on cats and dogs, then very confident that a picture of a boat is a dog).</p>
<p>A classical idea (we saw it in the “Detecting the Unexpected” paper) is that if we think about how well we can <em>reconstruct</em> a given image, that might tell us something about how often our network has seen it; i.e. if it’s “in-dstribution” or not.</p>
<p>This paper notes that one problem with that idea is that if the thing we’re looking at is “simple” (technically, has “small variance”), then because the generative models are powerful, they might still do a good job.</p>
<p>The approach they provide in the paper is to use a different kind of generative network, the so-called “Neural Rendering Model (NRM)”, to do the image generation, and that this new technique just happens to be better at being informative when the data is from a set the network has never seen.</p>
<p>The picture above shows that the NRM-approach does quite a good job of seperating between images the network has seen and hasn’t seen.</p>
<p>This is a bit of a technical result, but it’s a crucially important area of research for networks that are going to be used in the real world.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Learning%20to%20understand%20videos%20for%20answering%20questions.html" rel="noopener">Learning to understand videos for answering questions</a> &mdash; July 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/text%20%28NLP%29.html">text (NLP)</a>, <a href="./showreel-tags/visual-question-answering.html">visual-question-answering</a>
            | <small><a href="https://arxiv.org/pdf/1907.04553.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning to understand videos for answering questions.jpg"><img alt src="./images/showreel/Learning to understand videos for answering questions.jpg" /></a></center>
          

          <p>Videos are becoming increasingly prolific on the internet. Naturally, then, it makes sense that researchers are spending time trying to understand them. One particular area of research is so-called “Visual queastion-answering”. The point is to train a network to be able to watch a video, then answer questions (via text) about what happened in the video. Some examples are provided in the image above.</p>
<p>This work introduces a nice idea to this area, one that we’re seeing frequently on the showreel, namely: building up a rich representation first, and then using that representation to further refine answers. This should be a bit similar, conceptually, to the “Scene Graph” work, for example.</p>
<p>It’s also neat that the researchers are from <a href="https://www.deakin.edu.au/">Deakin</a>!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/What-If%20...%20We%20could%20interactively%20understand%20ML%20Models.html" rel="noopener">What-If ... We could interactively understand ML Models?</a> &mdash; July  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/visualisation.html">visualisation</a>
            | <small><a href="https://arxiv.org/pdf/1907.04135.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
          <center><a href="./images/showreel/What-If ... We could interactively understand ML Models.jpg"><img alt src="./images/showreel/What-If ... We could interactively understand ML Models.jpg" width="800" /></a></center>
          

          <p>This is some software that Google put out a few years ago under a different name (it was called “facets”). This specific tool I’m not so convinced on, but it’s a very good attempt to tackle a very important idea — how bias and decision-making can be understood interactively.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Machine%20Learning%20for%20Side%20Channel%20Attacks.html" rel="noopener">Machine Learning for Side Channel Attacks</a> &mdash; July  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/privacy.html">privacy</a>
            | <small><a href="https://arxiv.org/pdf/1907.04428.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Machine Learning for Side Channel Attacks.jpg"><img alt src="./images/showreel/Machine Learning for Side Channel Attacks.jpg" /></a></center>
          

          <p>This is a quirky one, but it’s kind of “flag-planting” in the ML/Security world. For years, security researchers have spent time finding what they call “side-channel” attacks. An example is, say, listening to the soup that someone makes when typing, and from that sound, working out <em>what they are typing</em>. It’s called “side-channel” because it’s not, say, capturing the keystrokes via the computer, it’s via an additional “channel”.</p>
<p>The main point of this paper is that they’re applying standard ML techniques, in particular in regards to voltage, and are able to make an estimate of which applications are running on a given piece of hardware. This might not sound super useful as it is, but, as always in the security world, there’s much more juice to be squeezed here.</p>
<p>This will definitely be a space to watch in the security space - bringing in AI techniques to enhance our offensive security capabilities!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Designing%20User%20Interfaces%20that%20Allow%20Everyone%20to%20Contribute%20to%20AI%20Safety.html" rel="noopener">Designing User Interfaces that Allow Everyone to Contribute to AI Safety</a> &mdash; July  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/ethics.html">ethics</a>, <a href="./showreel-tags/ux-of-ai.html">ux-of-ai</a>
            | <small><a href="https://arxiv.org/pdf/1907.04446.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Designing User Interfaces that Allow Everyone to Contribute to AI Safety.jpg"><img alt src="./images/showreel/Designing User Interfaces that Allow Everyone to Contribute to AI Safety.jpg" /></a></center>
          

          <p>Improving the situation around AI Ethics is strongly on our agenda at the Braneshop. This paper highlights an interesting situation: suppose you have people who want to provide feedback to some decision making process; what should the interface they use look like?</p>
<p>Here they explore a potential design that allows people to see the <em>impact</em> of their actions in a variety of ways.</p>
<p>This won’t be the last word on the matter, but it’s a nice contribution to the field, and hopefully pushes people to think very hard about this problem.</p>
<p>This is one bit of work in a growing field we refer to as “The UX of AI”. This will definitely be a huge area over the coming years.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Linking%20Art%20through%20Human%20Poses.html" rel="noopener">Linking Art through Human Poses</a> &mdash; July  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/pose.html">pose</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1907.03537.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Linking Art through Human Poses.jpg"><img alt src="./images/showreel/Linking Art through Human Poses.jpg" /></a></center>
          

          <p>This one is cool for the kind of neat technique it demonstrates. They use a pose network (something that just looks at an image of a person, say, and estimates what their skeleton looks like; i.e. it tries to guess some straight lines that connect their arms and legs and such) to connect different artworks. It’s a neat application of what is becoming a standard technique.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Estimating%20travel%20time%20without%20roads.html" rel="noopener">Estimating travel time without roads</a> &mdash; July  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1907.03381.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Estimating travel time without roads.jpg"><img alt src="./images/showreel/Estimating travel time without roads.jpg" /></a></center>
          

          <p>Again, a neat idea applied well. In this paper they suppose that, in fact, we don’t need detailed road networks to do reasonably well at estimating travel time. We just need to get a vague feeling for the kinds of areas we’ll be travelling though (i.e. highway, commercial, residential, country, park, urban, etc). They make these ideas precise and get some great results!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Action%20Recognition%20from%20Poses.html" rel="noopener">Action Recognition from Poses</a> &mdash; July  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/pose.html">pose</a>
            | <small><a href="https://arxiv.org/pdf/1907.03520.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Action Recognition from Poses.jpg"><img alt src="./images/showreel/Action Recognition from Poses.jpg" /></a></center>
          

          <p>A pretty standard, but useful, technique that uses a kind of multi-stage process to: 1) compute the pose, 2) then from the a series of these poses, ver time, work out what “action” people are performing. Specifically here they focus on people going past train ticket machines in various ways, but the application is general.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Albatrosses%20from%20Space.html" rel="noopener">Albatrosses from Space</a> &mdash; July  3, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/science.html">science</a>
            | <small><a href="https://arxiv.org/pdf/1907.02040.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Albatrosses from Space.jpg"><img alt src="./images/showreel/Albatrosses from Space.jpg" /></a></center>
          

          <p>A really nice scientific application of deep learning; and something that maybe any reasonable person would not assume is possible right now. We like this one because it’s the overlap of modern deep learning techniques to old (but important!) problems of tracking animal movements for conservation reasons.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card" style="width: 550px;">
        
        <h5> <a href="./showreel/AI%20for%20Economic%20Uplift%20of%20Handicraft.html" rel="noopener">AI for Economic Uplift of Handicraft</a> &mdash; May 31, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>
            | <small><a href="https://arxiv.org/pdf/1907.02014.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/AI for Economic Uplift of Handicraft.jpg"><img alt src="./images/showreel/AI for Economic Uplift of Handicraft.jpg" /></a></center>
          

          <p>While this one isn’t strictly using deep learning, it does use some classical machine learning techniques. But the reason we consider it particularly cool, is because the authors actually took their system “to the streets”, as it were, and verified that using the new design processes helped the artisans sell more items!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Attacking%20person-identification%20with%20patches.html" rel="noopener">Attacking person-identification with patches</a> &mdash; April 18, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/privacy.html">privacy</a>
            | <small><a href="https://arxiv.org/pdf/1904.08653.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Attacking person-identification with patches.jpg"><img alt src="./images/showreel/Attacking person-identification with patches.jpg" /></a></center>
          

          <p>The game in this one is - can we make a picture, that can be printed and held in front of us, that will fool a person-detector? Yes, it turns out.</p>
<p>This is refered to as an “adversarial” attack, and they have gained a lot of attention recently. This one in particular is interesting because they attack a standard person-detector (so-called “Yolo”) and the image they use is “local” and “printable”. There had been a few results in this area, but nothing attacking person detectors.</p>
<p>In the research world, we’re seeing work on both fronts. There are a lot of work on how to do more of these, and make them more robust, and likewise there is a lot of work on how to make classifiers and detectors less vulnerable to such attacks. Who will win? It’s not clear. I’d put my money on it always being possible to make such attacks, given enough information on the classifier. But, the cost of such attacks will rise significantly, making it unfeasible for most of us.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Detecting%20the%20Unexpected.html" rel="noopener">Detecting the Unexpected</a> &mdash; April 16, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1904.07595.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Detecting the Unexpected.jpg"><img alt src="./images/showreel/Detecting the Unexpected.jpg" /></a></center>
          

          <p>This is a really neat and important idea. The application here is in self-driving cars, but the central idea is very general. The main point is, if we’ve trained a network to detect certain classes of thing (“car”, “road”, “person”, “truck”) then, if it sees something completely unexpected, (“goose”), what will it predict? Depending on how you set up the network, it will predict one of the known classes. This work is about quantifying how confident the network should feel about such prediction. Their idea is to ask the network to think about how well it can reconstrut the thing it thought it saw. If it finds it hard, then that indicates that the thing it saw is moderately unknown to it, and so it shouldn’t be confident. As we have more AI out in real life making decisions, quantifying uncertainty will become increasingly important.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Expressive%203D%20Body%20Capture%20from%20a%20Single%20Image.html" rel="noopener">Expressive 3D Body Capture from a Single Image</a> &mdash; April 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/pose.html">pose</a>
            | <small><a href="https://arxiv.org/pdf/1904.05866.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Expressive 3D Body Capture from a Single Image.jpg"><img alt src="./images/showreel/Expressive 3D Body Capture from a Single Image.jpg" /></a></center>
          

          <p>More and more we’re seeing deep learning tackle rich reconstruction problems from simple inputs. This is a classic of the genre. As humans, we can easily imagine the 3D structure of the person in the photo; and it turns out now deep learning can do the same, via the techniques in this paper. It’s very impressive work, and is applicable for those people wishing to capture this information without a complicated set up of a 3D body scanner. As usual, the typical applications will be in retail, but maybe also augmented-reality and other such fun things. As is the case with all these body-pose-related papers, they use an underlying pose network and build on top of it’s outputs. This is also a central and important topic in modern AI: building up rich and strong capabilities by combining different techniques.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Extreme%20Image%20Compression.html" rel="noopener">Extreme Image Compression</a> &mdash; April  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1904.03851.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Extreme Image Compression.jpg"><img alt src="./images/showreel/Extreme Image Compression.jpg" /></a></center>
          

          <p>A natural thought would be that if we know a lot about the thing we’re trying to compress, we can do a better job. Standard compression algorithms are general-purpose, and as such, there is probably room to improve. This is the observation and work in this paper: They <em>learn</em> a compression function for a specific set of data, and they do really well! Probably not suitable for most of us, but you can be sure the big data storage providers will be working on these kinds of techniques into the future.</p>
<p>If we wanted to be trendy we could summarise this as “big data makes small data”.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card" style="width: 600px;">
        
        <h5> <a href="./showreel/Robot-Movie-Director.html" rel="noopener">Can a Robot Become a Movie Director?</a> &mdash; April  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/drones.html">drones</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://scirate.com/arxiv/1904.02579">Paper</a></small>
          </span>
        

        <div class="content">
          
          <center><a href="./images/showreel/1904.02579.png"><img alt src="./images/showreel/1904.02579.png" width="500" /></a></center>
          

          <p>The main point here is that if we’re interested in determining where to point a drone while filming some scene, it might be hard, because the director would need to be able to somehow see everything, while the drone is flying. This paper proposes that perhaps thee could be a method to have the drone know where to look.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Image2StyleGan%20-%20aka%20Ryan%20Obama%20aka%20Oprah%20Johansson.html" rel="noopener">Image2StyleGan - aka Ryan Obama aka Oprah Johansson</a> &mdash; April  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1904.03189.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Image2StyleGan - aka Ryan Obama aka Oprah Johansson.jpg"><img alt src="./images/showreel/Image2StyleGan - aka Ryan Obama aka Oprah Johansson.jpg" /></a></center>
          

          <p>One of the most exciting areas of AI is the generative/creative opportunities. And in this area, something people are always fascinated by is the exploring the “space” of images; i.e here are all the photos of people, but what does a person who is “halfway between these two people” look like? This paper works on that problem, and produces some very cool looking people such as Ryan Obama, Oprah Johansson and Hugh de Niro. Notably, in this paper it seems like it doesn’t work so well for abstract/non-person style photos; but that’s probably due to the data, and not a general problem.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Learning%20how%20music%20and%20images%20relate.html" rel="noopener">Learning how music and images relate</a> &mdash; March 30, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/music.html">music</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1904.00150.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning how music and images relate.jpg"><img alt src="./images/showreel/Learning how music and images relate.jpg" /></a></center>
          

          <p>This result is nice because it’s using a concept that we think is so important, we’ve made it a central part of our <a href="./6-week-workshop-on-deep-learning.html">technical workshop</a>: the autoencoder.</p>
<p>In this work they map images and music into the same “space” (i.e. points on the graph in the picture), and in-so-doing, they can learn when images and music are related! Nice, simple, and useful!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Detecting%20people%20using%20only%20WiFi.html" rel="noopener">Detecting people using only WiFi</a> &mdash; March 30, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/pose.html">pose</a>, <a href="./showreel-tags/privacy.html">privacy</a>
            | <small><a href="https://arxiv.org/pdf/1904.00276.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Detecting people using only WiFi.jpg"><img alt src="./images/showreel/Detecting people using only WiFi.jpg" /></a></center>
          

          <p>This is an interesting one. WiFi is everywhere; and probably a reasonable person wouldn’t assume they could be tracked (down to estimates of where they are walking, and the overall pose of their body) if there isn’t a camera around. But it turns out that this data actually <em>can</em> be gathered in (an ideal) WiFi set up. That is, the pose of people was determined <em>without</em> a camera; using <em>only</em> WiFi signals. No doubt this field - sensing human activity through non-camera based sensors - will continue to grow.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Face%20Synthesis%20from%20a%20Single%20Image.html" rel="noopener">Face Synthesis from a Single Image</a> &mdash; March 26, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1903.10873.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Face Synthesis from a Single Image.jpg"><img alt src="./images/showreel/Face Synthesis from a Single Image.jpg" /></a></center>
          

          <p>Ignoring the specific contributions, this is a conceptually simple paper; but the results look amazing. The idea is: can we find a 3D model from a single image? And how much detail can it capture?</p>
<p>Turns out, heaps of detail! They introduce some nice techniques for modelling the facial features and such, but the main thing I like are the results.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Unconstrained%20Ear%20Recognition.html" rel="noopener">Unconstrained Ear Recognition</a> &mdash; March 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/funny.html">funny</a>
            | <small><a href="https://arxiv.org/pdf/1903.04143.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Unconstrained Ear Recognition.jpg"><img alt src="./images/showreel/Unconstrained Ear Recognition.jpg" /></a></center>
          

          <p>Trust no-one. If you think covering your face is enough to stop <a href="https://en.wikipedia.org/wiki/Ferengi">people</a> from detecting who you are, you’re wrong. It turns out it’s possible to identify people from their ears. Why would anyone want to do this? Who knows. But it’s happening!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Finding%20small%20objects%20in%20a%20large%20scene.html" rel="noopener">Finding small objects in a large scene</a> &mdash; February  6, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1902.05387.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Finding small objects in a large scene.jpg"><img alt src="./images/showreel/Finding small objects in a large scene.jpg" /></a></center>
          

          <p>Satellite imagery is a hot topic. There’s been many stories of people using such imagery to gain competitive advantage in many ways; from estimating the number of sales at department stores, to prediction crop yield.</p>
<p>This paper in particular is very neat because they discuss a network that allows them to compute fine-grained information — colour, position, and angle of cars — in very large satellite photos.</p>
<p>This is really an impressive result.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/What%20Is%20It%20Like%20Down%20There.html" rel="noopener">What Is It Like Down There</a> &mdash; June 13, 2018 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1806.05129.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/What Is It Like Down There.jpg"><img alt src="./images/showreel/What Is It Like Down There.jpg" /></a></center>
          

          <p>This is already a classic of the generative genre. They take a satellite photo, and then use GANs to work out what that particular region woud look like if viewed from the ground.</p>
<p>It’s amusing to me because it’s moderately well-posed; i.e. there is definitely the data present to make <em>some</em> kind of guess, but getting to the ground truth is kind of “obviously” impossible.</p>
<p>Even under such contraints, they do pretty well! And, as we most of this generative work, this is something that will only get better.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Image%20Generation%20from%20Scene%20Graphs.html" rel="noopener">Image Generation from Scene Graphs</a> &mdash; April  4, 2018 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>
            | <small><a href="https://arxiv.org/pdf/1804.01622.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Image Generation from Scene Graphs.jpg"><img alt src="./images/showreel/Image Generation from Scene Graphs.jpg" /></a></center>
          

          <p>Work from the famous <a href="https://en.wikipedia.org/wiki/Fei-Fei_Li">Fei Fei Li</a>, this is a very neat idea. There’s been some famous networks (“StackGAN”) that are able to generate pictures from text. But, they fail when you want to generate a complicated and unfamiliar scene. Humans, of course, can “dis-entangle” different concepts when thinking of complicated scenes, such as “a cat waiting to catch the train”. Even if we haven’t seen this exact thing before, we can easily imagine it, because we know how the things look, independently. The contribution in this work is the same idea, for neural networks, and they achieve awesome results! We can definitely expect significant improvements in this area, over the coming years.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Women%20also%20Snowboard.html" rel="noopener">Women also Snowboard</a> &mdash; March 26, 2018 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/ethics.html">ethics</a>, <a href="./showreel-tags/technical.html">technical</a>
            | <small><a href="https://arxiv.org/pdf/1803.09797.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Women also Snowboard.jpg"><img alt src="./images/showreel/Women also Snowboard.jpg" /></a></center>
          

          <p>This is a famous and interesting paper. They identify a common problem in so-called “captioning” networks: namely, they can be right for the wrong reasons. In the photo, we see that a network guesed it was a man sitting at a computer; but it only spent time “looking” at the computer to work this out. In other words, a computer was strongly correlated with the photo being of “a man at the computer” in the training data. In this paper they introduce some techniques to deal with this problem. Basically, their idea is that we can penalise the network for thinking about gender when no gender information is present, and reward it for thinking about gender when it <em>is</em> apparent. Furthermore, their approach is generally useful for other models and situations.</p>
<p>We can expect more technical results in this area to be implemented alongside the social techniques (i.e. having more diverse people involved in the building of AI systems).</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Trying%20clothes%20on%2C%20virtually.html" rel="noopener">Trying clothes on, virtually</a> &mdash; November 22, 2017 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/fashion.html">fashion</a>, <a href="./showreel-tags/pose.html">pose</a>
            | <small><a href="https://arxiv.org/pdf/1711.08447.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Trying clothes on, virtually.jpg"><img alt src="./images/showreel/Trying clothes on, virtually.jpg" /></a></center>
          

          <p>This is a great example of attempting to apply AI in the real world. The problem here is the typical online-shopping problem: Here’s a thing that maybe I want to buy; but how would it look on me? This paper attempts to solve that problem by using pose information. It does a pretty good job for photos that are “simple” (i.e. model on a white wall), and does a reasonable, but not great, job on what is referred to as photos “in the wild” — just photos from everyday life; inside or outside. Over the years we can expect to see this kind of technology hit on-line retailers.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Priming%20Neural%20Networks.html" rel="noopener">Priming Neural Networks</a> &mdash; November 16, 2017 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>
            | <small><a href="https://arxiv.org/pdf/1711.05918.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Priming Neural Networks.jpg"><img alt src="./images/showreel/Priming Neural Networks.jpg" /></a></center>
          

          <p>This is a fun one. First, try and find “something” in the photo (it’s normal-sized; and you’ll know it when you see it).</p>
<p>…</p>
<p>Did you find anything?</p>
<p>Now, try searching for: <span class="hidden">a cat</span> (highlight this section of text to see it). Can you find it now that I’ve told you what to look for? Even if you can’t, it turns out that neural networks can. I think this is a really neat idea - priming a network to help it know what it’s trying to do.</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Come-Swim.html" rel="noopener">Style Transfer in Come Swim</a> &mdash; January 19, 2017 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/style-transfer.html">style-transfer</a>, <a href="./showreel-tags/art.html">art</a>
            | <small><a href="https://scirate.com/arxiv/1701.04928">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/1701.04928.png"><img alt src="./images/showreel/1701.04928.png" /></a></center>
          

          <p>This is a landmark paper for a few reasons. First of all, it’s co-authored by a movie star; secondly it’s an application of the famous “style transfer” algorithm to a short film, and importantly that put a significant amount of work into making sure that the sylistic <em>quality</em> of the style transfer is high; which you don’t typically see. It’s a really interesting collaboration between the researchers and the film industry. I’m sure we’ll see a lot more like this over the years!</p>

        </div>
      </div>
    </li>
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="./showreel/Understanding%20and%20Predicting%20Visual%20Humour.html" rel="noopener">Understanding and Predicting Visual Humour</a> &mdash; December 14, 2015 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/funny.html">funny</a>
            | <small><a href="https://arxiv.org/pdf/1512.04407.pdf">Paper</a></small>
          </span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Understanding and Predicting Visual Humour.jpg"><img alt src="./images/showreel/Understanding and Predicting Visual Humour.jpg" /></a></center>
          

          <p>Easily Noon’s favourite paper of 2015. In life we face many problems. One of them is, given some non-funny situation, how can we make it funny? Naively one might think computers can’t begin to attemp to solve this problem. One would be wrong. Consider the top row of this image. Two people having dinner. Very unfunny. Two dogs having dinner at a dinner table? Hilarious. Likewise, cats in a park? Unfunny. A racoon riding a scooter in the same park? Brilliant.</p>
<p>This network was trained on data generated by humans who took specific scenes and adjusted them to make them funny.</p>
<p>We’re not totally sure where we’ll see more applications of this work, but we love it.</p>

        </div>
      </div>
    </li>
    
  
</ul>


  </div>
</div>

  </div>

  <!-- Footer -->
  <div id="footer">
    <div id="newsletter">
      <h4>Newsletter!</h4>
<!-- Mailchimp -->
<div id="mc_embed_signup">
<form action="https://braneshop.us19.list-manage.com/subscribe/post?u=e94e88a100517dd09f1720e55&amp;id=7957d3fc4c" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<p>✌ Thanks for visiting! If you're interested, sign up to our newsletter to
receive monthly updates, and notifications of courses.
</p>
<div id="mc_embed_signup_scroll">
<div class="mc-field-group">
	<label for="mce-EMAIL">Email Address
</label>
	<input type="email" value name="EMAIL" class="required email" id="mce-EMAIL">
</div>
<div class="mc-field-group">
	<label for="mce-NAME">Name </label>
	<input type="text" value name="NAME" class id="mce-NAME">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e94e88a100517dd09f1720e55_7957d3fc4c" tabindex="-1" value></div>
    <div class="clear"><input type="submit" value="Subscribe! 🗞" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<!-- /Mailchimp -->
    </div>

    <div id="acknowledgement">
      <img src="./images/atsi.webp" alt="Flags of the First Nations Peoples of Australia" />
      <p> Braneshop is located on the traditional lands of the people of the
      Kulin nation. We acknowledge that sovereignty was never ceded and pay
      our respects to elders past, present and emerging.
      <a href="https://www.goodreads.com/book/show/48254342-finding-the-heart-of-the-nation" rel="noopener">Finding the Heart of the Nation</a>.
      </p>
    </div>

    <div id="small-links">
      <div class="link-section">
        <h6>Links</h6>
        <a href="./">Home</a>
        <a href="./blog.html">Blog</a>
        <a href="./events.html" title="Events">Community &amp; Events</a>
        <a href="./team.html">Team</a>
        <a href="./contact.html">Contact</a>
        <a href="./community.html">Community</a>
        <a href="./privacy.html">Privacy Policy</a>
        <a href="./6-week-workshop-on-deep-learning.html" title="Technical Deep Learning Workshop">Technical Deep Learning Workshop</a>
        <a href="./ai-for-leadership.html" title="AI For Leadership">AI For Leadership</a>
        <a href="./custom-ai-workshop.html" title="Custom AI Workshop">Custom AI Workshop</a>
        <a href="./advisory-and-consulting.html" title="AI Advisory and Consulting">AI Advisory and Consulting</a>
      </div>

      <div class="link-section">
        <h6>Tools/Fun</h6>
        <a href="./quickstart.html">Deep learning quick start</a>
        <a href="./showreel.html">Deep learning showreel</a>
        <a href="./thesetestimonialsdontexist.html">These testimonials don't exist ...</a>
        <a href="./object-detection-in-the-browser.html">Object-detecion in the browser!</a>
        <a href="https://silky.github.io/cppn-playground/">The CPPN Playground</a>
      </div>
    </div>
  </div>

  <script type="text/javascript">
  window.onload = function() {
    const prism = document.getElementById("braneshop-fixed")
    var h = 80;

    function setFrame(frame) {
      var f = frame % 120;
      prism.style["background-position"] = "0 " + "-" + (f*h) + "pt";
    }

    document.addEventListener('scroll', function(x) {
      setFrame(Math.floor(window.scrollY/30))
    }, false)
  }
  </script>
</html>
