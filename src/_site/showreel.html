<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="google-site-verification" content="cKie6SUiby1JmI2F8RMscJRBTa28kTM6XNHS5l1GXxc" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131177872-1"></script>
  <script>
    var host = window.location.hostname;
    if ( host != "localhost") {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-131177872-1');
    }
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>Braneshop - Deep learning showreel!</title>
  <meta name="geo.region" content="AU-VIC" />
  <meta name="geo.placename" content="Melbourne" />
  <link rel="stylesheet" type="text/css" href="./css/default.css" />
  <link rel="stylesheet" type="text/css" href="./css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=PT+Mono|Lato|DM+Serif+Text" rel="stylesheet">
  <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- Facebook Pixel Code -->
  <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window, document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '382844492347906');
    fbq('track', 'PageView');
  </script>
  <noscript><img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=382844492347906&ev=PageView&noscript=1" /></noscript>
  <!-- End Facebook Pixel Code -->
</head>
<body>
  <div id="braneshop-fixed" onclick="document.location = '/';"></div>


  <!-- Header -->
  <div id="header">
    <div class="top">
      <div class="logos">
        <h1><a href="./"><img id="header-logo" class="compressed" src="./images/braneshop-blue.png" title="Braneshop" alt="Braneshop" /></a></h1>
      </div>

      
        <div class="m">
          <ul class="menu">
            <li><a href="./" title="Home">Home</a>|</li>
            <li><a href="./team.html" title="Team">Team</a>|</li>
            <li><a href="./community.html" title="Community">Community</a>|</li>
            <li><a href="./blog.html" title="Blog">Blog</a>|</li>
            <li><a href="./faq.html" title="FAQs">FAQs</a>|</li>
            <li><a href="./showreel.html" title="Showreel">Showreel</a>|</li>
            <li><a href="./contact.html" title="Contact">Contact</a></li>
          </ul>
          <ul class="menu smaller">
            <li><a href="./6-week-workshop-on-deep-learning.html" title="6 Week Workshop">6 Week Workshop</a>
              <span class="important">(Starts 8-Aug)</span> |
              </li>
            <li><a href="./ai-for-leadership.html" title="AI For Leadership">AI For Leadership</a>
              <span class="important">(On 23-Sep)</span> |
              </li>
            <li><a href="./deep-learning-workshop.html" title="Intensive Technical Deep Learning Workshop">Intensive Workshop</a></li>
          </ul>
        </div>
      
    </div>
    <small>Expand your knowledge manifold!</small>
  </div>


  <!-- Content -->
  <div id="content">
    <div class="section" id="showreel">
  <a name="showreel"></a>
  <h3> <span>üé• Deep learning showreel! </span> </h3>
  <div id="show-reel">
    <p>
    We closely follow research in deep learning and AI. Here are a collection
    of cool, interesting, and fun applications that we've seen, with a brief
    explanation and a link to the original paper. All figures are taken
    directly from the paper or the associated website.
    </p>

    <p>Check back for regular updates, or sign up to the newsletter (below) to
    receive the latest cool stuff, monthly!</p>

    <ul class="blog-list normal">
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.04759.pdf" rel="noopener">Synthetic fruit</a> &mdash; July 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Synthetic fruit.jpg"><img src="./images/showreel/Synthetic fruit.jpg" /></a></center>
          

          <p>This is an old idea, and just one example among many. There‚Äôs nothing inherently outstanding in this paper, but we just wanted to note the very useful technique of using ‚Äúfake‚Äù (synthetic) data to help solve real-world problems. This is a very useful technique, especially in light of the remarkable abilities of <a href="./posts/2018-12-17-How-Much-Data-For-Retraining.html">transfer learning</a> to help us adapt to new data.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.04572.pdf" rel="noopener">Out-of-Distribution Detection using Generative Models</a> &mdash; July 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/technical.html">technical</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Out-of-Distribution Detection using Generative Models.jpg"><img src="./images/showreel/Out-of-Distribution Detection using Generative Models.jpg" /></a></center>
          

          <p><a href="./posts/2018-12-17-How-Much-Data-For-Retraining.html">In an old blogpost</a> we discussed the problem of networks making over-confident predictions. This paper focused on over-confidence on images that the network has <em>never</em> seen (i.e.¬†trained on cats and dogs, then very confident that a picture of a boat is a dog).</p>
<p>A classical idea (we saw it in the ‚ÄúDetecting the Unexpected‚Äù paper) is that if we think about how well we can <em>reconstruct</em> a given image, that might tell us something about how often our network has seen it; i.e.¬†if it‚Äôs ‚Äúin-dstribution‚Äù or not.</p>
<p>This paper notes that one problem with that idea is that if the thing we‚Äôre looking at is ‚Äúsimple‚Äù (technically, has ‚Äúsmall variance‚Äù), then because the generative models are powerful, they might still do a good job.</p>
<p>The approach they provide in the paper is to use a different kind of generative network, the so-called ‚ÄúNeural Rendering Model (NRM)‚Äù, to do the image generation, and that this new technique just happens to be better at being informative when the data is from a set the network has never seen.</p>
<p>The picture above shows that the NRM-approach does quite a good job of seperating between images the network has seen and hasn‚Äôt seen.</p>
<p>This is a bit of a technical result, but it‚Äôs a crucially important area of research for networks that are going to be used in the real world.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.04553.pdf" rel="noopener">Learning to understand videos for answering questions</a> &mdash; July 10, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/text%20%28nlp%29.html">text (nlp)</a>, <a href="./showreel-tags/visual-question-answering.html">visual-question-answering</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning to understand videos for answering questions.jpg"><img src="./images/showreel/Learning to understand videos for answering questions.jpg" /></a></center>
          

          <p>Videos are becoming increasingly prolific on the internet. Naturally, then, it makes sense that researchers are spending time trying to understand them. One particular area of research is so-called ‚ÄúVisual queastion-answering‚Äù. The point is to train a network to be able to watch a video, then answer questions (via text) about what happened in the video. Some examples are provided in the image above.</p>
<p>This work introduces a nice idea to this area, one that we‚Äôre seeing frequently on the showreel, namely: building up a rich representation first, and then using that representation to further refine answers. This should be a bit similar, conceptually, to the ‚ÄúScene Graph‚Äù work, for example.</p>
<p>It‚Äôs also neat that the researchers are from <a href="https://www.deakin.edu.au/">Deakin</a>!</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.04135.pdf" rel="noopener">What-If ... We could interactively understand ML Models?</a> &mdash; July  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/visualisation.html">visualisation</a></span>
        

        <div class="content">
          
          <center><a href="./images/showreel/What-If ... We could interactively understand ML Models.jpg"><img src="./images/showreel/What-If ... We could interactively understand ML Models.jpg" width="800" /></a></center>
          

          <p>This is some software that Google put out a few years ago under a different name (it was called ‚Äúfacets‚Äù). This specific tool I‚Äôm not so convinced on, but it‚Äôs a very good attempt to tackle a very important idea ‚Äî how bias and decision-making can be understood interactively.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.04428.pdf" rel="noopener">Machine Learning for Side Channel Attacks</a> &mdash; July  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/privacy.html">privacy</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Machine Learning for Side Channel Attacks.jpg"><img src="./images/showreel/Machine Learning for Side Channel Attacks.jpg" /></a></center>
          

          <p>This is a quirky one, but it‚Äôs kind of ‚Äúflag-planting‚Äù in the ML/Security world. For years, security researchers have spent time finding what they call ‚Äúside-channel‚Äù attacks. An example is, say, listening to the soup that someone makes when typing, and from that sound, working out <em>what they are typing</em>. It‚Äôs called ‚Äúside-channel‚Äù because it‚Äôs not, say, capturing the keystrokes via the computer, it‚Äôs via an additional ‚Äúchannel‚Äù.</p>
<p>The main point of this paper is that they‚Äôre applying standard ML techniques, in particular in regards to voltage, and are able to make an estimate of which applications are running on a given piece of hardware. This might not sound super useful as it is, but, as always in the security world, there‚Äôs much more juice to be squeezed here.</p>
<p>This will definitely be a space to watch in the security space - bringing in AI techniques to enhance our offensive security capabilities!</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.04446.pdf" rel="noopener">Designing User Interfaces that Allow Everyone to Contribute to AI Safety</a> &mdash; July  9, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/ethics.html">ethics</a>, <a href="./showreel-tags/ux-of-ai.html">ux-of-ai</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Designing User Interfaces that Allow Everyone to Contribute to AI Safety.jpg"><img src="./images/showreel/Designing User Interfaces that Allow Everyone to Contribute to AI Safety.jpg" /></a></center>
          

          <p>Improving the situation around AI Ethics is strongly on our agenda at the Braneshop. This paper highlights an interesting situation: suppose you have people who want to provide feedback to some decision making process; what should the interface they use look like?</p>
<p>Here they explore a potential design that allows people to see the <em>impact</em> of their actions in a variety of ways.</p>
<p>This won‚Äôt be the last word on the matter, but it‚Äôs a nice contribution to the field, and hopefully pushes people to think very hard about this problem.</p>
<p>This is one bit of work in a growing field we refer to as ‚ÄúThe UX of AI‚Äù. This will definitely be a huge area over the coming years.</p>

        </div>
      </li>
      
  
    
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.03537.pdf" rel="noopener">Linking Art through Human Poses</a> &mdash; July  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/pose.html">pose</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Linking Art through Human Poses.jpg"><img src="./images/showreel/Linking Art through Human Poses.jpg" /></a></center>
          

          <p>This one is cool for the kind of neat technique it demonstrates. They use a pose network (something that just looks at an image of a person, say, and estimates what their skeleton looks like; i.e.¬†it tries to guess some straight lines that connect their arms and legs and such) to connect different artworks. It‚Äôs a neat application of what is becoming a standard technique.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.03381.pdf" rel="noopener">Estimating travel time without roads</a> &mdash; July  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Estimating travel time without roads.jpg"><img src="./images/showreel/Estimating travel time without roads.jpg" /></a></center>
          

          <p>Again, a neat idea applied well. In this paper they suppose that, in fact, we don‚Äôt need detailed road networks to do reasonably well at estimating travel time. We just need to get a vague feeling for the kinds of areas we‚Äôll be travelling though (i.e.¬†highway, commercial, residential, country, park, urban, etc). They make these ideas precise and get some great results!</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.03520.pdf" rel="noopener">Action Recognition from Poses</a> &mdash; July  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/pose.html">pose</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Action Recognition from Poses.jpg"><img src="./images/showreel/Action Recognition from Poses.jpg" /></a></center>
          

          <p>A pretty standard, but useful, technique that uses a kind of multi-stage process to: 1) compute the pose, 2) then from the a series of these poses, ver time, work out what ‚Äúaction‚Äù people are performing. Specifically here they focus on people going past train ticket machines in various ways, but the application is general.</p>

        </div>
      </li>
      
  
    
    
  
    
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1907.02040.pdf" rel="noopener">Albatrosses from Space</a> &mdash; July  3, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/science.html">science</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Albatrosses from Space.jpg"><img src="./images/showreel/Albatrosses from Space.jpg" /></a></center>
          

          <p>A really nice scientific application of deep learning; and something that maybe any reasonable person would not assume is possible right now. We like this one because it‚Äôs the overlap of modern deep learning techniques to old (but important!) problems of tracking animal movements for conservation reasons.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card" style="width: 550px;">
        
        <h5> <a href="https://arxiv.org/pdf/1907.02014.pdf" rel="noopener">AI for Economic Uplift of Handicraft</a> &mdash; May 31, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/AI for Economic Uplift of Handicraft.jpg"><img src="./images/showreel/AI for Economic Uplift of Handicraft.jpg" /></a></center>
          

          <p>While this one isn‚Äôt strictly using deep learning, it does use some classical machine learning techniques. But the reason we consider it particularly cool, is because the authors actually took their system ‚Äúto the streets‚Äù, as it were, and verified that using the new design processes helped the artisans sell more items!</p>

        </div>
      </li>
      
  
    
    
  
    
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.08653.pdf" rel="noopener">Attacking person-identification with patches</a> &mdash; April 18, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/privacy.html">privacy</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Attacking person-identification with patches.jpg"><img src="./images/showreel/Attacking person-identification with patches.jpg" /></a></center>
          

          <p>The game in this one is - can we make a picture, that can be printed and held in front of us, that will fool a person-detector? Yes, it turns out.</p>
<p>This is refered to as an ‚Äúadversarial‚Äù attack, and they have gained a lot of attention recently. This one in particular is interesting because they attack a standard person-detector (so-called ‚ÄúYolo‚Äù) and the image they use is ‚Äúlocal‚Äù and ‚Äúprintable‚Äù. There had been a few results in this area, but nothing attacking person detectors.</p>
<p>In the research world, we‚Äôre seeing work on both fronts. There are a lot of work on how to do more of these, and make them more robust, and likewise there is a lot of work on how to make classifiers and detectors less vulnerable to such attacks. Who will win? It‚Äôs not clear. I‚Äôd put my money on it always being possible to make such attacks, given enough information on the classifier. But, the cost of such attacks will rise significantly, making it unfeasible for most of us.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.07595.pdf" rel="noopener">Detecting the Unexpected</a> &mdash; April 16, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/technical.html">technical</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Detecting the Unexpected.jpg"><img src="./images/showreel/Detecting the Unexpected.jpg" /></a></center>
          

          <p>This is a really neat and important idea. The application here is in self-driving cars, but the central idea is very general. The main point is, if we‚Äôve trained a network to detect certain classes of thing (‚Äúcar‚Äù, ‚Äúroad‚Äù, ‚Äúperson‚Äù, ‚Äútruck‚Äù) then, if it sees something completely unexpected, (‚Äúgoose‚Äù), what will it predict? Depending on how you set up the network, it will predict one of the known classes. This work is about quantifying how confident the network should feel about such prediction. Their idea is to ask the network to think about how well it can reconstrut the thing it thought it saw. If it finds it hard, then that indicates that the thing it saw is moderately unknown to it, and so it shouldn‚Äôt be confident. As we have more AI out in real life making decisions, quantifying uncertainty will become increasingly important.</p>

        </div>
      </li>
      
  
    
    
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.05866.pdf" rel="noopener">Expressive 3D Body Capture from a Single Image</a> &mdash; April 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/pose.html">pose</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Expressive 3D Body Capture from a Single Image.jpg"><img src="./images/showreel/Expressive 3D Body Capture from a Single Image.jpg" /></a></center>
          

          <p>More and more we‚Äôre seeing deep learning tackle rich reconstruction problems from simple inputs. This is a classic of the genre. As humans, we can easily imagine the 3D structure of the person in the photo; and it turns out now deep learning can do the same, via the techniques in this paper. It‚Äôs very impressive work, and is applicable for those people wishing to capture this information without a complicated set up of a 3D body scanner. As usual, the typical applications will be in retail, but maybe also augmented-reality and other such fun things. As is the case with all these body-pose-related papers, they use an underlying pose network and build on top of it‚Äôs outputs. This is also a central and important topic in modern AI: building up rich and strong capabilities by combining different techniques.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.03851.pdf" rel="noopener">Extreme Image Compression</a> &mdash; April  8, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/technical.html">technical</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Extreme Image Compression.jpg"><img src="./images/showreel/Extreme Image Compression.jpg" /></a></center>
          

          <p>A natural thought would be that if we know a lot about the thing we‚Äôre trying to compress, we can do a better job. Standard compression algorithms are general-purpose, and as such, there is probably room to improve. This is the observation and work in this paper: They <em>learn</em> a compression function for a specific set of data, and they do really well! Probably not suitable for most of us, but you can be sure the big data storage providers will be working on these kinds of techniques into the future.</p>
<p>If we wanted to be trendy we could summarise this as ‚Äúbig data makes small data‚Äù.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card" style="width: 600px;">
        
        <h5> <a href="https://scirate.com/arxiv/1904.02579" rel="noopener">Can a Robot Become a Movie Director?</a> &mdash; April  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/drones.html">drones</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
          <center><a href="./images/showreel/1904.02579.png"><img src="./images/showreel/1904.02579.png" width="500px" /></a></center>
          

          <p>The main point here is that if we‚Äôre interested in determining where to point a drone while filming some scene, it might be hard, because the director would need to be able to somehow see everything, while the drone is flying. This paper proposes that perhaps thee could be a method to have the drone know where to look.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.03189.pdf" rel="noopener">Image2StyleGan - aka Ryan Obama aka Oprah Johansson</a> &mdash; April  5, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/art.html">art</a>, <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a>, <a href="./showreel-tags/technical.html">technical</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Image2StyleGan - aka Ryan Obama aka Oprah Johansson.jpg"><img src="./images/showreel/Image2StyleGan - aka Ryan Obama aka Oprah Johansson.jpg" /></a></center>
          

          <p>One of the most exciting areas of AI is the generative/creative opportunities. And in this area, something people are always fascinated by is the exploring the ‚Äúspace‚Äù of images; i.e here are all the photos of people, but what does a person who is ‚Äúhalfway between these two people‚Äù look like? This paper works on that problem, and produces some very cool looking people such as Ryan Obama, Oprah Johansson and Hugh de Niro. Notably, in this paper it seems like it doesn‚Äôt work so well for abstract/non-person style photos; but that‚Äôs probably due to the data, and not a general problem.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.00150.pdf" rel="noopener">Learning how music and images relate</a> &mdash; March 30, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/music.html">music</a>, <a href="./showreel-tags/technical.html">technical</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Learning how music and images relate.jpg"><img src="./images/showreel/Learning how music and images relate.jpg" /></a></center>
          

          <p>This result is nice because it‚Äôs using a concept that we think is so important, we‚Äôve made it a central part of our <a href="./6-week-workshop-on-deep-learning.html">technical workshop</a>: the autoencoder.</p>
<p>In this work they map images and music into the same ‚Äúspace‚Äù (i.e.¬†points on the graph in the picture), and in-so-doing, they can learn when images and music are related! Nice, simple, and useful!</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1904.00276.pdf" rel="noopener">Detecting people using only WiFi</a> &mdash; March 30, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/pose.html">pose</a>, <a href="./showreel-tags/privacy.html">privacy</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Detecting people using only WiFi.jpg"><img src="./images/showreel/Detecting people using only WiFi.jpg" /></a></center>
          

          <p>This is an interesting one. WiFi is everywhere; and probably a reasonable person wouldn‚Äôt assume they could be tracked (down to estimates of where they are walking, and the overall pose of their body) if there isn‚Äôt a camera around. But it turns out that this data actually <em>can</em> be gathered in (an ideal) WiFi set up. That is, the pose of people was determined <em>without</em> a camera; using <em>only</em> WiFi signals. No doubt this field - sensing human activity through non-camera based sensors - will continue to grow.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1903.10873.pdf" rel="noopener">Face Synthesis from a Single Image</a> &mdash; March 26, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Face Synthesis from a Single Image.jpg"><img src="./images/showreel/Face Synthesis from a Single Image.jpg" /></a></center>
          

          <p>Ignoring the specific contributions, this is a conceptually simple paper; but the results look amazing. The idea is: can we find a 3D model from a single image? And how much detail can it capture?</p>
<p>Turns out, heaps of detail! They introduce some nice techniques for modelling the facial features and such, but the main thing I like are the results.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1903.04143.pdf" rel="noopener">Unconstrained Ear Recognition</a> &mdash; March 11, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/funny.html">funny</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Unconstrained Ear Recognition.jpg"><img src="./images/showreel/Unconstrained Ear Recognition.jpg" /></a></center>
          

          <p>Trust no-one. If you think covering your face is enough to stop <a href="https://en.wikipedia.org/wiki/Ferengi">people</a> from detecting who you are, you‚Äôre wrong. It turns out it‚Äôs possible to identify people from their ears. Why would anyone want to do this? Who knows. But it‚Äôs happening!</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1902.05387.pdf" rel="noopener">Finding small objects in a large scene</a> &mdash; February  6, 2019 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Finding small objects in a large scene.jpg"><img src="./images/showreel/Finding small objects in a large scene.jpg" /></a></center>
          

          <p>Satellite imagery is a hot topic. There‚Äôs been many stories of people using such imagery to gain competitive advantage in many ways; from estimating the number of sales at department stores, to prediction crop yield.</p>
<p>This paper in particular is very neat because they discuss a network that allows them to compute fine-grained information ‚Äî colour, position, and angle of cars ‚Äî in very large satellite photos.</p>
<p>This is really an impressive result.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1804.01622.pdf" rel="noopener">Image Generation from Scene Graphs</a> &mdash; April  4, 2018 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a>, <a href="./showreel-tags/generative.html">generative</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Image Generation from Scene Graphs.jpg"><img src="./images/showreel/Image Generation from Scene Graphs.jpg" /></a></center>
          

          <p>Work from the famous <a href="https://en.wikipedia.org/wiki/Fei-Fei_Li">Fei Fei Li</a>, this is a very neat idea. There‚Äôs been some famous networks (‚ÄúStackGAN‚Äù) that are able to generate pictures from text. But, they fail when you want to generate a complicated and unfamiliar scene. Humans, of course, can ‚Äúdis-entangle‚Äù different concepts when thinking of complicated scenes, such as ‚Äúa cat waiting to catch the train‚Äù. Even if we haven‚Äôt seen this exact thing before, we can easily imagine it, because we know how the things look, independently. The contribution in this work is the same idea, for neural networks, and they achieve awesome results! We can definitely expect significant improvements in this area, over the coming years.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1803.09797.pdf" rel="noopener">Women also Snowboard</a> &mdash; March 26, 2018 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/ethics.html">ethics</a>, <a href="./showreel-tags/technical.html">technical</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Women also Snowboard.jpg"><img src="./images/showreel/Women also Snowboard.jpg" /></a></center>
          

          <p>This is a famous and interesting paper. They identify a common problem in so-called ‚Äúcaptioning‚Äù networks: namely, they can be right for the wrong reasons. In the photo, we see that a network guesed it was a man sitting at a computer; but it only spent time ‚Äúlooking‚Äù at the computer to work this out. In other words, a computer was strongly correlated with the photo being of ‚Äúa man at the computer‚Äù in the training data. In this paper they introduce some techniques to deal with this problem. Basically, their idea is that we can penalise the network for thinking about gender when no gender information is present, and reward it for thinking about gender when it <em>is</em> apparent. Furthermore, their approach is generally useful for other models and situations.</p>
<p>We can expect more technical results in this area to be implemented alongside the social techniques (i.e.¬†having more diverse people involved in the building of AI systems).</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1711.08447.pdf" rel="noopener">Trying clothes on, virtually</a> &mdash; November 22, 2017 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/fashion.html">fashion</a>, <a href="./showreel-tags/pose.html">pose</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Trying clothes on, virtually.jpg"><img src="./images/showreel/Trying clothes on, virtually.jpg" /></a></center>
          

          <p>This is a great example of attempting to apply AI in the real world. The problem here is the typical online-shopping problem: Here‚Äôs a thing that maybe I want to buy; but how would it look on me? This paper attempts to solve that problem by using pose information. It does a pretty good job for photos that are ‚Äúsimple‚Äù (i.e.¬†model on a white wall), and does a reasonable, but not great, job on what is referred to as photos ‚Äúin the wild‚Äù ‚Äî just photos from everyday life; inside or outside. Over the years we can expect to see this kind of technology hit on-line retailers.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1711.05918.pdf" rel="noopener">Priming Neural Networks</a> &mdash; November 16, 2017 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/computer-vision.html">computer-vision</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Priming Neural Networks.jpg"><img src="./images/showreel/Priming Neural Networks.jpg" /></a></center>
          

          <p>This is a fun one. First, try and find ‚Äúsomething‚Äù in the photo (it‚Äôs normal-sized; and you‚Äôll know it when you see it).</p>
<p>‚Ä¶</p>
<p>Did you find anything?</p>
<p>Now, try searching for: <span class="hidden">a cat</span> (highlight this section of text to see it). Can you find it now that I‚Äôve told you what to look for? Even if you can‚Äôt, it turns out that neural networks can. I think this is a really neat idea - priming a network to help it know what it‚Äôs trying to do.</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://scirate.com/arxiv/1701.04928" rel="noopener">Style Transfer in Come Swim</a> &mdash; January 19, 2017 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/style-transfer.html">style-transfer</a>, <a href="./showreel-tags/art.html">art</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/1701.04928.png"><img src="./images/showreel/1701.04928.png" /></a></center>
          

          <p>This is a landmark paper for a few reasons. First of all, it‚Äôs co-authored by a movie star; secondly it‚Äôs an application of the famous ‚Äústyle transfer‚Äù algorithm to a short film, and importantly that put a significant amount of work into making sure that the sylistic <em>quality</em> of the style transfer is high; which you don‚Äôt typically see. It‚Äôs a really interesting collaboration between the researchers and the film industry. I‚Äôm sure we‚Äôll see a lot more like this over the years!</p>

        </div>
      </li>
      
  
    
    <li> 
        
          <div class="showreel-card">
        
        <h5> <a href="https://arxiv.org/pdf/1512.04407.pdf" rel="noopener">Understanding and Predicting Visual Humour</a> &mdash; December 14, 2015 </h5>

        
          <span class="tags">Tags: <a href="./showreel-tags/funny.html">funny</a></span>
        

        <div class="content">
          
            <center><a href="./images/showreel/Understanding and Predicting Visual Humour.jpg"><img src="./images/showreel/Understanding and Predicting Visual Humour.jpg" /></a></center>
          

          <p>Easily Noons favourite paper of 2015. In life we face many problems. One of them is, given some non-funny situation, how can we make it funny? Naively one might think computers can‚Äôt begin to attemp to solve this problem. One would be wrong. Consider the top row of this image. Two people having dinner. Very unfunny. Two dogs having dinner at a dinner table? Hilarious. Likewise, cats in a park? Unfunny. A racoon riding a scooter in the same park? Brilliant.</p>
<p>This network was trained on data generated by humans who took specific scenes and adjusted them to make them funny.</p>
<p>We‚Äôre not totally sure where we‚Äôll see more applications of this work, but we love it.</p>

        </div>
      </li>
      
  
</ul>


  </div>
</div>

  </div>


  <!-- Footer -->
  <div id="footer">
    <div id="newsletter">

<!-- Mailchimp -->
      <a name="newsletter"></a>
      <h4>Newsletter!</h4>
<div id="mc_embed_signup">
<form action="https://braneshop.us19.list-manage.com/subscribe/post?u=e94e88a100517dd09f1720e55&amp;id=7957d3fc4c" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<p>‚úå Thanks for visiting! If you're interested, sign up to our newsletter to
receive monthly updates, and notifications of courses.
</p>
<div id="mc_embed_signup_scroll">
<div class="mc-field-group">
	<label for="mce-EMAIL">Email Address
</label>
	<input type="email" value name="EMAIL" class="required email" id="mce-EMAIL">
</div>
<div class="mc-field-group">
	<label for="mce-NAME">Name </label>
	<input type="text" value name="NAME" class id="mce-NAME">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e94e88a100517dd09f1720e55_7957d3fc4c" tabindex="-1" value></div>
    <div class="clear"><input type="submit" value="Subscribe! üóû" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<!-- /Mailchimp -->
    </div>


    <div id="acknowledgement">
      <img src="./images/atsi.webp" />
      <p> Braneshop is located on the traditional lands of the people of the
      Kulin nation. We acknowledge that sovereignty was never ceded and pay
      our respects to elders past, present and emerging.
      </p>
    </div>

    <div id="small-links">
      <div class="link-section">
        <h6>Links</h6>
        <a href="./">Home</a>
        <a href="./blog.html">Blog</a>
        <a href="./team.html">Team</a>
        <a href="./contact.html">Contact</a>
        <a href="./faq.html">FAQs</a>
        <a href="./community.html">Community</a>
        <a href="./privacy.html">Privacy Policy</a>
        <a href="./6-week-workshop-on-deep-learning.html" title="6 Week Workshop">6 Week Workshop</a>
        <a href="./ai-for-leadership.html" title="AI For Leadership">AI For Leadership</a>
        <a href="./deep-learning-workshop.html" title="Intensive Technical Deep Learning Workshop">Intensive Workshop</a>
      </div>

      <div class="link-section">
        <h6>Tools/Fun</h6>
        <a href="./quickstart.html">Deep learning quick start</a>
        <a href="./showreel.html">Deep learning showreel</a>
        <a href="./thesetestimonialsdontexist.html">These testimonials don't exist ...</a>
      </div>

      <div class="link-section">
        <h6>Upcoming Workshops</h6>
        <a title="Get ticket's for Braneshop's 6 week deep learning workshop, starting in August!" href="https://events.humanitix.com.au/braneshop-6-week-technical-deep-learning-workshop">8th-Aug - 6 Week Technical Deep Learning Workshop</a>
        <a title="Get tickets for Braneshop's AI For Leadership workshon in September!" href="https://events.humanitix.com.au/braneshop-ai-for-leadership">23rd-Sep - AI For Leadership</a>
      </div>
    </div>
  </div>


<script type="text/javascript">
window.onload = function() {
  const prism = document.getElementById("braneshop-fixed")
  var h = 80;

  function setFrame(frame) {
    var f = frame % 120;
    prism.style["background-position"] = "0 " + "-" + (f*h) + "pt";
  }

  document.addEventListener('scroll', function(x) {
  	setFrame(Math.floor(window.scrollY/30))
  }, false)
}
</script>

</body>
</html>
