<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<!--
You have entered the realm of

 ‚ñÑ‚ñÑ‚ñÑ‚ñÑ    ‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñà   ‚ñÑ‚ñÑ‚ñÑ       ‚ñà‚ñà‚ñà‚ñÑ    ‚ñà ‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñë ‚ñà‚ñà  ‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñì‚ñà‚ñà‚ñà  
‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñì‚ñà‚ñà ‚ñí ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñà‚ñà‚ñÑ     ‚ñà‚ñà ‚ñÄ‚ñà   ‚ñà ‚ñì‚ñà   ‚ñÄ ‚ñí‚ñà‚ñà    ‚ñí ‚ñì‚ñà‚ñà‚ñë ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñí  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñí
‚ñí‚ñà‚ñà‚ñí ‚ñÑ‚ñà‚ñà‚ñì‚ñà‚ñà ‚ñë‚ñÑ‚ñà ‚ñí‚ñí‚ñà‚ñà  ‚ñÄ‚ñà‚ñÑ  ‚ñì‚ñà‚ñà  ‚ñÄ‚ñà ‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñà   ‚ñë ‚ñì‚ñà‚ñà‚ñÑ   ‚ñí‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñà‚ñë‚ñí‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñí‚ñì‚ñà‚ñà‚ñë ‚ñà‚ñà‚ñì‚ñí
‚ñí‚ñà‚ñà‚ñë‚ñà‚ñÄ  ‚ñí‚ñà‚ñà‚ñÄ‚ñÄ‚ñà‚ñÑ  ‚ñë‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà ‚ñì‚ñà‚ñà‚ñí  ‚ñê‚ñå‚ñà‚ñà‚ñí‚ñí‚ñì‚ñà  ‚ñÑ   ‚ñí   ‚ñà‚ñà‚ñí‚ñë‚ñì‚ñà ‚ñë‚ñà‚ñà ‚ñí‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñí‚ñà‚ñà‚ñÑ‚ñà‚ñì‚ñí ‚ñí
‚ñë‚ñì‚ñà  ‚ñÄ‚ñà‚ñì‚ñë‚ñà‚ñà‚ñì ‚ñí‚ñà‚ñà‚ñí ‚ñì‚ñà   ‚ñì‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñë   ‚ñì‚ñà‚ñà‚ñë‚ñë‚ñí‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñí‚ñí‚ñë‚ñì‚ñà‚ñí‚ñë‚ñà‚ñà‚ñì‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñì‚ñí‚ñë‚ñí‚ñà‚ñà‚ñí ‚ñë  ‚ñë
‚ñë‚ñí‚ñì‚ñà‚ñà‚ñà‚ñÄ‚ñí‚ñë ‚ñí‚ñì ‚ñë‚ñí‚ñì‚ñë ‚ñí‚ñí   ‚ñì‚ñí‚ñà‚ñë‚ñë ‚ñí‚ñë   ‚ñí ‚ñí ‚ñë‚ñë ‚ñí‚ñë ‚ñë‚ñí ‚ñí‚ñì‚ñí ‚ñí ‚ñë ‚ñí ‚ñë‚ñë‚ñí‚ñë‚ñí‚ñë ‚ñí‚ñë‚ñí‚ñë‚ñí‚ñë ‚ñí‚ñì‚ñí‚ñë ‚ñë  ‚ñë
‚ñí‚ñë‚ñí   ‚ñë   ‚ñë‚ñí ‚ñë ‚ñí‚ñë  ‚ñí   ‚ñí‚ñí ‚ñë‚ñë ‚ñë‚ñë   ‚ñë ‚ñí‚ñë ‚ñë ‚ñë  ‚ñë‚ñë ‚ñë‚ñí  ‚ñë ‚ñë ‚ñí ‚ñë‚ñí‚ñë ‚ñë  ‚ñë ‚ñí ‚ñí‚ñë ‚ñë‚ñí ‚ñë     
 ‚ñë    ‚ñë   ‚ñë‚ñë   ‚ñë   ‚ñë   ‚ñí      ‚ñë   ‚ñë ‚ñë    ‚ñë   ‚ñë  ‚ñë  ‚ñë   ‚ñë  ‚ñë‚ñë ‚ñë‚ñë ‚ñë ‚ñë ‚ñí  ‚ñë‚ñë       
 ‚ñë         ‚ñë           ‚ñë  ‚ñë         ‚ñë    ‚ñë  ‚ñë      ‚ñë   ‚ñë  ‚ñë  ‚ñë    ‚ñë ‚ñë           
      ‚ñë                                                                         
-->

  <meta name="google-site-verification" content="cKie6SUiby1JmI2F8RMscJRBTa28kTM6XNHS5l1GXxc" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131177872-1"></script>
  <script type="text/javascript">
    var host = window.location.hostname;
    if ( host != "localhost") {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-131177872-1');
    }
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>How much data do you need to retrain a classifier? - Braneshop</title>
  <meta name="geo.region" content="AU-VIC" />
  <meta name="geo.placename" content="Melbourne" />
  <link rel="stylesheet" type="text/css" href="../css/default.css" />
  <link rel="stylesheet" type="text/css" href="../css/syntax.css" />

  <!-- TODO: Clean up fonts. -->
  <link href="https://fonts.googleapis.com/css?family=PT+Mono|Lato|DM+Serif+Text|Karma" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Gilda+Display|Quando" rel="stylesheet">



  <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- Meta tags for little cards and such. -->
  <meta property="og:title" content="How much data do you need to retrain a classifier? - Braneshop" />
  
  <meta property="og:url" content="https://braneshop.com.au/posts/2018-12-17-How-Much-Data-For-Retraining.html" />


<!-- Start of Async Drift Code -->
<script type="text/javascript">
"use strict";

!function() {
  var t = window.driftt = window.drift = window.driftt || [];
  if (!t.init) {
    if (t.invoked) return void (window.console && console.error && console.error("Drift snippet included twice."));
    t.invoked = !0, t.methods = [ "identify", "config", "track", "reset", "debug", "show", "ping", "page", "hide", "off", "on" ], 
    t.factory = function(e) {
      return function() {
        var n = Array.prototype.slice.call(arguments);
        return n.unshift(e), t.push(n), t;
      };
    }, t.methods.forEach(function(e) {
      t[e] = t.factory(e);
    }), t.load = function(t) {
      var e = 3e5, n = Math.ceil(new Date() / e) * e, o = document.createElement("script");
      o.type = "text/javascript", o.async = !0, o.crossorigin = "anonymous", o.src = "https://js.driftt.com/include/" + n + "/" + t + ".js";
      var i = document.getElementsByTagName("script")[0];
      i.parentNode.insertBefore(o, i);
    };
  }
}();
drift.SNIPPET_VERSION = '0.3.1';
drift.load('gge7kn9kp4gc');
</script>
<!-- End of Async Drift Code -->
<!-- LinkedIn -->
<script type="text/javascript">
_linkedin_partner_id = "1775578";
window._linkedin_data_partner_ids = window._linkedin_data_partner_ids || [];
window._linkedin_data_partner_ids.push(_linkedin_partner_id);
</script><script type="text/javascript">
(function(){var s = document.getElementsByTagName("script")[0];
var b = document.createElement("script");
b.type = "text/javascript";b.async = true;
b.src = "https://snap.licdn.com/li.lms-analytics/insight.min.js";
s.parentNode.insertBefore(b, s);})();
</script>


</head>
<body>
  <div id="braneshop-fixed" onclick="document.location = '/';"></div>
  <!-- Header -->
  <div id="header">
    <div id="header-top">
      <h1><a href="../"><img id="header-logo" class="compressed" src="../images/braneshop-blue.png" title="Braneshop" alt="Braneshop" /></a></h1>
    </div>
    <h2>Expand your knowledge manifold!</h2>
  </div>
  <div class="menu-box">
    <ul class="menu">
      <li><a href="../" title="Home">Home</a></li>
      <li><a href="../workshops.html" title="Workshops">Workshops</a></li>
      <li><a href="../about.html" title="About">About</a></li>
      <li><a href="../team.html" title="Team">Team</a></li>
      <li><a href="../events.html" title="Community and Events">Community &amp; Events</a></li>
      <li><a href="../blog.html" title="Blog">Blog</a></li>
      <li><a href="../faq.html" title="FAQs">FAQs</a></li>
      <li><a href="../showreel.html" title="Showreel">Showreel</a></li>
      <li><a href="../contact.html" title="Contact">Contact</a></li>
    </ul>
  </div>

  <!-- Content -->
  <div id="content">
    <div class="section">
  <div class="info">
    Posted on December 17, 2018
    
      by Noon van der Silk
    
		| <a title="Blog: Recent posts" href="../blog.html">Back to recent posts</a>
  </div>

  <h4 class="blog-title">How much data do you need to retrain a classifier?</h4>

  
    <span class="tags">Tags: <a href="../tags/deep-dive.html">deep-dive</a></span>
  


  
    <div class="info">
        <center><p> The code used for this blog-post is <a href="https://github.com/BraneShop/how-much-data-experiments">here</a>. </p></center>
    </div>
  

  


<script src="https://cdn.jsdelivr.net/npm/vega@4.3.0"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@3.0.0-rc10"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@3.24.1"></script>
<p>A while ago, Gala had a great idea for a little animation: Give people an idea of how much data they need to train a machine learning model, by showing an animation of a model trained on, say, 5, 50, 500, 5,000, 50,000, 500,000 data points. The way I pictured it in my mind was that as you proceed to more and more data, the number of successful classifications gets larger; ultimately tapering off as we get to larger numbers. In my mind I saw it as the standard S-curve:</p>
<center>
<p>
<img src="../images/blog/s-curve.png" />
</p>
</center>
<div class="important">
<p><b>Quiz</b>: Before reading the rest of this article, have a go at this quick quiz.</p>
<p>With the network and data-set we‚Äôre going to be focused on (<a href="https://www.tensorflow.org/hub/tutorials/image_retraining">the TensorFlow transfer learning example using InceptionV3</a>), the expected validation accuracy is 90-95%, when run on the full data set (~700 images per class).</p>
<p>Note that this is using five kinds of flowers as the classes: tulip, rose, daisy, dandelion, sunflower.</p>
<p>Suppose we set aside 70 images per class for validation, in each of the following scenarios:</p>
<ul class="normal">
<li>
Q1: If we use 50% of the remaining images, what would you expect the validation accuracy to be?
</li>
<li>
Q2: If we only use 5% of the remaining images?
</li>
<li>
Q3: If we only use <b>3 images in total</b> per class?
</li>
<li>
Q4: What about only <b>1 image</b> per class!?
</li>
</ul>
</div>
<h4 id="background-on-transfer-learning">Background on Transfer Learning</h4>
<p>For this article, we‚Äôre focusing on using a pre-trained network and specialising it to our specific (classification) task. This idea is called ‚ÄúTransfer Learnining‚Äù.</p>
<p>Here‚Äôs a picture that describes the idea:</p>
<center>
<img src="../images/blog/transfer-learning.png" />
</center>
<p>There‚Äôs two steps:</p>
<p>First, you take some ‚Äúpre-trained‚Äù network that is good at a particular task. That‚Äôs ‚Äú1)‚Äù in the picture above; some network that is good a recognising trees, houses, and people. It‚Äôs been trained for this task and is really good at it.</p>
<p>Secondly, you cut off the final layer, and plug in your new classes that you want to predict. This gives you new parameters to learn (the edges coloured in magenta above). Then, you train your new network on your new data.</p>
<p>The point is, this network should have a lot of ‚Äújuice‚Äù (i.e.¬†learned recognition capability) in the earlier layers, that capture general-purpose structure; and the later layers, that you leave as learnable paremeters, allow the network to specialise to your task.</p>
<p>This is a really powerful idea in deep learning, and, in essence, is incredibly heavily used (from the idea of pre-trained word vectors, to pre-trained image classification networks, as we‚Äôre using here).</p>
<h4 id="lets-dive-in">Let‚Äôs dive in</h4>
<p>Here‚Äôs the plan:</p>
<ol style="list-style-type: decimal">
<li>Follow the TensorFlow Transfer Learning Tutorial, but</li>
<li>Create a ‚Äúhold-out‚Äù dataset from the original dataset, say 10%,</li>
<li>Create downsampled datasets, from the new dataset, of the following: 100%, 90%, 80%, 70%, 60%, 50%, 40%, 30%, 20%, 10%, 5%, as well as the 3-image and 1-image sets. Visually:</li>
</ol>
<center>
<img src="../images/blog/how-much-data-selection.png" />
</center>
<ol start="4" style="list-style-type: decimal">
<li>Throw in some data augmentation, using <a href="https://github.com/aleju/imgaug">imgaug</a>,</li>
<li>Train all these experiments,</li>
<li>See how it all goes!</li>
</ol>
<p>I‚Äôve wrapped this up into a repo so you can reproduce my results: <a href="https://github.com/BraneShop/how-much-data-experiments">braneshop/how-much-data-experiments</a>. Follow the link and check out the README if you want to run it yourself. It takes a few hours to run everything.</p>
<h4 id="results">Results</h4>
<p>Let‚Äôs take a look at the train/validation curves for all the training runs. You can click on the circle next to a given experiment name to show only that. Shift-click will show many at once. First, note that in the ‚Äújust-1‚Äù and ‚Äújust-3‚Äù image case, I hacked the normal training code to not worry about having any validation/test data allocation. As a result, we don‚Äôt have them in this graph.</p>
<div id="train-val-curves-1">

</div>
<style type="text/css"> div.vega-actions { display: none; } </style>
<script type="text/javascript">
	var spec = { 
		"$schema": "https://vega.github.io/schema/vega-lite/v3.json",
		"data": {"url": "/data/train-val-curves-1.json" },
		"hconcat": [ 
			{ "title": "Train (solid) / Validation (dashed) Curves For Experiments",
				"width": 700, "height": 400,
				"layer":
					[ // Training
						{ "mark": { "type": "line", "interpolate": "basis" },
							"encoding": {
								"x": {"field": "step", "type": "quantitative", "axis": {"title": "Step"}},
								"y": {"field": "loss_train", "type": "quantitative", "axis": {"title": "Value"}},
								"color": {"field": "Experiment", "type": "nominal", "legend": null},
								"tooltip": {"field": "loss_train", "type": "nominal"}
							},
							"transform": [{"filter": {"selection": "legend"}}],
						}
						// Validation
						, { "mark": { "type": "line", "interpolate": "basis", "strokeDash": [4,4] },
								"encoding": {
									"x": {"field": "step", "type": "quantitative", "axis": {"title": "Step"}},
									"y": {"field": "loss_val", "type": "quantitative", "axis": {"title": "Value"}},
									"color": {"field": "Experiment", "type": "nominal", "legend": null},
									"tooltip": {"field": "loss_val", "type": "nominal"}
								},
								"transform": [{"filter": {"selection": "legend"}}],
							}]
				},
				// Legend
				{ "mark": { "type": "circle", "size": 200, "cursor": "pointer"},
					"encoding": { 
						"y": {"type": "nominal", "field": "Experiment", "title": "Experiments"}, 
						"color": { 
							"condition": { "type": "nominal", "field": "Experiment", "legend": null, "selection": "legend" }, 
							"value": "lightgray" 
						}
					}, 
					"selection": { 
						"legend": { 
							"type": "multi", 
							"encodings": ["color"], 
							"on": "click", 
							"toggle": "event.shiftKey", 
							"resolve": "global", 
							"empty": "all"
							}
						}
					}
			]
	};

	vegaEmbed("#train-val-curves-1", spec);
</script>
<p>Observations:</p>
<ul class="normal">
<li>
When we have tiny data (the 5% case) the train loss is immediately tiny; and the val loss immediately high. This was my expectation.
</li>
<li>
If we compare, say, the 100-image case and the 100 with 10x augmentation, the augmented version is worth in train and validation. This was surprising to me.
</li>
<li>
There‚Äôs not a significant different in the curves between say the 100% case and the 50% case. This, again, was surprising to me.
</li>
<li>
Augmentation always made both curves worse. I wasn‚Äôt expecting this, and am not yet cetrain why it‚Äôs the case. I have two main ideas: I used a very elaborate augmentation (the complicated one from the readme on the <a href="https://github.com/aleju/imgaug">imgaug</a> repo), maybe that was a mistake. I think ultimately, the augmented images don‚Äôt look much like the testing images, so they encouraged they network to do worse.
</li>
</ul>
<p>The real proof is in the accuracy against the holdout set, though, so let‚Äôs see how the models performed in this case. The following graph is the result of running all the models on the holdout images, and taking <strong>the maximum predicted value</strong> of all the resulting class predictions.</p>
<div id="holdout-accuracy">

</div>
<script type="text/javascript">
	var spec 
		= { "$schema": "https://vega.github.io/schema/vega-lite/v3.json", "width": 700, "height": 400,
			  "mark": { "type": "bar", "interpolate": "basis" }, 
				"data": {"url": "/data/holdout-accuracies.json" },
				"encoding": { 
					"y": {"field": "Experiment", 
								"type": "nominal", 
								"sort": {"field": "Accuracy", "op": "average", "order": "ascending"}
							 },
				 "x": {"field": "Accuracy", 
							 "type": "quantitative", 
							 "axis": { "grid": true }, 
							 "scale": { "domain": [0,1] } 
							},
				 "tooltip": {"field": "Accuracy", "type": "quantitative"}
				 }
	};

	vegaEmbed("#holdout-accuracy", spec);
</script>
<p>There were <em>many</em> surprising things about this graph for me. Notably, it doesn‚Äôt look like the graph that I predicted at the start (note that the axes are flipped), but probably <em>the</em> most shocking thing for me is this:</p>
<p><strong>With just a single image per class</strong>, we get 68% accuracy on the holdout set! And with just 3 images, we get 75%! Recall that the holdout set consists of ~70 images per class.</p>
<p>Now, when I ran this initially, I hand-picked the 1-image and 3-images per class. I picked ones that looked pretty good, but I was still amazed at the results! I set up a notebook to run a few different examples of that, and I also evaluated the accuracy with the threshold approach, instead of the argmax; here are a few runs:</p>
<pre><code>seed = 1                    seed = 2                    seed = 3

1 image                     1 image                     1 image
              max: 0.5                    max: 0.53                   max: 0.55
  threshold (0.5): 0.3        threshold (0.5): 0.38       threshold (0.5): 0.39
  threshold (0.8): 0.13       threshold (0.8): 0.18       threshold (0.8): 0.15

3 images                    3 images                    3 images
              max: 0.68                   max: 0.66                   max: 0.67
  threshold (0.5): 0.55       threshold (0.5): 0.51       threshold (0.5): 0.54
  threshold (0.8): 0.23       threshold (0.8): 0.25       threshold (0.8): 0.23</code></pre>
<p>You can see that for randomly-picked images, the argmax sits aroud 53% for 1 image, and 67% for three images.</p>
<p>The point is, to me it‚Äôs quite amazing to see that picking the argmax, instead of using the threshold, gives such a radical improvement <em>on withheld data</em>.</p>
<p>I‚Äôve made a <a href="https://colab.research.google.com/drive/1VWxeyhOGMTHHVZFe33s5zaHIKXdDUkA2">notebook on Google Colaboratory</a>, so if you have a Google account, you can run the experiments for yourself (you‚Äôll have to save it as a new copy).</p>
<h4 id="discussion">Discussion</h4>
<p>For me, this was quite surprising. For one, I didn‚Äôt see the improvement at the middle-range of data that I was expecting (50% training data wasn‚Äôt much better than 100%, excluding the holdout set). But moreso, just how much juice can be squeezed out of this particular network + a handful of images is amazing.</p>
<p>The main thing this highlights to me is the power of <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised learning</a>, <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)">active learning</a>, and <a href="https://en.wikipedia.org/wiki/One-shot_learning">One-shot learning</a>.</p>
<p>Specifically, it suggests to me that the best thing to do is to set up a learning+data acquisition process that makes predictions as fast as possible, and constantly learns as it goes:</p>
<center>
<img src="../images/blog/proposed-workflow.png" />
</center>
<p>Overall, the idea of incorporating data into a model as-it-infers isn‚Äôt new, but to me, again, what is surprising is just how little data you need to get started; at least in this specific case!</p>
<p>To reiterate differently: If you‚Äôre a business wondering how to adopt AI, one plan would be to build this kind of human-in-the-loop system, where any prediction that is, say, better than 50% chance of being right will be useful to humans. Then, have the humans feed this system as they use it, and it will radically improve with just a small number of confirmed answers!</p>
<p>So, to answer the question from the title, we might say ‚Äúless than you think!‚Äù, if you have the right setup to incorporate new data into your system, as you go.</p>
<h4 id="open-questions">Open questions</h4>
<ul class="normal">
<li>
How do other models go in this measure? Say, segmentation models or bounding box models? Or NLP?
</li>
<li>
Is it really right to think of the softmax layer as <em>probabilities</em>? Certainly it satisfies a probability-distribution constraint, but training seems to simply push these values more apart? i.e.¬†we don‚Äôt really care if they are probabilities, if we‚Äôre just going to pick the largest. Further, maybe we should only ever think of the probability as the combination of the decision rule (say argmax or &gt; 80%) <em>and</em> the holdout dataset that we evaluate that rule on.
</li>
<li>
Why did augmentation make things worse? Did I over-do it? Related paper: <a href="https://scirate.com/arxiv/1811.04768">Learning data augmentation policies using augmented random search</a>.
</li>
<li>
Could be build some kind of ‚Äúdataset juicer‚Äù that tells us just how much juice we‚Äôll get, for given increases in our labelled data, with respect to certain models and targets?
</li>
<li>
Relatedly, it would be nice to calculate some kind of ‚Äúpotential-juice‚Äù measure where we look at the number of variables we‚Äôre going to retrain; the number of free variables in our data, and then consider what kind of space of we‚Äôll explore in this domain.
</li>
</ul>
<h4 id="related-papers">Related papers</h4>
<ul class="normal">
<li>
<a href="https://scirate.com/arxiv/1712.00409">Deep Learning Scaling is Predictable, Empirically</a> - This is a deeper investigation into these ideas, across a few different model types. It empirically answers my first open question.
</li>
<li>
<a href="https://scirate.com/arxiv/1811.04768">Learning data augmentation policies using augmented random search</a>.
</li>
</ul>
<h4 id="quiz-answers">Quiz answers</h4>
<div class="important">
<p>Using the maximum value of the class estimates (argmax):</p>
<ul class="normal">
<li>
Q1: If we use 50% of the remaining images, what would you expect the validation accuracy to be? Answer: ~91%
</li>
<li>
Q2: If we only use 5% of the remaining images? Answer: ~83%
</li>
<li>
Q3: If we only use <b>3 images in total</b> per class? Answer: ~67%
</li>
<li>
Q4: What about only <b>1 image</b> per class!? Answer: ~53%
</li>
</ul>
</div>
<h4 id="the-animation">The animation</h4>
<p>Here‚Äôs my attempt at the animation (click to view):</p>
<center>
<a href="https://imgur.com/a/swZ9qiQ"><img src="../images/blog/aug-00.jpg" /></a>
</center>
<p>The images with the red text above them are the ones where the inferences were wrong. Unfortunately (or fortunately!) it‚Äôs not very compelling, because all the accuracies are basically very high!</p>



  <hr />

  <div class="next-posts"> 
  </div>
</div>



  </div>


  <!-- Footer -->
  <div id="footer">
    <div id="newsletter">
      <h4>Newsletter!</h4>
<!-- Mailchimp -->
<div id="mc_embed_signup">
<form action="https://braneshop.us19.list-manage.com/subscribe/post?u=e94e88a100517dd09f1720e55&amp;id=7957d3fc4c" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<p>‚úå Thanks for visiting! If you're interested, sign up to our newsletter to
receive monthly updates, and notifications of courses.
</p>
<div id="mc_embed_signup_scroll">
<div class="mc-field-group">
	<label for="mce-EMAIL">Email Address
</label>
	<input type="email" value name="EMAIL" class="required email" id="mce-EMAIL">
</div>
<div class="mc-field-group">
	<label for="mce-NAME">Name </label>
	<input type="text" value name="NAME" class id="mce-NAME">
</div>
	<div id="mce-responses" class="clear">
		<div class="response" id="mce-error-response" style="display:none"></div>
		<div class="response" id="mce-success-response" style="display:none"></div>
	</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_e94e88a100517dd09f1720e55_7957d3fc4c" tabindex="-1" value></div>
    <div class="clear"><input type="submit" value="Subscribe! üóû" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
    </div>
</form>
</div>
<!-- /Mailchimp -->
    </div>

    <div id="acknowledgement">
      <img src="../images/atsi.webp" alt="Flags of the First Nations Peoples of Australia" />
      <p> Braneshop is located on the traditional lands of the people of the
      Kulin nation. We acknowledge that sovereignty was never ceded and pay
      our respects to elders past, present and emerging.
      <a href="https://www.goodreads.com/book/show/48254342-finding-the-heart-of-the-nation" rel="noopener">Finding the Heart of the Nation</a>.
      </p>
    </div>

    <div id="small-links">
      <div class="link-section">
        <h6>Links</h6>
        <a href="../">Home</a>
        <a href="../blog.html">Blog</a>
        <a href="../events.html" title="Events">Community &amp; Events</a>
        <a href="../team.html">Team</a>
        <a href="../contact.html">Contact</a>
        <a href="../faq.html">FAQs</a>
        <a href="../community.html">Community</a>
        <a href="../privacy.html">Privacy Policy</a>
        <a href="../6-week-workshop-on-deep-learning.html" title="6 Week Workshop">6 Week Workshop</a>
        <a href="../ai-for-leadership.html" title="AI For Leadership">AI For Leadership</a>
        <a href="../custom-ai-workshop.html" title="Custom AI Workshop">Custom AI Workshop</a>
      </div>

      <div class="link-section">
        <h6>Tools/Fun</h6>
        <a href="../quickstart.html">Deep learning quick start</a>
        <a href="../showreel.html">Deep learning showreel</a>
        <a href="../thesetestimonialsdontexist.html">These testimonials don't exist ...</a>
        <a href="../object-detection-in-the-browser.html">Object-detecion in the browser!</a>
        <a href="https://silky.github.io/cppn-playground/">The CPPN Playground</a>
      </div>

      <div class="link-section">
        <h6>Upcoming Workshops</h6>
        
          <a title="Get tickets for AI For Leadership - March 23, 2020 " href="https://events.humanitix.com.au/braneshop-ai-for-leadership-march">March 23, 2020 - AI For Leadership</a>
        
          <a title="Get tickets for Technical Deep Learning Workshop - April 10, 2020 " href="https://events.humanitix.com.au/braneshop-technical-deep-learning-workshop-april">April 10, 2020 - Technical Deep Learning Workshop</a>
        
          <a title="Get tickets for AI For Leadership - May 18, 2020 " href="https://events.humanitix.com.au/braneshop-ai-for-leadership-may">May 18, 2020 - AI For Leadership</a>
        
      </div>
    </div>
  </div>

  <script type="text/javascript">
  window.onload = function() {
    const prism = document.getElementById("braneshop-fixed")
    var h = 80;

    function setFrame(frame) {
      var f = frame % 120;
      prism.style["background-position"] = "0 " + "-" + (f*h) + "pt";
    }

    document.addEventListener('scroll', function(x) {
      setFrame(Math.floor(window.scrollY/30))
    }, false)
  }
  </script>

</html>
